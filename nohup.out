Traceback (most recent call last):
  File "/home/name/Desktop/Codes/ScalingOptimization (1)/./TheAlgorithm.py", line 8, in <module>
    get_ipython().system('pip install mujoco')
NameError: name 'get_ipython' is not defined
0

--------

Iteration0-Robot0

--------

{'eval/walltime': 49.64081168174744, 'training/sps': 35040.35667142899, 'training/walltime': 74.81202387809753, 'training/entropy_loss': Array(-0.013, dtype=float32), 'training/policy_loss': Array(0.003, dtype=float32), 'training/total_loss': Array(0.109, dtype=float32), 'training/v_loss': Array(0.119, dtype=float32), 'eval/episode_distance_from_origin': Array(37.89, dtype=float32), 'eval/episode_forward_reward': Array(10.593, dtype=float32), 'eval/episode_reward': Array(206.543, dtype=float32), 'eval/episode_reward_alive': Array(220.898, dtype=float32), 'eval/episode_reward_linvel': Array(10.593, dtype=float32), 'eval/episode_reward_quadctrl': Array(-24.949, dtype=float32), 'eval/episode_x_position': Array(3.689, dtype=float32), 'eval/episode_x_velocity': Array(8.475, dtype=float32), 'eval/episode_y_position': Array(1.024, dtype=float32), 'eval/episode_y_velocity': Array(1.905, dtype=float32), 'eval/episode_distance_from_origin_std': Array(6.718, dtype=float32), 'eval/episode_forward_reward_std': Array(10.352, dtype=float32), 'eval/episode_reward_std': Array(33.987, dtype=float32), 'eval/episode_reward_alive_std': Array(38.747, dtype=float32), 'eval/episode_reward_linvel_std': Array(10.352, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(4.607, dtype=float32), 'eval/episode_x_position_std': Array(2.519, dtype=float32), 'eval/episode_x_velocity_std': Array(8.282, dtype=float32), 'eval/episode_y_position_std': Array(3.351, dtype=float32), 'eval/episode_y_velocity_std': Array(9.842, dtype=float32), 'eval/avg_episode_length': Array(44.18, dtype=float32), 'eval/epoch_eval_time': 10.30774211883545, 'eval/sps': 12417.850439438547}
{'eval/walltime': 59.988845109939575, 'training/sps': 49304.279922553054, 'training/walltime': 127.98063325881958, 'training/entropy_loss': Array(-0.011, dtype=float32), 'training/policy_loss': Array(-0.013, dtype=float32), 'training/total_loss': Array(0.024, dtype=float32), 'training/v_loss': Array(0.049, dtype=float32), 'eval/episode_distance_from_origin': Array(48.382, dtype=float32), 'eval/episode_forward_reward': Array(14.811, dtype=float32), 'eval/episode_reward': Array(262.929, dtype=float32), 'eval/episode_reward_alive': Array(277.148, dtype=float32), 'eval/episode_reward_linvel': Array(14.811, dtype=float32), 'eval/episode_reward_quadctrl': Array(-29.03, dtype=float32), 'eval/episode_x_position': Array(6.418, dtype=float32), 'eval/episode_x_velocity': Array(11.849, dtype=float32), 'eval/episode_y_position': Array(0.409, dtype=float32), 'eval/episode_y_velocity': Array(0.197, dtype=float32), 'eval/episode_distance_from_origin_std': Array(8.678, dtype=float32), 'eval/episode_forward_reward_std': Array(14.612, dtype=float32), 'eval/episode_reward_std': Array(44.609, dtype=float32), 'eval/episode_reward_alive_std': Array(49.709, dtype=float32), 'eval/episode_reward_linvel_std': Array(14.612, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(5.518, dtype=float32), 'eval/episode_x_position_std': Array(3.566, dtype=float32), 'eval/episode_x_velocity_std': Array(11.69, dtype=float32), 'eval/episode_y_position_std': Array(4.481, dtype=float32), 'eval/episode_y_velocity_std': Array(10.24, dtype=float32), 'eval/avg_episode_length': Array(55.43, dtype=float32), 'eval/epoch_eval_time': 10.348033428192139, 'eval/sps': 12369.500049282537}
{'eval/walltime': 70.39497780799866, 'training/sps': 49014.80909069722, 'training/walltime': 181.46324491500854, 'training/entropy_loss': Array(-0.01, dtype=float32), 'training/policy_loss': Array(-0.01, dtype=float32), 'training/total_loss': Array(0.022, dtype=float32), 'training/v_loss': Array(0.043, dtype=float32), 'eval/episode_distance_from_origin': Array(54.638, dtype=float32), 'eval/episode_forward_reward': Array(19.817, dtype=float32), 'eval/episode_reward': Array(300.593, dtype=float32), 'eval/episode_reward_alive': Array(310.938, dtype=float32), 'eval/episode_reward_linvel': Array(19.817, dtype=float32), 'eval/episode_reward_quadctrl': Array(-30.161, dtype=float32), 'eval/episode_x_position': Array(9.069, dtype=float32), 'eval/episode_x_velocity': Array(15.853, dtype=float32), 'eval/episode_y_position': Array(1.239, dtype=float32), 'eval/episode_y_velocity': Array(2.365, dtype=float32), 'eval/episode_distance_from_origin_std': Array(12.551, dtype=float32), 'eval/episode_forward_reward_std': Array(14.864, dtype=float32), 'eval/episode_reward_std': Array(63.113, dtype=float32), 'eval/episode_reward_alive_std': Array(73.364, dtype=float32), 'eval/episode_reward_linvel_std': Array(14.864, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(7.998, dtype=float32), 'eval/episode_x_position_std': Array(4.4, dtype=float32), 'eval/episode_x_velocity_std': Array(11.891, dtype=float32), 'eval/episode_y_position_std': Array(4.405, dtype=float32), 'eval/episode_y_velocity_std': Array(9.186, dtype=float32), 'eval/avg_episode_length': Array(62.188, dtype=float32), 'eval/epoch_eval_time': 10.406132698059082, 'eval/sps': 12300.43895402892}
{'eval/walltime': 80.75543332099915, 'training/sps': 48836.775943589935, 'training/walltime': 235.14082598686218, 'training/entropy_loss': Array(-0.009, dtype=float32), 'training/policy_loss': Array(-0.01, dtype=float32), 'training/total_loss': Array(0.018, dtype=float32), 'training/v_loss': Array(0.037, dtype=float32), 'eval/episode_distance_from_origin': Array(67.199, dtype=float32), 'eval/episode_forward_reward': Array(26.716, dtype=float32), 'eval/episode_reward': Array(364.443, dtype=float32), 'eval/episode_reward_alive': Array(372.695, dtype=float32), 'eval/episode_reward_linvel': Array(26.716, dtype=float32), 'eval/episode_reward_quadctrl': Array(-34.968, dtype=float32), 'eval/episode_x_position': Array(14.93, dtype=float32), 'eval/episode_x_velocity': Array(21.373, dtype=float32), 'eval/episode_y_position': Array(3.702, dtype=float32), 'eval/episode_y_velocity': Array(4.278, dtype=float32), 'eval/episode_distance_from_origin_std': Array(17.969, dtype=float32), 'eval/episode_forward_reward_std': Array(16.746, dtype=float32), 'eval/episode_reward_std': Array(84.212, dtype=float32), 'eval/episode_reward_alive_std': Array(89.726, dtype=float32), 'eval/episode_reward_linvel_std': Array(16.746, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(9.14, dtype=float32), 'eval/episode_x_position_std': Array(9.041, dtype=float32), 'eval/episode_x_velocity_std': Array(13.397, dtype=float32), 'eval/episode_y_position_std': Array(7.893, dtype=float32), 'eval/episode_y_velocity_std': Array(10.817, dtype=float32), 'eval/avg_episode_length': Array(74.539, dtype=float32), 'eval/epoch_eval_time': 10.360455513000488, 'eval/sps': 12354.669139728776}
time to jit: 0:00:54.819460
time to train: 0:04:36.790913

--------

Iteration0-Robot1

--------

{'eval/walltime': 49.80555248260498, 'training/sps': 34220.88530787875, 'training/walltime': 76.60351204872131, 'training/entropy_loss': Array(-0.013, dtype=float32), 'training/policy_loss': Array(0.001, dtype=float32), 'training/total_loss': Array(0.118, dtype=float32), 'training/v_loss': Array(0.129, dtype=float32), 'eval/episode_distance_from_origin': Array(36.992, dtype=float32), 'eval/episode_forward_reward': Array(8.757, dtype=float32), 'eval/episode_reward': Array(198.253, dtype=float32), 'eval/episode_reward_alive': Array(213.828, dtype=float32), 'eval/episode_reward_linvel': Array(8.757, dtype=float32), 'eval/episode_reward_quadctrl': Array(-24.332, dtype=float32), 'eval/episode_x_position': Array(3.169, dtype=float32), 'eval/episode_x_velocity': Array(7.005, dtype=float32), 'eval/episode_y_position': Array(1.527, dtype=float32), 'eval/episode_y_velocity': Array(3.253, dtype=float32), 'eval/episode_distance_from_origin_std': Array(8.576, dtype=float32), 'eval/episode_forward_reward_std': Array(8.922, dtype=float32), 'eval/episode_reward_std': Array(44.727, dtype=float32), 'eval/episode_reward_alive_std': Array(49.464, dtype=float32), 'eval/episode_reward_linvel_std': Array(8.922, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(5.759, dtype=float32), 'eval/episode_x_position_std': Array(2.525, dtype=float32), 'eval/episode_x_velocity_std': Array(7.138, dtype=float32), 'eval/episode_y_position_std': Array(3.113, dtype=float32), 'eval/episode_y_velocity_std': Array(9.137, dtype=float32), 'eval/avg_episode_length': Array(42.766, dtype=float32), 'eval/epoch_eval_time': 10.371467590332031, 'eval/sps': 12341.551365335967}
{'eval/walltime': 60.184587240219116, 'training/sps': 49000.086500214726, 'training/walltime': 130.10219311714172, 'training/entropy_loss': Array(-0.011, dtype=float32), 'training/policy_loss': Array(-0.014, dtype=float32), 'training/total_loss': Array(0.032, dtype=float32), 'training/v_loss': Array(0.058, dtype=float32), 'eval/episode_distance_from_origin': Array(49.699, dtype=float32), 'eval/episode_forward_reward': Array(13.112, dtype=float32), 'eval/episode_reward': Array(265.464, dtype=float32), 'eval/episode_reward_alive': Array(282.422, dtype=float32), 'eval/episode_reward_linvel': Array(13.112, dtype=float32), 'eval/episode_reward_quadctrl': Array(-30.071, dtype=float32), 'eval/episode_x_position': Array(6.065, dtype=float32), 'eval/episode_x_velocity': Array(10.49, dtype=float32), 'eval/episode_y_position': Array(1.447, dtype=float32), 'eval/episode_y_velocity': Array(1.904, dtype=float32), 'eval/episode_distance_from_origin_std': Array(10.649, dtype=float32), 'eval/episode_forward_reward_std': Array(13.354, dtype=float32), 'eval/episode_reward_std': Array(54.864, dtype=float32), 'eval/episode_reward_alive_std': Array(60.892, dtype=float32), 'eval/episode_reward_linvel_std': Array(13.354, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(6.845, dtype=float32), 'eval/episode_x_position_std': Array(3.936, dtype=float32), 'eval/episode_x_velocity_std': Array(10.683, dtype=float32), 'eval/episode_y_position_std': Array(5.125, dtype=float32), 'eval/episode_y_velocity_std': Array(11.308, dtype=float32), 'eval/avg_episode_length': Array(56.484, dtype=float32), 'eval/epoch_eval_time': 10.379034757614136, 'eval/sps': 12332.553362546383}
{'eval/walltime': 70.55037021636963, 'training/sps': 48925.58342500461, 'training/walltime': 183.6823410987854, 'training/entropy_loss': Array(-0.01, dtype=float32), 'training/policy_loss': Array(-0.01, dtype=float32), 'training/total_loss': Array(0.028, dtype=float32), 'training/v_loss': Array(0.048, dtype=float32), 'eval/episode_distance_from_origin': Array(55.776, dtype=float32), 'eval/episode_forward_reward': Array(17.722, dtype=float32), 'eval/episode_reward': Array(300.673, dtype=float32), 'eval/episode_reward_alive': Array(314.297, dtype=float32), 'eval/episode_reward_linvel': Array(17.722, dtype=float32), 'eval/episode_reward_quadctrl': Array(-31.346, dtype=float32), 'eval/episode_x_position': Array(8.501, dtype=float32), 'eval/episode_x_velocity': Array(14.177, dtype=float32), 'eval/episode_y_position': Array(3.579, dtype=float32), 'eval/episode_y_velocity': Array(5.74, dtype=float32), 'eval/episode_distance_from_origin_std': Array(10.902, dtype=float32), 'eval/episode_forward_reward_std': Array(14.562, dtype=float32), 'eval/episode_reward_std': Array(56.823, dtype=float32), 'eval/episode_reward_alive_std': Array(59.895, dtype=float32), 'eval/episode_reward_linvel_std': Array(14.562, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(6.287, dtype=float32), 'eval/episode_x_position_std': Array(4.749, dtype=float32), 'eval/episode_x_velocity_std': Array(11.65, dtype=float32), 'eval/episode_y_position_std': Array(5.42, dtype=float32), 'eval/episode_y_velocity_std': Array(10.115, dtype=float32), 'eval/avg_episode_length': Array(62.859, dtype=float32), 'eval/epoch_eval_time': 10.365782976150513, 'eval/sps': 12348.31949448499}
{'eval/walltime': 80.92043685913086, 'training/sps': 48808.693721285854, 'training/walltime': 237.39080572128296, 'training/entropy_loss': Array(-0.009, dtype=float32), 'training/policy_loss': Array(-0.01, dtype=float32), 'training/total_loss': Array(0.025, dtype=float32), 'training/v_loss': Array(0.044, dtype=float32), 'eval/episode_distance_from_origin': Array(67.133, dtype=float32), 'eval/episode_forward_reward': Array(23.649, dtype=float32), 'eval/episode_reward': Array(360.921, dtype=float32), 'eval/episode_reward_alive': Array(372.695, dtype=float32), 'eval/episode_reward_linvel': Array(23.649, dtype=float32), 'eval/episode_reward_quadctrl': Array(-35.423, dtype=float32), 'eval/episode_x_position': Array(13.342, dtype=float32), 'eval/episode_x_velocity': Array(18.919, dtype=float32), 'eval/episode_y_position': Array(5.104, dtype=float32), 'eval/episode_y_velocity': Array(5.721, dtype=float32), 'eval/episode_distance_from_origin_std': Array(18.259, dtype=float32), 'eval/episode_forward_reward_std': Array(13.962, dtype=float32), 'eval/episode_reward_std': Array(85.662, dtype=float32), 'eval/episode_reward_alive_std': Array(95.09, dtype=float32), 'eval/episode_reward_linvel_std': Array(13.962, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(9.848, dtype=float32), 'eval/episode_x_position_std': Array(7.358, dtype=float32), 'eval/episode_x_velocity_std': Array(11.17, dtype=float32), 'eval/episode_y_position_std': Array(7.811, dtype=float32), 'eval/episode_y_velocity_std': Array(10.345, dtype=float32), 'eval/avg_episode_length': Array(74.539, dtype=float32), 'eval/epoch_eval_time': 10.37006664276123, 'eval/sps': 12343.218651285111}
time to jit: 0:00:54.651276
time to train: 0:04:39.100810

--------

Iteration0-Robot2

--------

{'eval/walltime': 48.22685956954956, 'training/sps': 35768.48483795655, 'training/walltime': 73.28909826278687, 'training/entropy_loss': Array(-0.013, dtype=float32), 'training/policy_loss': Array(0.001, dtype=float32), 'training/total_loss': Array(0.104, dtype=float32), 'training/v_loss': Array(0.115, dtype=float32), 'eval/episode_distance_from_origin': Array(40.449, dtype=float32), 'eval/episode_forward_reward': Array(10.894, dtype=float32), 'eval/episode_reward': Array(210.935, dtype=float32), 'eval/episode_reward_alive': Array(225.391, dtype=float32), 'eval/episode_reward_linvel': Array(10.894, dtype=float32), 'eval/episode_reward_quadctrl': Array(-25.35, dtype=float32), 'eval/episode_x_position': Array(3.953, dtype=float32), 'eval/episode_x_velocity': Array(8.715, dtype=float32), 'eval/episode_y_position': Array(1.054, dtype=float32), 'eval/episode_y_velocity': Array(2.461, dtype=float32), 'eval/episode_distance_from_origin_std': Array(8.359, dtype=float32), 'eval/episode_forward_reward_std': Array(12.278, dtype=float32), 'eval/episode_reward_std': Array(40.039, dtype=float32), 'eval/episode_reward_alive_std': Array(45.726, dtype=float32), 'eval/episode_reward_linvel_std': Array(12.278, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(5.5, dtype=float32), 'eval/episode_x_position_std': Array(3.121, dtype=float32), 'eval/episode_x_velocity_std': Array(9.823, dtype=float32), 'eval/episode_y_position_std': Array(3.654, dtype=float32), 'eval/episode_y_velocity_std': Array(10.537, dtype=float32), 'eval/avg_episode_length': Array(45.078, dtype=float32), 'eval/epoch_eval_time': 10.421590805053711, 'eval/sps': 12282.19399459911}
{'eval/walltime': 58.6812527179718, 'training/sps': 48981.65489036736, 'training/walltime': 126.80791068077087, 'training/entropy_loss': Array(-0.011, dtype=float32), 'training/policy_loss': Array(-0.013, dtype=float32), 'training/total_loss': Array(0.022, dtype=float32), 'training/v_loss': Array(0.047, dtype=float32), 'eval/episode_distance_from_origin': Array(50.934, dtype=float32), 'eval/episode_forward_reward': Array(17.095, dtype=float32), 'eval/episode_reward': Array(266.139, dtype=float32), 'eval/episode_reward_alive': Array(278.203, dtype=float32), 'eval/episode_reward_linvel': Array(17.095, dtype=float32), 'eval/episode_reward_quadctrl': Array(-29.159, dtype=float32), 'eval/episode_x_position': Array(7.186, dtype=float32), 'eval/episode_x_velocity': Array(13.676, dtype=float32), 'eval/episode_y_position': Array(0.501, dtype=float32), 'eval/episode_y_velocity': Array(0.301, dtype=float32), 'eval/episode_distance_from_origin_std': Array(9.179, dtype=float32), 'eval/episode_forward_reward_std': Array(14.028, dtype=float32), 'eval/episode_reward_std': Array(44.903, dtype=float32), 'eval/episode_reward_alive_std': Array(48.897, dtype=float32), 'eval/episode_reward_linvel_std': Array(14.028, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(5.588, dtype=float32), 'eval/episode_x_position_std': Array(3.965, dtype=float32), 'eval/episode_x_velocity_std': Array(11.223, dtype=float32), 'eval/episode_y_position_std': Array(5.044, dtype=float32), 'eval/episode_y_velocity_std': Array(11.645, dtype=float32), 'eval/avg_episode_length': Array(55.641, dtype=float32), 'eval/epoch_eval_time': 10.454393148422241, 'eval/sps': 12243.656631501135}
{'eval/walltime': 69.12686514854431, 'training/sps': 48881.43684919467, 'training/walltime': 180.43644881248474, 'training/entropy_loss': Array(-0.01, dtype=float32), 'training/policy_loss': Array(-0.01, dtype=float32), 'training/total_loss': Array(0.019, dtype=float32), 'training/v_loss': Array(0.04, dtype=float32), 'eval/episode_distance_from_origin': Array(58.969, dtype=float32), 'eval/episode_forward_reward': Array(22.509, dtype=float32), 'eval/episode_reward': Array(309.895, dtype=float32), 'eval/episode_reward_alive': Array(318.359, dtype=float32), 'eval/episode_reward_linvel': Array(22.509, dtype=float32), 'eval/episode_reward_quadctrl': Array(-30.974, dtype=float32), 'eval/episode_x_position': Array(10.592, dtype=float32), 'eval/episode_x_velocity': Array(18.008, dtype=float32), 'eval/episode_y_position': Array(2.765, dtype=float32), 'eval/episode_y_velocity': Array(4.484, dtype=float32), 'eval/episode_distance_from_origin_std': Array(11.5, dtype=float32), 'eval/episode_forward_reward_std': Array(16.388, dtype=float32), 'eval/episode_reward_std': Array(55.786, dtype=float32), 'eval/episode_reward_alive_std': Array(60.883, dtype=float32), 'eval/episode_reward_linvel_std': Array(16.388, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(6.504, dtype=float32), 'eval/episode_x_position_std': Array(5.494, dtype=float32), 'eval/episode_x_velocity_std': Array(13.111, dtype=float32), 'eval/episode_y_position_std': Array(4.954, dtype=float32), 'eval/episode_y_velocity_std': Array(9.743, dtype=float32), 'eval/avg_episode_length': Array(63.672, dtype=float32), 'eval/epoch_eval_time': 10.44561243057251, 'eval/sps': 12253.948808723368}
{'eval/walltime': 79.51220226287842, 'training/sps': 48808.977557865015, 'training/walltime': 234.14460110664368, 'training/entropy_loss': Array(-0.009, dtype=float32), 'training/policy_loss': Array(-0.01, dtype=float32), 'training/total_loss': Array(0.017, dtype=float32), 'training/v_loss': Array(0.036, dtype=float32), 'eval/episode_distance_from_origin': Array(70.993, dtype=float32), 'eval/episode_forward_reward': Array(27.153, dtype=float32), 'eval/episode_reward': Array(369.864, dtype=float32), 'eval/episode_reward_alive': Array(377.891, dtype=float32), 'eval/episode_reward_linvel': Array(27.153, dtype=float32), 'eval/episode_reward_quadctrl': Array(-35.179, dtype=float32), 'eval/episode_x_position': Array(15.446, dtype=float32), 'eval/episode_x_velocity': Array(21.722, dtype=float32), 'eval/episode_y_position': Array(3.535, dtype=float32), 'eval/episode_y_velocity': Array(3.991, dtype=float32), 'eval/episode_distance_from_origin_std': Array(14.488, dtype=float32), 'eval/episode_forward_reward_std': Array(16.955, dtype=float32), 'eval/episode_reward_std': Array(66.947, dtype=float32), 'eval/episode_reward_alive_std': Array(74.348, dtype=float32), 'eval/episode_reward_linvel_std': Array(16.955, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(7.854, dtype=float32), 'eval/episode_x_position_std': Array(7.73, dtype=float32), 'eval/episode_x_velocity_std': Array(13.564, dtype=float32), 'eval/episode_y_position_std': Array(6.532, dtype=float32), 'eval/episode_y_velocity_std': Array(11.909, dtype=float32), 'eval/avg_episode_length': Array(75.578, dtype=float32), 'eval/epoch_eval_time': 10.385337114334106, 'eval/sps': 12325.069334853959}
time to jit: 0:00:51.281343
time to train: 0:04:35.978384

--------

Iteration0-Robot3

--------

{'eval/walltime': 47.98970937728882, 'training/sps': 35672.20044549971, 'training/walltime': 73.48691606521606, 'training/entropy_loss': Array(-0.013, dtype=float32), 'training/policy_loss': Array(0., dtype=float32), 'training/total_loss': Array(0.117, dtype=float32), 'training/v_loss': Array(0.13, dtype=float32), 'eval/episode_distance_from_origin': Array(34.354, dtype=float32), 'eval/episode_forward_reward': Array(6.431, dtype=float32), 'eval/episode_reward': Array(190.799, dtype=float32), 'eval/episode_reward_alive': Array(207.734, dtype=float32), 'eval/episode_reward_linvel': Array(6.431, dtype=float32), 'eval/episode_reward_quadctrl': Array(-23.367, dtype=float32), 'eval/episode_x_position': Array(2.562, dtype=float32), 'eval/episode_x_velocity': Array(5.145, dtype=float32), 'eval/episode_y_position': Array(0.597, dtype=float32), 'eval/episode_y_velocity': Array(0.915, dtype=float32), 'eval/episode_distance_from_origin_std': Array(8.661, dtype=float32), 'eval/episode_forward_reward_std': Array(9.329, dtype=float32), 'eval/episode_reward_std': Array(47.498, dtype=float32), 'eval/episode_reward_alive_std': Array(52.878, dtype=float32), 'eval/episode_reward_linvel_std': Array(9.329, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(6.266, dtype=float32), 'eval/episode_x_position_std': Array(2.814, dtype=float32), 'eval/episode_x_velocity_std': Array(7.463, dtype=float32), 'eval/episode_y_position_std': Array(2.812, dtype=float32), 'eval/episode_y_velocity_std': Array(8.564, dtype=float32), 'eval/avg_episode_length': Array(41.547, dtype=float32), 'eval/epoch_eval_time': 10.309342622756958, 'eval/sps': 12415.922594079992}
{'eval/walltime': 58.3286828994751, 'training/sps': 48973.22289081953, 'training/walltime': 127.01494312286377, 'training/entropy_loss': Array(-0.011, dtype=float32), 'training/policy_loss': Array(-0.015, dtype=float32), 'training/total_loss': Array(0.033, dtype=float32), 'training/v_loss': Array(0.059, dtype=float32), 'eval/episode_distance_from_origin': Array(44.8, dtype=float32), 'eval/episode_forward_reward': Array(12.106, dtype=float32), 'eval/episode_reward': Array(251.731, dtype=float32), 'eval/episode_reward_alive': Array(267.734, dtype=float32), 'eval/episode_reward_linvel': Array(12.106, dtype=float32), 'eval/episode_reward_quadctrl': Array(-28.11, dtype=float32), 'eval/episode_x_position': Array(5.156, dtype=float32), 'eval/episode_x_velocity': Array(9.685, dtype=float32), 'eval/episode_y_position': Array(1.032, dtype=float32), 'eval/episode_y_velocity': Array(1.968, dtype=float32), 'eval/episode_distance_from_origin_std': Array(7.25, dtype=float32), 'eval/episode_forward_reward_std': Array(12.503, dtype=float32), 'eval/episode_reward_std': Array(38.291, dtype=float32), 'eval/episode_reward_alive_std': Array(44.834, dtype=float32), 'eval/episode_reward_linvel_std': Array(12.503, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(5.079, dtype=float32), 'eval/episode_x_position_std': Array(3.18, dtype=float32), 'eval/episode_x_velocity_std': Array(10.003, dtype=float32), 'eval/episode_y_position_std': Array(3.675, dtype=float32), 'eval/episode_y_velocity_std': Array(9.028, dtype=float32), 'eval/avg_episode_length': Array(53.547, dtype=float32), 'eval/epoch_eval_time': 10.33897352218628, 'eval/sps': 12380.339278877766}
{'eval/walltime': 68.68371438980103, 'training/sps': 48843.51388988007, 'training/walltime': 180.68511939048767, 'training/entropy_loss': Array(-0.01, dtype=float32), 'training/policy_loss': Array(-0.011, dtype=float32), 'training/total_loss': Array(0.027, dtype=float32), 'training/v_loss': Array(0.048, dtype=float32), 'eval/episode_distance_from_origin': Array(51.419, dtype=float32), 'eval/episode_forward_reward': Array(17.902, dtype=float32), 'eval/episode_reward': Array(292.398, dtype=float32), 'eval/episode_reward_alive': Array(304.102, dtype=float32), 'eval/episode_reward_linvel': Array(17.902, dtype=float32), 'eval/episode_reward_quadctrl': Array(-29.606, dtype=float32), 'eval/episode_x_position': Array(8.078, dtype=float32), 'eval/episode_x_velocity': Array(14.322, dtype=float32), 'eval/episode_y_position': Array(1.505, dtype=float32), 'eval/episode_y_velocity': Array(2.733, dtype=float32), 'eval/episode_distance_from_origin_std': Array(11.76, dtype=float32), 'eval/episode_forward_reward_std': Array(12.92, dtype=float32), 'eval/episode_reward_std': Array(63.132, dtype=float32), 'eval/episode_reward_alive_std': Array(70.693, dtype=float32), 'eval/episode_reward_linvel_std': Array(12.92, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(7.473, dtype=float32), 'eval/episode_x_position_std': Array(4.561, dtype=float32), 'eval/episode_x_velocity_std': Array(10.336, dtype=float32), 'eval/episode_y_position_std': Array(3.653, dtype=float32), 'eval/episode_y_velocity_std': Array(7.771, dtype=float32), 'eval/avg_episode_length': Array(60.82, dtype=float32), 'eval/epoch_eval_time': 10.355031490325928, 'eval/sps': 12361.140583645985}
{'eval/walltime': 79.03445076942444, 'training/sps': 48825.78765191881, 'training/walltime': 234.37478065490723, 'training/entropy_loss': Array(-0.009, dtype=float32), 'training/policy_loss': Array(-0.011, dtype=float32), 'training/total_loss': Array(0.021, dtype=float32), 'training/v_loss': Array(0.041, dtype=float32), 'eval/episode_distance_from_origin': Array(63.876, dtype=float32), 'eval/episode_forward_reward': Array(20.979, dtype=float32), 'eval/episode_reward': Array(361.17, dtype=float32), 'eval/episode_reward_alive': Array(375.234, dtype=float32), 'eval/episode_reward_linvel': Array(20.979, dtype=float32), 'eval/episode_reward_quadctrl': Array(-35.043, dtype=float32), 'eval/episode_x_position': Array(11.606, dtype=float32), 'eval/episode_x_velocity': Array(16.783, dtype=float32), 'eval/episode_y_position': Array(3.641, dtype=float32), 'eval/episode_y_velocity': Array(3.928, dtype=float32), 'eval/episode_distance_from_origin_std': Array(13.07, dtype=float32), 'eval/episode_forward_reward_std': Array(14.989, dtype=float32), 'eval/episode_reward_std': Array(69.772, dtype=float32), 'eval/episode_reward_alive_std': Array(75.663, dtype=float32), 'eval/episode_reward_linvel_std': Array(14.989, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(7.618, dtype=float32), 'eval/episode_x_position_std': Array(5.968, dtype=float32), 'eval/episode_x_velocity_std': Array(11.991, dtype=float32), 'eval/episode_y_position_std': Array(5.402, dtype=float32), 'eval/episode_y_velocity_std': Array(9.178, dtype=float32), 'eval/avg_episode_length': Array(75.047, dtype=float32), 'eval/epoch_eval_time': 10.350736379623413, 'eval/sps': 12366.26992568204}
time to jit: 0:00:51.678354
time to train: 0:04:35.860541
1

--------

Iteration1-Robot0

--------

{'eval/walltime': 48.50182247161865, 'training/sps': 35622.70612491563, 'training/walltime': 73.58901906013489, 'training/entropy_loss': Array(-0.011, dtype=float32), 'training/policy_loss': Array(0.012, dtype=float32), 'training/total_loss': Array(0.097, dtype=float32), 'training/v_loss': Array(0.096, dtype=float32), 'eval/episode_distance_from_origin': Array(42.67, dtype=float32), 'eval/episode_forward_reward': Array(15.121, dtype=float32), 'eval/episode_reward': Array(235.351, dtype=float32), 'eval/episode_reward_alive': Array(248.945, dtype=float32), 'eval/episode_reward_linvel': Array(15.121, dtype=float32), 'eval/episode_reward_quadctrl': Array(-28.715, dtype=float32), 'eval/episode_x_position': Array(6.335, dtype=float32), 'eval/episode_x_velocity': Array(12.097, dtype=float32), 'eval/episode_y_position': Array(1.194, dtype=float32), 'eval/episode_y_velocity': Array(1.847, dtype=float32), 'eval/episode_distance_from_origin_std': Array(9.29, dtype=float32), 'eval/episode_forward_reward_std': Array(10.371, dtype=float32), 'eval/episode_reward_std': Array(47.862, dtype=float32), 'eval/episode_reward_alive_std': Array(53.567, dtype=float32), 'eval/episode_reward_linvel_std': Array(10.371, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(6.176, dtype=float32), 'eval/episode_x_position_std': Array(3.143, dtype=float32), 'eval/episode_x_velocity_std': Array(8.297, dtype=float32), 'eval/episode_y_position_std': Array(3.726, dtype=float32), 'eval/episode_y_velocity_std': Array(9.82, dtype=float32), 'eval/avg_episode_length': Array(49.789, dtype=float32), 'eval/epoch_eval_time': 10.358187198638916, 'eval/sps': 12357.374658841793}
{'eval/walltime': 58.92548751831055, 'training/sps': 48980.98806130555, 'training/walltime': 127.10856008529663, 'training/entropy_loss': Array(-0.01, dtype=float32), 'training/policy_loss': Array(-0.014, dtype=float32), 'training/total_loss': Array(0.033, dtype=float32), 'training/v_loss': Array(0.057, dtype=float32), 'eval/episode_distance_from_origin': Array(61.313, dtype=float32), 'eval/episode_forward_reward': Array(25.097, dtype=float32), 'eval/episode_reward': Array(332.274, dtype=float32), 'eval/episode_reward_alive': Array(345.039, dtype=float32), 'eval/episode_reward_linvel': Array(25.097, dtype=float32), 'eval/episode_reward_quadctrl': Array(-37.862, dtype=float32), 'eval/episode_x_position': Array(15.301, dtype=float32), 'eval/episode_x_velocity': Array(20.078, dtype=float32), 'eval/episode_y_position': Array(0.565, dtype=float32), 'eval/episode_y_velocity': Array(2.296, dtype=float32), 'eval/episode_distance_from_origin_std': Array(12.914, dtype=float32), 'eval/episode_forward_reward_std': Array(16.333, dtype=float32), 'eval/episode_reward_std': Array(62.352, dtype=float32), 'eval/episode_reward_alive_std': Array(68.518, dtype=float32), 'eval/episode_reward_linvel_std': Array(16.333, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(7.543, dtype=float32), 'eval/episode_x_position_std': Array(8.026, dtype=float32), 'eval/episode_x_velocity_std': Array(13.067, dtype=float32), 'eval/episode_y_position_std': Array(5.811, dtype=float32), 'eval/episode_y_velocity_std': Array(11.817, dtype=float32), 'eval/avg_episode_length': Array(69.008, dtype=float32), 'eval/epoch_eval_time': 10.423665046691895, 'eval/sps': 12279.749917772224}
{'eval/walltime': 69.27779984474182, 'training/sps': 48762.50185151413, 'training/walltime': 180.867901802063, 'training/entropy_loss': Array(-0.009, dtype=float32), 'training/policy_loss': Array(-0.013, dtype=float32), 'training/total_loss': Array(0.042, dtype=float32), 'training/v_loss': Array(0.064, dtype=float32), 'eval/episode_distance_from_origin': Array(88.678, dtype=float32), 'eval/episode_forward_reward': Array(45.794, dtype=float32), 'eval/episode_reward': Array(446.797, dtype=float32), 'eval/episode_reward_alive': Array(448.867, dtype=float32), 'eval/episode_reward_linvel': Array(45.794, dtype=float32), 'eval/episode_reward_quadctrl': Array(-47.864, dtype=float32), 'eval/episode_x_position': Array(38.419, dtype=float32), 'eval/episode_x_velocity': Array(36.635, dtype=float32), 'eval/episode_y_position': Array(-0.685, dtype=float32), 'eval/episode_y_velocity': Array(-0.234, dtype=float32), 'eval/episode_distance_from_origin_std': Array(30.824, dtype=float32), 'eval/episode_forward_reward_std': Array(19.871, dtype=float32), 'eval/episode_reward_std': Array(114.103, dtype=float32), 'eval/episode_reward_alive_std': Array(117.282, dtype=float32), 'eval/episode_reward_linvel_std': Array(19.871, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(12.956, dtype=float32), 'eval/episode_x_position_std': Array(24.411, dtype=float32), 'eval/episode_x_velocity_std': Array(15.897, dtype=float32), 'eval/episode_y_position_std': Array(8.362, dtype=float32), 'eval/episode_y_velocity_std': Array(14.855, dtype=float32), 'eval/avg_episode_length': Array(89.773, dtype=float32), 'eval/epoch_eval_time': 10.352312326431274, 'eval/sps': 12364.387391326427}
{'eval/walltime': 79.56622171401978, 'training/sps': 48723.44906541946, 'training/walltime': 234.6703326702118, 'training/entropy_loss': Array(-0.008, dtype=float32), 'training/policy_loss': Array(-0.011, dtype=float32), 'training/total_loss': Array(0.058, dtype=float32), 'training/v_loss': Array(0.077, dtype=float32), 'eval/episode_distance_from_origin': Array(168.004, dtype=float32), 'eval/episode_forward_reward': Array(79.482, dtype=float32), 'eval/episode_reward': Array(667.717, dtype=float32), 'eval/episode_reward_alive': Array(657.383, dtype=float32), 'eval/episode_reward_linvel': Array(79.482, dtype=float32), 'eval/episode_reward_quadctrl': Array(-69.147, dtype=float32), 'eval/episode_x_position': Array(108.932, dtype=float32), 'eval/episode_x_velocity': Array(63.585, dtype=float32), 'eval/episode_y_position': Array(-8.69, dtype=float32), 'eval/episode_y_velocity': Array(-5.084, dtype=float32), 'eval/episode_distance_from_origin_std': Array(122.168, dtype=float32), 'eval/episode_forward_reward_std': Array(35.958, dtype=float32), 'eval/episode_reward_std': Array(244.646, dtype=float32), 'eval/episode_reward_alive_std': Array(244.37, dtype=float32), 'eval/episode_reward_linvel_std': Array(35.958, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(25.54, dtype=float32), 'eval/episode_x_position_std': Array(112.634, dtype=float32), 'eval/episode_x_velocity_std': Array(28.767, dtype=float32), 'eval/episode_y_position_std': Array(30.355, dtype=float32), 'eval/episode_y_velocity_std': Array(20.136, dtype=float32), 'eval/avg_episode_length': Array(131.477, dtype=float32), 'eval/epoch_eval_time': 10.288421869277954, 'eval/sps': 12441.169464698778}
time to jit: 0:00:51.208398
time to train: 0:04:36.212936

--------

Iteration1-Robot1

--------

{'eval/walltime': 49.1886682510376, 'training/sps': 34934.806217567195, 'training/walltime': 75.03805756568909, 'training/entropy_loss': Array(-0.001, dtype=float32), 'training/policy_loss': Array(0.074, dtype=float32), 'training/total_loss': Array(0.135, dtype=float32), 'training/v_loss': Array(0.062, dtype=float32), 'eval/episode_distance_from_origin': Array(23.585, dtype=float32), 'eval/episode_forward_reward': Array(6.619, dtype=float32), 'eval/episode_reward': Array(120.148, dtype=float32), 'eval/episode_reward_alive': Array(132.305, dtype=float32), 'eval/episode_reward_linvel': Array(6.619, dtype=float32), 'eval/episode_reward_quadctrl': Array(-18.776, dtype=float32), 'eval/episode_x_position': Array(1.513, dtype=float32), 'eval/episode_x_velocity': Array(5.296, dtype=float32), 'eval/episode_y_position': Array(-0.623, dtype=float32), 'eval/episode_y_velocity': Array(-3.763, dtype=float32), 'eval/episode_distance_from_origin_std': Array(5.26, dtype=float32), 'eval/episode_forward_reward_std': Array(5.728, dtype=float32), 'eval/episode_reward_std': Array(25.555, dtype=float32), 'eval/episode_reward_alive_std': Array(28.688, dtype=float32), 'eval/episode_reward_linvel_std': Array(5.728, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(4.149, dtype=float32), 'eval/episode_x_position_std': Array(1.095, dtype=float32), 'eval/episode_x_velocity_std': Array(4.582, dtype=float32), 'eval/episode_y_position_std': Array(1.008, dtype=float32), 'eval/episode_y_velocity_std': Array(4.285, dtype=float32), 'eval/avg_episode_length': Array(26.461, dtype=float32), 'eval/epoch_eval_time': 10.431236505508423, 'eval/sps': 12270.836725100331}
time to jit: 0:00:52.841354
time to train: 0:01:25.516210

--------

Iteration1-Robot2

--------

{'eval/walltime': 48.80675554275513, 'training/sps': 35121.30153650316, 'training/walltime': 74.63960289955139, 'training/entropy_loss': Array(-0.012, dtype=float32), 'training/policy_loss': Array(0.008, dtype=float32), 'training/total_loss': Array(0.111, dtype=float32), 'training/v_loss': Array(0.115, dtype=float32), 'eval/episode_distance_from_origin': Array(38.103, dtype=float32), 'eval/episode_forward_reward': Array(12.514, dtype=float32), 'eval/episode_reward': Array(199.645, dtype=float32), 'eval/episode_reward_alive': Array(213.281, dtype=float32), 'eval/episode_reward_linvel': Array(12.514, dtype=float32), 'eval/episode_reward_quadctrl': Array(-26.15, dtype=float32), 'eval/episode_x_position': Array(4.284, dtype=float32), 'eval/episode_x_velocity': Array(10.011, dtype=float32), 'eval/episode_y_position': Array(-0.236, dtype=float32), 'eval/episode_y_velocity': Array(-1.288, dtype=float32), 'eval/episode_distance_from_origin_std': Array(7.973, dtype=float32), 'eval/episode_forward_reward_std': Array(10.433, dtype=float32), 'eval/episode_reward_std': Array(41.07, dtype=float32), 'eval/episode_reward_alive_std': Array(43.855, dtype=float32), 'eval/episode_reward_linvel_std': Array(10.433, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(5.593, dtype=float32), 'eval/episode_x_position_std': Array(3.025, dtype=float32), 'eval/episode_x_velocity_std': Array(8.347, dtype=float32), 'eval/episode_y_position_std': Array(3.208, dtype=float32), 'eval/episode_y_velocity_std': Array(9.495, dtype=float32), 'eval/avg_episode_length': Array(42.656, dtype=float32), 'eval/epoch_eval_time': 10.382330417633057, 'eval/sps': 12328.638643845165}
{'eval/walltime': 59.23271417617798, 'training/sps': 48998.31863684527, 'training/walltime': 128.1402142047882, 'training/entropy_loss': Array(-0.012, dtype=float32), 'training/policy_loss': Array(-0.016, dtype=float32), 'training/total_loss': Array(0.032, dtype=float32), 'training/v_loss': Array(0.06, dtype=float32), 'eval/episode_distance_from_origin': Array(56.481, dtype=float32), 'eval/episode_forward_reward': Array(27.93, dtype=float32), 'eval/episode_reward': Array(297.369, dtype=float32), 'eval/episode_reward_alive': Array(305.859, dtype=float32), 'eval/episode_reward_linvel': Array(27.93, dtype=float32), 'eval/episode_reward_quadctrl': Array(-36.42, dtype=float32), 'eval/episode_x_position': Array(13.049, dtype=float32), 'eval/episode_x_velocity': Array(22.344, dtype=float32), 'eval/episode_y_position': Array(0.177, dtype=float32), 'eval/episode_y_velocity': Array(0.3, dtype=float32), 'eval/episode_distance_from_origin_std': Array(13.563, dtype=float32), 'eval/episode_forward_reward_std': Array(15.583, dtype=float32), 'eval/episode_reward_std': Array(65.876, dtype=float32), 'eval/episode_reward_alive_std': Array(69.071, dtype=float32), 'eval/episode_reward_linvel_std': Array(15.583, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(8.249, dtype=float32), 'eval/episode_x_position_std': Array(7.029, dtype=float32), 'eval/episode_x_velocity_std': Array(12.466, dtype=float32), 'eval/episode_y_position_std': Array(4.887, dtype=float32), 'eval/episode_y_velocity_std': Array(10.543, dtype=float32), 'eval/avg_episode_length': Array(61.172, dtype=float32), 'eval/epoch_eval_time': 10.425958633422852, 'eval/sps': 12277.048519036516}
{'eval/walltime': 69.68003344535828, 'training/sps': 48844.35751155881, 'training/walltime': 181.80946350097656, 'training/entropy_loss': Array(-0.011, dtype=float32), 'training/policy_loss': Array(-0.016, dtype=float32), 'training/total_loss': Array(0.044, dtype=float32), 'training/v_loss': Array(0.071, dtype=float32), 'eval/episode_distance_from_origin': Array(98.884, dtype=float32), 'eval/episode_forward_reward': Array(59.914, dtype=float32), 'eval/episode_reward': Array(458.007, dtype=float32), 'eval/episode_reward_alive': Array(451.875, dtype=float32), 'eval/episode_reward_linvel': Array(59.914, dtype=float32), 'eval/episode_reward_quadctrl': Array(-53.783, dtype=float32), 'eval/episode_x_position': Array(47.104, dtype=float32), 'eval/episode_x_velocity': Array(47.931, dtype=float32), 'eval/episode_y_position': Array(0.991, dtype=float32), 'eval/episode_y_velocity': Array(1.537, dtype=float32), 'eval/episode_distance_from_origin_std': Array(39.043, dtype=float32), 'eval/episode_forward_reward_std': Array(29.898, dtype=float32), 'eval/episode_reward_std': Array(124.387, dtype=float32), 'eval/episode_reward_alive_std': Array(118.781, dtype=float32), 'eval/episode_reward_linvel_std': Array(29.898, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(14.159, dtype=float32), 'eval/episode_x_position_std': Array(34.03, dtype=float32), 'eval/episode_x_velocity_std': Array(23.919, dtype=float32), 'eval/episode_y_position_std': Array(12.317, dtype=float32), 'eval/episode_y_velocity_std': Array(17.317, dtype=float32), 'eval/avg_episode_length': Array(90.375, dtype=float32), 'eval/epoch_eval_time': 10.447319269180298, 'eval/sps': 12251.946810661884}
{'eval/walltime': 80.12322187423706, 'training/sps': 48770.8209848133, 'training/walltime': 235.55963516235352, 'training/entropy_loss': Array(-0.009, dtype=float32), 'training/policy_loss': Array(-0.013, dtype=float32), 'training/total_loss': Array(0.07, dtype=float32), 'training/v_loss': Array(0.092, dtype=float32), 'eval/episode_distance_from_origin': Array(259.352, dtype=float32), 'eval/episode_forward_reward': Array(143.622, dtype=float32), 'eval/episode_reward': Array(786.706, dtype=float32), 'eval/episode_reward_alive': Array(730.469, dtype=float32), 'eval/episode_reward_linvel': Array(143.622, dtype=float32), 'eval/episode_reward_quadctrl': Array(-87.385, dtype=float32), 'eval/episode_x_position': Array(200.398, dtype=float32), 'eval/episode_x_velocity': Array(114.898, dtype=float32), 'eval/episode_y_position': Array(2.551, dtype=float32), 'eval/episode_y_velocity': Array(4.137, dtype=float32), 'eval/episode_distance_from_origin_std': Array(188.271, dtype=float32), 'eval/episode_forward_reward_std': Array(66.132, dtype=float32), 'eval/episode_reward_std': Array(270.195, dtype=float32), 'eval/episode_reward_alive_std': Array(240.929, dtype=float32), 'eval/episode_reward_linvel_std': Array(66.132, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(29.381, dtype=float32), 'eval/episode_x_position_std': Array(182.268, dtype=float32), 'eval/episode_x_velocity_std': Array(52.906, dtype=float32), 'eval/episode_y_position_std': Array(37.821, dtype=float32), 'eval/episode_y_velocity_std': Array(28.171, dtype=float32), 'eval/avg_episode_length': Array(146.094, dtype=float32), 'eval/epoch_eval_time': 10.443188428878784, 'eval/sps': 12256.793111770225}
time to jit: 0:00:51.873782
time to train: 0:04:37.377680

--------

Iteration1-Robot3

--------

{'eval/walltime': 48.79586410522461, 'training/sps': 34600.87935705392, 'training/walltime': 75.76223635673523, 'training/entropy_loss': Array(-0.012, dtype=float32), 'training/policy_loss': Array(0.011, dtype=float32), 'training/total_loss': Array(0.088, dtype=float32), 'training/v_loss': Array(0.088, dtype=float32), 'eval/episode_distance_from_origin': Array(46.617, dtype=float32), 'eval/episode_forward_reward': Array(15.586, dtype=float32), 'eval/episode_reward': Array(250.588, dtype=float32), 'eval/episode_reward_alive': Array(260.977, dtype=float32), 'eval/episode_reward_linvel': Array(15.586, dtype=float32), 'eval/episode_reward_quadctrl': Array(-25.975, dtype=float32), 'eval/episode_x_position': Array(5.428, dtype=float32), 'eval/episode_x_velocity': Array(12.469, dtype=float32), 'eval/episode_y_position': Array(-0.05, dtype=float32), 'eval/episode_y_velocity': Array(-0.486, dtype=float32), 'eval/episode_distance_from_origin_std': Array(10.334, dtype=float32), 'eval/episode_forward_reward_std': Array(15.637, dtype=float32), 'eval/episode_reward_std': Array(49.259, dtype=float32), 'eval/episode_reward_alive_std': Array(57.009, dtype=float32), 'eval/episode_reward_linvel_std': Array(15.637, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(5.697, dtype=float32), 'eval/episode_x_position_std': Array(4.45, dtype=float32), 'eval/episode_x_velocity_std': Array(12.51, dtype=float32), 'eval/episode_y_position_std': Array(4.094, dtype=float32), 'eval/episode_y_velocity_std': Array(10.909, dtype=float32), 'eval/avg_episode_length': Array(52.195, dtype=float32), 'eval/epoch_eval_time': 10.44021201133728, 'eval/sps': 12260.287421462484}
{'eval/walltime': 59.34016466140747, 'training/sps': 48917.9720520144, 'training/walltime': 129.35072112083435, 'training/entropy_loss': Array(-0.01, dtype=float32), 'training/policy_loss': Array(-0.014, dtype=float32), 'training/total_loss': Array(0.015, dtype=float32), 'training/v_loss': Array(0.04, dtype=float32), 'eval/episode_distance_from_origin': Array(61.726, dtype=float32), 'eval/episode_forward_reward': Array(23.946, dtype=float32), 'eval/episode_reward': Array(329.404, dtype=float32), 'eval/episode_reward_alive': Array(337.539, dtype=float32), 'eval/episode_reward_linvel': Array(23.946, dtype=float32), 'eval/episode_reward_quadctrl': Array(-32.081, dtype=float32), 'eval/episode_x_position': Array(11.501, dtype=float32), 'eval/episode_x_velocity': Array(19.157, dtype=float32), 'eval/episode_y_position': Array(0.786, dtype=float32), 'eval/episode_y_velocity': Array(1.299, dtype=float32), 'eval/episode_distance_from_origin_std': Array(13.638, dtype=float32), 'eval/episode_forward_reward_std': Array(19.408, dtype=float32), 'eval/episode_reward_std': Array(68.29, dtype=float32), 'eval/episode_reward_alive_std': Array(72.298, dtype=float32), 'eval/episode_reward_linvel_std': Array(19.408, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(6.96, dtype=float32), 'eval/episode_x_position_std': Array(6.555, dtype=float32), 'eval/episode_x_velocity_std': Array(15.526, dtype=float32), 'eval/episode_y_position_std': Array(6.397, dtype=float32), 'eval/episode_y_velocity_std': Array(12.91, dtype=float32), 'eval/avg_episode_length': Array(67.508, dtype=float32), 'eval/epoch_eval_time': 10.544300556182861, 'eval/sps': 12139.259433850702}
{'eval/walltime': 69.75682187080383, 'training/sps': 48707.53099400853, 'training/walltime': 183.17073512077332, 'training/entropy_loss': Array(-0.009, dtype=float32), 'training/policy_loss': Array(-0.015, dtype=float32), 'training/total_loss': Array(0.021, dtype=float32), 'training/v_loss': Array(0.045, dtype=float32), 'eval/episode_distance_from_origin': Array(104.68, dtype=float32), 'eval/episode_forward_reward': Array(64.362, dtype=float32), 'eval/episode_reward': Array(500.721, dtype=float32), 'eval/episode_reward_alive': Array(482.344, dtype=float32), 'eval/episode_reward_linvel': Array(64.362, dtype=float32), 'eval/episode_reward_quadctrl': Array(-45.984, dtype=float32), 'eval/episode_x_position': Array(47.943, dtype=float32), 'eval/episode_x_velocity': Array(51.489, dtype=float32), 'eval/episode_y_position': Array(3.147, dtype=float32), 'eval/episode_y_velocity': Array(1.102, dtype=float32), 'eval/episode_distance_from_origin_std': Array(40.919, dtype=float32), 'eval/episode_forward_reward_std': Array(30.429, dtype=float32), 'eval/episode_reward_std': Array(131.788, dtype=float32), 'eval/episode_reward_alive_std': Array(123.437, dtype=float32), 'eval/episode_reward_linvel_std': Array(30.429, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(12.069, dtype=float32), 'eval/episode_x_position_std': Array(34.285, dtype=float32), 'eval/episode_x_velocity_std': Array(24.343, dtype=float32), 'eval/episode_y_position_std': Array(12.329, dtype=float32), 'eval/episode_y_velocity_std': Array(15.516, dtype=float32), 'eval/avg_episode_length': Array(96.469, dtype=float32), 'eval/epoch_eval_time': 10.416657209396362, 'eval/sps': 12288.011156260129}
{'eval/walltime': 80.24307799339294, 'training/sps': 48759.43074917854, 'training/walltime': 236.93346285820007, 'training/entropy_loss': Array(-0.007, dtype=float32), 'training/policy_loss': Array(-0.014, dtype=float32), 'training/total_loss': Array(0.048, dtype=float32), 'training/v_loss': Array(0.07, dtype=float32), 'eval/episode_distance_from_origin': Array(202.663, dtype=float32), 'eval/episode_forward_reward': Array(135.296, dtype=float32), 'eval/episode_reward': Array(726.63, dtype=float32), 'eval/episode_reward_alive': Array(654.844, dtype=float32), 'eval/episode_reward_linvel': Array(135.296, dtype=float32), 'eval/episode_reward_quadctrl': Array(-63.51, dtype=float32), 'eval/episode_x_position': Array(145.366, dtype=float32), 'eval/episode_x_velocity': Array(108.237, dtype=float32), 'eval/episode_y_position': Array(0.528, dtype=float32), 'eval/episode_y_velocity': Array(-0.826, dtype=float32), 'eval/episode_distance_from_origin_std': Array(87.312, dtype=float32), 'eval/episode_forward_reward_std': Array(45.514, dtype=float32), 'eval/episode_reward_std': Array(187.864, dtype=float32), 'eval/episode_reward_alive_std': Array(162.428, dtype=float32), 'eval/episode_reward_linvel_std': Array(45.514, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(15.86, dtype=float32), 'eval/episode_x_position_std': Array(81.498, dtype=float32), 'eval/episode_x_velocity_std': Array(36.411, dtype=float32), 'eval/episode_y_position_std': Array(21.126, dtype=float32), 'eval/episode_y_velocity_std': Array(24.725, dtype=float32), 'eval/avg_episode_length': Array(130.969, dtype=float32), 'eval/epoch_eval_time': 10.486256122589111, 'eval/sps': 12206.453714616702}
time to jit: 0:00:53.514444
time to train: 0:04:38.944318
2

--------

Iteration2-Robot0

--------

{'eval/walltime': 48.587677240371704, 'training/sps': 35718.44734360315, 'training/walltime': 73.39176797866821, 'training/entropy_loss': Array(-0.008, dtype=float32), 'training/policy_loss': Array(-0.018, dtype=float32), 'training/total_loss': Array(0.085, dtype=float32), 'training/v_loss': Array(0.112, dtype=float32), 'eval/episode_distance_from_origin': Array(550.587, dtype=float32), 'eval/episode_forward_reward': Array(215.948, dtype=float32), 'eval/episode_reward': Array(1089.606, dtype=float32), 'eval/episode_reward_alive': Array(988.633, dtype=float32), 'eval/episode_reward_linvel': Array(215.948, dtype=float32), 'eval/episode_reward_quadctrl': Array(-114.975, dtype=float32), 'eval/episode_x_position': Array(478.273, dtype=float32), 'eval/episode_x_velocity': Array(172.759, dtype=float32), 'eval/episode_y_position': Array(58.71, dtype=float32), 'eval/episode_y_velocity': Array(19.409, dtype=float32), 'eval/episode_distance_from_origin_std': Array(798.737, dtype=float32), 'eval/episode_forward_reward_std': Array(126.547, dtype=float32), 'eval/episode_reward_std': Array(561.574, dtype=float32), 'eval/episode_reward_alive_std': Array(497.616, dtype=float32), 'eval/episode_reward_linvel_std': Array(126.547, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(58.771, dtype=float32), 'eval/episode_x_position_std': Array(783.012, dtype=float32), 'eval/episode_x_velocity_std': Array(101.238, dtype=float32), 'eval/episode_y_position_std': Array(140.927, dtype=float32), 'eval/episode_y_velocity_std': Array(39.168, dtype=float32), 'eval/avg_episode_length': Array(197.727, dtype=float32), 'eval/epoch_eval_time': 10.395586013793945, 'eval/sps': 12312.918177980182}
{'eval/walltime': 59.01784610748291, 'training/sps': 49039.065818307085, 'training/walltime': 126.84792494773865, 'training/entropy_loss': Array(-0.007, dtype=float32), 'training/policy_loss': Array(-0.012, dtype=float32), 'training/total_loss': Array(0.071, dtype=float32), 'training/v_loss': Array(0.09, dtype=float32), 'eval/episode_distance_from_origin': Array(4899.184, dtype=float32), 'eval/episode_forward_reward': Array(750.94, dtype=float32), 'eval/episode_reward': Array(2833.986, dtype=float32), 'eval/episode_reward_alive': Array(2361.68, dtype=float32), 'eval/episode_reward_linvel': Array(750.94, dtype=float32), 'eval/episode_reward_quadctrl': Array(-278.633, dtype=float32), 'eval/episode_x_position': Array(4809.397, dtype=float32), 'eval/episode_x_velocity': Array(600.753, dtype=float32), 'eval/episode_y_position': Array(309.504, dtype=float32), 'eval/episode_y_velocity': Array(28.457, dtype=float32), 'eval/episode_distance_from_origin_std': Array(5612.053, dtype=float32), 'eval/episode_forward_reward_std': Array(507.011, dtype=float32), 'eval/episode_reward_std': Array(1822.76, dtype=float32), 'eval/episode_reward_alive_std': Array(1495.015, dtype=float32), 'eval/episode_reward_linvel_std': Array(507.011, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(177.462, dtype=float32), 'eval/episode_x_position_std': Array(5576.673, dtype=float32), 'eval/episode_x_velocity_std': Array(405.61, dtype=float32), 'eval/episode_y_position_std': Array(607.481, dtype=float32), 'eval/episode_y_velocity_std': Array(58.738, dtype=float32), 'eval/avg_episode_length': Array(472.336, dtype=float32), 'eval/epoch_eval_time': 10.430168867111206, 'eval/sps': 12272.092775373401}
{'eval/walltime': 69.45407152175903, 'training/sps': 48831.70971544578, 'training/walltime': 180.53107500076294, 'training/entropy_loss': Array(-0.005, dtype=float32), 'training/policy_loss': Array(-0.01, dtype=float32), 'training/total_loss': Array(0.049, dtype=float32), 'training/v_loss': Array(0.065, dtype=float32), 'eval/episode_distance_from_origin': Array(9610.786, dtype=float32), 'eval/episode_forward_reward': Array(1308.147, dtype=float32), 'eval/episode_reward': Array(3988.885, dtype=float32), 'eval/episode_reward_alive': Array(3057.188, dtype=float32), 'eval/episode_reward_linvel': Array(1308.147, dtype=float32), 'eval/episode_reward_quadctrl': Array(-376.449, dtype=float32), 'eval/episode_x_position': Array(9531.371, dtype=float32), 'eval/episode_x_velocity': Array(1046.52, dtype=float32), 'eval/episode_y_position': Array(-11.97, dtype=float32), 'eval/episode_y_velocity': Array(-5.144, dtype=float32), 'eval/episode_distance_from_origin_std': Array(7932.172, dtype=float32), 'eval/episode_forward_reward_std': Array(674.48, dtype=float32), 'eval/episode_reward_std': Array(2001.564, dtype=float32), 'eval/episode_reward_alive_std': Array(1514.927, dtype=float32), 'eval/episode_reward_linvel_std': Array(674.48, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(186.389, dtype=float32), 'eval/episode_x_position_std': Array(7915.385, dtype=float32), 'eval/episode_x_velocity_std': Array(539.586, dtype=float32), 'eval/episode_y_position_std': Array(638.298, dtype=float32), 'eval/episode_y_velocity_std': Array(58.272, dtype=float32), 'eval/avg_episode_length': Array(611.438, dtype=float32), 'eval/epoch_eval_time': 10.436225414276123, 'eval/sps': 12264.970803036102}
{'eval/walltime': 79.84278392791748, 'training/sps': 48794.28761024691, 'training/walltime': 234.25539660453796, 'training/entropy_loss': Array(-0.003, dtype=float32), 'training/policy_loss': Array(-0.009, dtype=float32), 'training/total_loss': Array(0.057, dtype=float32), 'training/v_loss': Array(0.069, dtype=float32), 'eval/episode_distance_from_origin': Array(16435.75, dtype=float32), 'eval/episode_forward_reward': Array(1956.903, dtype=float32), 'eval/episode_reward': Array(5154.746, dtype=float32), 'eval/episode_reward_alive': Array(3668.125, dtype=float32), 'eval/episode_reward_linvel': Array(1956.903, dtype=float32), 'eval/episode_reward_quadctrl': Array(-470.281, dtype=float32), 'eval/episode_x_position': Array(16358.339, dtype=float32), 'eval/episode_x_velocity': Array(1565.527, dtype=float32), 'eval/episode_y_position': Array(-389.348, dtype=float32), 'eval/episode_y_velocity': Array(-38.72, dtype=float32), 'eval/episode_distance_from_origin_std': Array(10524.063, dtype=float32), 'eval/episode_forward_reward_std': Array(877.282, dtype=float32), 'eval/episode_reward_std': Array(2249.437, dtype=float32), 'eval/episode_reward_alive_std': Array(1575.945, dtype=float32), 'eval/episode_reward_linvel_std': Array(877.282, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(202.989, dtype=float32), 'eval/episode_x_position_std': Array(10508.334, dtype=float32), 'eval/episode_x_velocity_std': Array(701.829, dtype=float32), 'eval/episode_y_position_std': Array(774.714, dtype=float32), 'eval/episode_y_velocity_std': Array(60.178, dtype=float32), 'eval/avg_episode_length': Array(733.625, dtype=float32), 'eval/epoch_eval_time': 10.388712406158447, 'eval/sps': 12321.064920819386}
time to jit: 0:00:52.150098
time to train: 0:04:36.026501

--------

Iteration2-Robot1

--------

{'eval/walltime': 50.531001329422, 'training/sps': 35697.39575547811, 'training/walltime': 73.43504881858826, 'training/entropy_loss': Array(-0.009, dtype=float32), 'training/policy_loss': Array(0.01, dtype=float32), 'training/total_loss': Array(0.103, dtype=float32), 'training/v_loss': Array(0.102, dtype=float32), 'eval/episode_distance_from_origin': Array(51.651, dtype=float32), 'eval/episode_forward_reward': Array(26.955, dtype=float32), 'eval/episode_reward': Array(280.595, dtype=float32), 'eval/episode_reward_alive': Array(283.828, dtype=float32), 'eval/episode_reward_linvel': Array(26.955, dtype=float32), 'eval/episode_reward_quadctrl': Array(-30.189, dtype=float32), 'eval/episode_x_position': Array(11.428, dtype=float32), 'eval/episode_x_velocity': Array(21.564, dtype=float32), 'eval/episode_y_position': Array(1.556, dtype=float32), 'eval/episode_y_velocity': Array(1.64, dtype=float32), 'eval/episode_distance_from_origin_std': Array(12.244, dtype=float32), 'eval/episode_forward_reward_std': Array(16.112, dtype=float32), 'eval/episode_reward_std': Array(59.964, dtype=float32), 'eval/episode_reward_alive_std': Array(61.946, dtype=float32), 'eval/episode_reward_linvel_std': Array(16.112, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(6.965, dtype=float32), 'eval/episode_x_position_std': Array(7.591, dtype=float32), 'eval/episode_x_velocity_std': Array(12.889, dtype=float32), 'eval/episode_y_position_std': Array(4.656, dtype=float32), 'eval/episode_y_velocity_std': Array(10.723, dtype=float32), 'eval/avg_episode_length': Array(56.766, dtype=float32), 'eval/epoch_eval_time': 10.428811311721802, 'eval/sps': 12273.690277254344}
time to jit: 0:00:53.923852
time to train: 0:01:23.912020

--------

Iteration2-Robot2

--------

{'eval/walltime': 49.45749759674072, 'training/sps': 35071.044805262725, 'training/walltime': 74.74656128883362, 'training/entropy_loss': Array(-0.009, dtype=float32), 'training/policy_loss': Array(0.014, dtype=float32), 'training/total_loss': Array(0.114, dtype=float32), 'training/v_loss': Array(0.109, dtype=float32), 'eval/episode_distance_from_origin': Array(62.216, dtype=float32), 'eval/episode_forward_reward': Array(41.758, dtype=float32), 'eval/episode_reward': Array(324.015, dtype=float32), 'eval/episode_reward_alive': Array(315.312, dtype=float32), 'eval/episode_reward_linvel': Array(41.758, dtype=float32), 'eval/episode_reward_quadctrl': Array(-33.055, dtype=float32), 'eval/episode_x_position': Array(21.854, dtype=float32), 'eval/episode_x_velocity': Array(33.406, dtype=float32), 'eval/episode_y_position': Array(3.622, dtype=float32), 'eval/episode_y_velocity': Array(4.671, dtype=float32), 'eval/episode_distance_from_origin_std': Array(18.229, dtype=float32), 'eval/episode_forward_reward_std': Array(20.494, dtype=float32), 'eval/episode_reward_std': Array(75.544, dtype=float32), 'eval/episode_reward_alive_std': Array(72.243, dtype=float32), 'eval/episode_reward_linvel_std': Array(20.494, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(8.204, dtype=float32), 'eval/episode_x_position_std': Array(13.254, dtype=float32), 'eval/episode_x_velocity_std': Array(16.395, dtype=float32), 'eval/episode_y_position_std': Array(7.009, dtype=float32), 'eval/episode_y_velocity_std': Array(12.597, dtype=float32), 'eval/avg_episode_length': Array(63.062, dtype=float32), 'eval/epoch_eval_time': 10.301668405532837, 'eval/sps': 12425.171822775188}
time to jit: 0:00:54.479168
time to train: 0:01:25.095317

--------

Iteration2-Robot3

--------

{'eval/walltime': 50.14872670173645, 'training/sps': 35432.386206695286, 'training/walltime': 73.9842917919159, 'training/entropy_loss': Array(-0.008, dtype=float32), 'training/policy_loss': Array(0.016, dtype=float32), 'training/total_loss': Array(0.146, dtype=float32), 'training/v_loss': Array(0.138, dtype=float32), 'eval/episode_distance_from_origin': Array(77.099, dtype=float32), 'eval/episode_forward_reward': Array(69.261, dtype=float32), 'eval/episode_reward': Array(375.739, dtype=float32), 'eval/episode_reward_alive': Array(344.961, dtype=float32), 'eval/episode_reward_linvel': Array(69.261, dtype=float32), 'eval/episode_reward_quadctrl': Array(-38.483, dtype=float32), 'eval/episode_x_position': Array(37.077, dtype=float32), 'eval/episode_x_velocity': Array(55.409, dtype=float32), 'eval/episode_y_position': Array(-3.497, dtype=float32), 'eval/episode_y_velocity': Array(-8.835, dtype=float32), 'eval/episode_distance_from_origin_std': Array(24.839, dtype=float32), 'eval/episode_forward_reward_std': Array(26.474, dtype=float32), 'eval/episode_reward_std': Array(86.948, dtype=float32), 'eval/episode_reward_alive_std': Array(73.326, dtype=float32), 'eval/episode_reward_linvel_std': Array(26.474, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(9.022, dtype=float32), 'eval/episode_x_position_std': Array(21.43, dtype=float32), 'eval/episode_x_velocity_std': Array(21.18, dtype=float32), 'eval/episode_y_position_std': Array(6.682, dtype=float32), 'eval/episode_y_velocity_std': Array(12.745, dtype=float32), 'eval/avg_episode_length': Array(68.992, dtype=float32), 'eval/epoch_eval_time': 10.541656017303467, 'eval/sps': 12142.30475647242}
time to jit: 0:00:53.963728
time to train: 0:01:24.574698
3

--------

Iteration3-Robot0

--------

{'eval/walltime': 47.920371770858765, 'training/sps': 35310.723008295085, 'training/walltime': 74.23920488357544, 'training/entropy_loss': Array(-0.004, dtype=float32), 'training/policy_loss': Array(-0.02, dtype=float32), 'training/total_loss': Array(0.246, dtype=float32), 'training/v_loss': Array(0.27, dtype=float32), 'eval/episode_distance_from_origin': Array(914.257, dtype=float32), 'eval/episode_forward_reward': Array(424.299, dtype=float32), 'eval/episode_reward': Array(1242.269, dtype=float32), 'eval/episode_reward_alive': Array(927.969, dtype=float32), 'eval/episode_reward_linvel': Array(424.299, dtype=float32), 'eval/episode_reward_quadctrl': Array(-109.999, dtype=float32), 'eval/episode_x_position': Array(853.471, dtype=float32), 'eval/episode_x_velocity': Array(339.44, dtype=float32), 'eval/episode_y_position': Array(153.711, dtype=float32), 'eval/episode_y_velocity': Array(59.413, dtype=float32), 'eval/episode_distance_from_origin_std': Array(1113.654, dtype=float32), 'eval/episode_forward_reward_std': Array(230.452, dtype=float32), 'eval/episode_reward_std': Array(616.961, dtype=float32), 'eval/episode_reward_alive_std': Array(440.689, dtype=float32), 'eval/episode_reward_linvel_std': Array(230.452, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(52.476, dtype=float32), 'eval/episode_x_position_std': Array(1085.759, dtype=float32), 'eval/episode_x_velocity_std': Array(184.362, dtype=float32), 'eval/episode_y_position_std': Array(243.057, dtype=float32), 'eval/episode_y_velocity_std': Array(52.898, dtype=float32), 'eval/avg_episode_length': Array(185.594, dtype=float32), 'eval/epoch_eval_time': 10.286423444747925, 'eval/sps': 12443.58650871549}
{'eval/walltime': 58.29066801071167, 'training/sps': 49129.437046725696, 'training/walltime': 127.59703183174133, 'training/entropy_loss': Array(-0.002, dtype=float32), 'training/policy_loss': Array(-0.01, dtype=float32), 'training/total_loss': Array(0.159, dtype=float32), 'training/v_loss': Array(0.171, dtype=float32), 'eval/episode_distance_from_origin': Array(13464.58, dtype=float32), 'eval/episode_forward_reward': Array(1854.349, dtype=float32), 'eval/episode_reward': Array(4471.042, dtype=float32), 'eval/episode_reward_alive': Array(2988.164, dtype=float32), 'eval/episode_reward_linvel': Array(1854.349, dtype=float32), 'eval/episode_reward_quadctrl': Array(-371.47, dtype=float32), 'eval/episode_x_position': Array(13258.266, dtype=float32), 'eval/episode_x_velocity': Array(1483.483, dtype=float32), 'eval/episode_y_position': Array(1936.134, dtype=float32), 'eval/episode_y_velocity': Array(220.704, dtype=float32), 'eval/episode_distance_from_origin_std': Array(11585.458, dtype=float32), 'eval/episode_forward_reward_std': Array(973.109, dtype=float32), 'eval/episode_reward_std': Array(2283.462, dtype=float32), 'eval/episode_reward_alive_std': Array(1496.785, dtype=float32), 'eval/episode_reward_linvel_std': Array(973.109, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(185.623, dtype=float32), 'eval/episode_x_position_std': Array(11440.793, dtype=float32), 'eval/episode_x_velocity_std': Array(778.49, dtype=float32), 'eval/episode_y_position_std': Array(1886.007, dtype=float32), 'eval/episode_y_velocity_std': Array(144.465, dtype=float32), 'eval/avg_episode_length': Array(597.633, dtype=float32), 'eval/epoch_eval_time': 10.370296239852905, 'eval/sps': 12342.945373932305}
{'eval/walltime': 68.70893478393555, 'training/sps': 48874.80901302401, 'training/walltime': 181.23284244537354, 'training/entropy_loss': Array(-0., dtype=float32), 'training/policy_loss': Array(-0.007, dtype=float32), 'training/total_loss': Array(0.07, dtype=float32), 'training/v_loss': Array(0.077, dtype=float32), 'eval/episode_distance_from_origin': Array(23196.568, dtype=float32), 'eval/episode_forward_reward': Array(2741.4, dtype=float32), 'eval/episode_reward': Array(5966.075, dtype=float32), 'eval/episode_reward_alive': Array(3704.102, dtype=float32), 'eval/episode_reward_linvel': Array(2741.4, dtype=float32), 'eval/episode_reward_quadctrl': Array(-479.424, dtype=float32), 'eval/episode_x_position': Array(22932.244, dtype=float32), 'eval/episode_x_velocity': Array(2193.126, dtype=float32), 'eval/episode_y_position': Array(3052.002, dtype=float32), 'eval/episode_y_velocity': Array(309.053, dtype=float32), 'eval/episode_distance_from_origin_std': Array(14328.372, dtype=float32), 'eval/episode_forward_reward_std': Array(1180.558, dtype=float32), 'eval/episode_reward_std': Array(2516.574, dtype=float32), 'eval/episode_reward_alive_std': Array(1535.181, dtype=float32), 'eval/episode_reward_linvel_std': Array(1180.558, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(198.471, dtype=float32), 'eval/episode_x_position_std': Array(14186.951, dtype=float32), 'eval/episode_x_velocity_std': Array(944.451, dtype=float32), 'eval/episode_y_position_std': Array(2099.171, dtype=float32), 'eval/episode_y_velocity_std': Array(159.62, dtype=float32), 'eval/avg_episode_length': Array(740.82, dtype=float32), 'eval/epoch_eval_time': 10.418266773223877, 'eval/sps': 12286.112727404376}
{'eval/walltime': 79.07463216781616, 'training/sps': 48794.00156258087, 'training/walltime': 234.95747900009155, 'training/entropy_loss': Array(0.001, dtype=float32), 'training/policy_loss': Array(-0.005, dtype=float32), 'training/total_loss': Array(0.072, dtype=float32), 'training/v_loss': Array(0.076, dtype=float32), 'eval/episode_distance_from_origin': Array(29301.113, dtype=float32), 'eval/episode_forward_reward': Array(3333.84, dtype=float32), 'eval/episode_reward': Array(6765.044, dtype=float32), 'eval/episode_reward_alive': Array(3958.477, dtype=float32), 'eval/episode_reward_linvel': Array(3333.84, dtype=float32), 'eval/episode_reward_quadctrl': Array(-527.269, dtype=float32), 'eval/episode_x_position': Array(28994.076, dtype=float32), 'eval/episode_x_velocity': Array(2667.08, dtype=float32), 'eval/episode_y_position': Array(3789.775, dtype=float32), 'eval/episode_y_velocity': Array(375.12, dtype=float32), 'eval/episode_distance_from_origin_std': Array(16078.202, dtype=float32), 'eval/episode_forward_reward_std': Array(1293.574, dtype=float32), 'eval/episode_reward_std': Array(2570.378, dtype=float32), 'eval/episode_reward_alive_std': Array(1471.766, dtype=float32), 'eval/episode_reward_linvel_std': Array(1293.574, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(194.418, dtype=float32), 'eval/episode_x_position_std': Array(15929.324, dtype=float32), 'eval/episode_x_velocity_std': Array(1034.864, dtype=float32), 'eval/episode_y_position_std': Array(2313.755, dtype=float32), 'eval/episode_y_velocity_std': Array(169.364, dtype=float32), 'eval/avg_episode_length': Array(791.695, dtype=float32), 'eval/epoch_eval_time': 10.365697383880615, 'eval/sps': 12348.421457783337}
time to jit: 0:00:52.205593
time to train: 0:04:36.505057

--------

Iteration3-Robot1

--------

{'eval/walltime': 50.18193745613098, 'training/sps': 35021.72637256824, 'training/walltime': 74.85182118415833, 'training/entropy_loss': Array(-0.002, dtype=float32), 'training/policy_loss': Array(-0.016, dtype=float32), 'training/total_loss': Array(0.237, dtype=float32), 'training/v_loss': Array(0.255, dtype=float32), 'eval/episode_distance_from_origin': Array(6043.331, dtype=float32), 'eval/episode_forward_reward': Array(1207.959, dtype=float32), 'eval/episode_reward': Array(2961.194, dtype=float32), 'eval/episode_reward_alive': Array(2000.195, dtype=float32), 'eval/episode_reward_linvel': Array(1207.959, dtype=float32), 'eval/episode_reward_quadctrl': Array(-246.96, dtype=float32), 'eval/episode_x_position': Array(5980.041, dtype=float32), 'eval/episode_x_velocity': Array(966.369, dtype=float32), 'eval/episode_y_position': Array(-158.514, dtype=float32), 'eval/episode_y_velocity': Array(-21.406, dtype=float32), 'eval/episode_distance_from_origin_std': Array(7024.55, dtype=float32), 'eval/episode_forward_reward_std': Array(729.284, dtype=float32), 'eval/episode_reward_std': Array(1718.862, dtype=float32), 'eval/episode_reward_alive_std': Array(1130.304, dtype=float32), 'eval/episode_reward_linvel_std': Array(729.284, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(139.417, dtype=float32), 'eval/episode_x_position_std': Array(7012.591, dtype=float32), 'eval/episode_x_velocity_std': Array(583.429, dtype=float32), 'eval/episode_y_position_std': Array(488.743, dtype=float32), 'eval/episode_y_velocity_std': Array(66.465, dtype=float32), 'eval/avg_episode_length': Array(400.039, dtype=float32), 'eval/epoch_eval_time': 10.39265513420105, 'eval/sps': 12316.390599623239}
{'eval/walltime': 60.601773262023926, 'training/sps': 49019.810667742895, 'training/walltime': 128.3289759159088, 'training/entropy_loss': Array(-0.001, dtype=float32), 'training/policy_loss': Array(-0.008, dtype=float32), 'training/total_loss': Array(0.12, dtype=float32), 'training/v_loss': Array(0.129, dtype=float32), 'eval/episode_distance_from_origin': Array(15841.354, dtype=float32), 'eval/episode_forward_reward': Array(2229.893, dtype=float32), 'eval/episode_reward': Array(4825.843, dtype=float32), 'eval/episode_reward_alive': Array(2983.203, dtype=float32), 'eval/episode_reward_linvel': Array(2229.893, dtype=float32), 'eval/episode_reward_quadctrl': Array(-387.252, dtype=float32), 'eval/episode_x_position': Array(15777.146, dtype=float32), 'eval/episode_x_velocity': Array(1783.919, dtype=float32), 'eval/episode_y_position': Array(-251.391, dtype=float32), 'eval/episode_y_velocity': Array(-23.186, dtype=float32), 'eval/episode_distance_from_origin_std': Array(13463.82, dtype=float32), 'eval/episode_forward_reward_std': Array(1149.591, dtype=float32), 'eval/episode_reward_std': Array(2410.159, dtype=float32), 'eval/episode_reward_alive_std': Array(1451.043, dtype=float32), 'eval/episode_reward_linvel_std': Array(1149.591, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(189.83, dtype=float32), 'eval/episode_x_position_std': Array(13452.131, dtype=float32), 'eval/episode_x_velocity_std': Array(919.676, dtype=float32), 'eval/episode_y_position_std': Array(761.137, dtype=float32), 'eval/episode_y_velocity_std': Array(63.175, dtype=float32), 'eval/avg_episode_length': Array(596.641, dtype=float32), 'eval/epoch_eval_time': 10.419835805892944, 'eval/sps': 12284.262668285955}
{'eval/walltime': 70.93740034103394, 'training/sps': 48870.355468583395, 'training/walltime': 181.96967434883118, 'training/entropy_loss': Array(0.001, dtype=float32), 'training/policy_loss': Array(-0.006, dtype=float32), 'training/total_loss': Array(0.073, dtype=float32), 'training/v_loss': Array(0.078, dtype=float32), 'eval/episode_distance_from_origin': Array(29079.887, dtype=float32), 'eval/episode_forward_reward': Array(3406.51, dtype=float32), 'eval/episode_reward': Array(6831.435, dtype=float32), 'eval/episode_reward_alive': Array(3956.406, dtype=float32), 'eval/episode_reward_linvel': Array(3406.51, dtype=float32), 'eval/episode_reward_quadctrl': Array(-531.479, dtype=float32), 'eval/episode_x_position': Array(29010.783, dtype=float32), 'eval/episode_x_velocity': Array(2725.216, dtype=float32), 'eval/episode_y_position': Array(269.789, dtype=float32), 'eval/episode_y_velocity': Array(36.652, dtype=float32), 'eval/episode_distance_from_origin_std': Array(15724.887, dtype=float32), 'eval/episode_forward_reward_std': Array(1241.846, dtype=float32), 'eval/episode_reward_std': Array(2432.097, dtype=float32), 'eval/episode_reward_alive_std': Array(1376.431, dtype=float32), 'eval/episode_reward_linvel_std': Array(1241.846, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(185.898, dtype=float32), 'eval/episode_x_position_std': Array(15713.083, dtype=float32), 'eval/episode_x_velocity_std': Array(993.481, dtype=float32), 'eval/episode_y_position_std': Array(1115.142, dtype=float32), 'eval/episode_y_velocity_std': Array(79.042, dtype=float32), 'eval/avg_episode_length': Array(791.281, dtype=float32), 'eval/epoch_eval_time': 10.33562707901001, 'eval/sps': 12384.347753794962}
{'eval/walltime': 81.37603735923767, 'training/sps': 48768.59200411839, 'training/walltime': 235.7223026752472, 'training/entropy_loss': Array(0.003, dtype=float32), 'training/policy_loss': Array(-0.003, dtype=float32), 'training/total_loss': Array(0.067, dtype=float32), 'training/v_loss': Array(0.067, dtype=float32), 'eval/episode_distance_from_origin': Array(35971.844, dtype=float32), 'eval/episode_forward_reward': Array(4019.603, dtype=float32), 'eval/episode_reward': Array(7653.125, dtype=float32), 'eval/episode_reward_alive': Array(4215.781, dtype=float32), 'eval/episode_reward_linvel': Array(4019.603, dtype=float32), 'eval/episode_reward_quadctrl': Array(-582.256, dtype=float32), 'eval/episode_x_position': Array(35890.367, dtype=float32), 'eval/episode_x_velocity': Array(3215.693, dtype=float32), 'eval/episode_y_position': Array(-1199.15, dtype=float32), 'eval/episode_y_velocity': Array(-111.719, dtype=float32), 'eval/episode_distance_from_origin_std': Array(16316.612, dtype=float32), 'eval/episode_forward_reward_std': Array(1334.006, dtype=float32), 'eval/episode_reward_std': Array(2486.705, dtype=float32), 'eval/episode_reward_alive_std': Array(1338.794, dtype=float32), 'eval/episode_reward_linvel_std': Array(1334.006, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(185.743, dtype=float32), 'eval/episode_x_position_std': Array(16303.147, dtype=float32), 'eval/episode_x_velocity_std': Array(1067.209, dtype=float32), 'eval/episode_y_position_std': Array(1158.516, dtype=float32), 'eval/episode_y_velocity_std': Array(76.31, dtype=float32), 'eval/avg_episode_length': Array(843.156, dtype=float32), 'eval/epoch_eval_time': 10.438637018203735, 'eval/sps': 12262.137267229744}
time to jit: 0:00:54.287856
time to train: 0:04:37.419555

--------

Iteration3-Robot2

--------

{'eval/walltime': 48.82934904098511, 'training/sps': 35344.568489670295, 'training/walltime': 74.16811442375183, 'training/entropy_loss': Array(-0.009, dtype=float32), 'training/policy_loss': Array(0.01, dtype=float32), 'training/total_loss': Array(0.092, dtype=float32), 'training/v_loss': Array(0.091, dtype=float32), 'eval/episode_distance_from_origin': Array(42.412, dtype=float32), 'eval/episode_forward_reward': Array(32.241, dtype=float32), 'eval/episode_reward': Array(227.154, dtype=float32), 'eval/episode_reward_alive': Array(221.641, dtype=float32), 'eval/episode_reward_linvel': Array(32.241, dtype=float32), 'eval/episode_reward_quadctrl': Array(-26.727, dtype=float32), 'eval/episode_x_position': Array(10.582, dtype=float32), 'eval/episode_x_velocity': Array(25.793, dtype=float32), 'eval/episode_y_position': Array(-0.383, dtype=float32), 'eval/episode_y_velocity': Array(-2.172, dtype=float32), 'eval/episode_distance_from_origin_std': Array(8.552, dtype=float32), 'eval/episode_forward_reward_std': Array(15.294, dtype=float32), 'eval/episode_reward_std': Array(43.428, dtype=float32), 'eval/episode_reward_alive_std': Array(37.573, dtype=float32), 'eval/episode_reward_linvel_std': Array(15.294, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(4.952, dtype=float32), 'eval/episode_x_position_std': Array(5.806, dtype=float32), 'eval/episode_x_velocity_std': Array(12.235, dtype=float32), 'eval/episode_y_position_std': Array(3.011, dtype=float32), 'eval/episode_y_velocity_std': Array(9.615, dtype=float32), 'eval/avg_episode_length': Array(44.328, dtype=float32), 'eval/epoch_eval_time': 10.388425350189209, 'eval/sps': 12321.405380043347}
time to jit: 0:00:53.806846
time to train: 0:01:24.603540

--------

Iteration3-Robot3

--------

{'eval/walltime': 49.456310987472534, 'training/sps': 35093.385552334585, 'training/walltime': 74.69897699356079, 'training/entropy_loss': Array(-0.003, dtype=float32), 'training/policy_loss': Array(-0.02, dtype=float32), 'training/total_loss': Array(0.163, dtype=float32), 'training/v_loss': Array(0.186, dtype=float32), 'eval/episode_distance_from_origin': Array(287.888, dtype=float32), 'eval/episode_forward_reward': Array(260.282, dtype=float32), 'eval/episode_reward': Array(765.681, dtype=float32), 'eval/episode_reward_alive': Array(578.828, dtype=float32), 'eval/episode_reward_linvel': Array(260.282, dtype=float32), 'eval/episode_reward_quadctrl': Array(-73.43, dtype=float32), 'eval/episode_x_position': Array(242.596, dtype=float32), 'eval/episode_x_velocity': Array(208.226, dtype=float32), 'eval/episode_y_position': Array(-13.088, dtype=float32), 'eval/episode_y_velocity': Array(-16.261, dtype=float32), 'eval/episode_distance_from_origin_std': Array(146.671, dtype=float32), 'eval/episode_forward_reward_std': Array(82.727, dtype=float32), 'eval/episode_reward_std': Array(190.987, dtype=float32), 'eval/episode_reward_alive_std': Array(127.681, dtype=float32), 'eval/episode_reward_linvel_std': Array(82.727, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(16.657, dtype=float32), 'eval/episode_x_position_std': Array(143.355, dtype=float32), 'eval/episode_x_velocity_std': Array(66.181, dtype=float32), 'eval/episode_y_position_std': Array(38.303, dtype=float32), 'eval/episode_y_velocity_std': Array(35.16, dtype=float32), 'eval/avg_episode_length': Array(115.766, dtype=float32), 'eval/epoch_eval_time': 10.367921113967896, 'eval/sps': 12345.772946473862}
time to jit: 0:00:53.131386
time to train: 0:01:25.112649
4

--------

Iteration4-Robot0

--------

{'eval/walltime': 48.33003306388855, 'training/sps': 35230.04736611352, 'training/walltime': 74.40921020507812, 'training/entropy_loss': Array(-0.005, dtype=float32), 'training/policy_loss': Array(0.015, dtype=float32), 'training/total_loss': Array(0.218, dtype=float32), 'training/v_loss': Array(0.209, dtype=float32), 'eval/episode_distance_from_origin': Array(68.715, dtype=float32), 'eval/episode_forward_reward': Array(48.822, dtype=float32), 'eval/episode_reward': Array(329.36, dtype=float32), 'eval/episode_reward_alive': Array(318.711, dtype=float32), 'eval/episode_reward_linvel': Array(48.822, dtype=float32), 'eval/episode_reward_quadctrl': Array(-38.173, dtype=float32), 'eval/episode_x_position': Array(25.523, dtype=float32), 'eval/episode_x_velocity': Array(39.058, dtype=float32), 'eval/episode_y_position': Array(6.18, dtype=float32), 'eval/episode_y_velocity': Array(12.735, dtype=float32), 'eval/episode_distance_from_origin_std': Array(29.228, dtype=float32), 'eval/episode_forward_reward_std': Array(35.268, dtype=float32), 'eval/episode_reward_std': Array(106.866, dtype=float32), 'eval/episode_reward_alive_std': Array(89.881, dtype=float32), 'eval/episode_reward_linvel_std': Array(35.268, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(11.416, dtype=float32), 'eval/episode_x_position_std': Array(25.326, dtype=float32), 'eval/episode_x_velocity_std': Array(28.215, dtype=float32), 'eval/episode_y_position_std': Array(9.97, dtype=float32), 'eval/episode_y_velocity_std': Array(19.65, dtype=float32), 'eval/avg_episode_length': Array(63.742, dtype=float32), 'eval/epoch_eval_time': 10.279821157455444, 'eval/sps': 12451.578489492294}
{'eval/walltime': 58.6207799911499, 'training/sps': 49119.652576596505, 'training/walltime': 127.77766585350037, 'training/entropy_loss': Array(-0.003, dtype=float32), 'training/policy_loss': Array(-0.015, dtype=float32), 'training/total_loss': Array(0.261, dtype=float32), 'training/v_loss': Array(0.278, dtype=float32), 'eval/episode_distance_from_origin': Array(629.46, dtype=float32), 'eval/episode_forward_reward': Array(366.134, dtype=float32), 'eval/episode_reward': Array(1024.771, dtype=float32), 'eval/episode_reward_alive': Array(766.875, dtype=float32), 'eval/episode_reward_linvel': Array(366.134, dtype=float32), 'eval/episode_reward_quadctrl': Array(-108.238, dtype=float32), 'eval/episode_x_position': Array(569.644, dtype=float32), 'eval/episode_x_velocity': Array(292.907, dtype=float32), 'eval/episode_y_position': Array(100.939, dtype=float32), 'eval/episode_y_velocity': Array(46.556, dtype=float32), 'eval/episode_distance_from_origin_std': Array(644., dtype=float32), 'eval/episode_forward_reward_std': Array(211.797, dtype=float32), 'eval/episode_reward_std': Array(482.307, dtype=float32), 'eval/episode_reward_alive_std': Array(321.568, dtype=float32), 'eval/episode_reward_linvel_std': Array(211.797, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(47.787, dtype=float32), 'eval/episode_x_position_std': Array(618.081, dtype=float32), 'eval/episode_x_velocity_std': Array(169.438, dtype=float32), 'eval/episode_y_position_std': Array(198.135, dtype=float32), 'eval/episode_y_velocity_std': Array(72.146, dtype=float32), 'eval/avg_episode_length': Array(153.375, dtype=float32), 'eval/epoch_eval_time': 10.290746927261353, 'eval/sps': 12438.358547222022}
{'eval/walltime': 68.87726163864136, 'training/sps': 48921.83414635119, 'training/walltime': 181.3619201183319, 'training/entropy_loss': Array(0., dtype=float32), 'training/policy_loss': Array(-0.009, dtype=float32), 'training/total_loss': Array(0.313, dtype=float32), 'training/v_loss': Array(0.322, dtype=float32), 'eval/episode_distance_from_origin': Array(15755.746, dtype=float32), 'eval/episode_forward_reward': Array(2226.365, dtype=float32), 'eval/episode_reward': Array(4718.353, dtype=float32), 'eval/episode_reward_alive': Array(2936.68, dtype=float32), 'eval/episode_reward_linvel': Array(2226.365, dtype=float32), 'eval/episode_reward_quadctrl': Array(-444.691, dtype=float32), 'eval/episode_x_position': Array(15578.577, dtype=float32), 'eval/episode_x_velocity': Array(1781.096, dtype=float32), 'eval/episode_y_position': Array(1715.387, dtype=float32), 'eval/episode_y_velocity': Array(211.593, dtype=float32), 'eval/episode_distance_from_origin_std': Array(13917.716, dtype=float32), 'eval/episode_forward_reward_std': Array(1177.094, dtype=float32), 'eval/episode_reward_std': Array(2409.43, dtype=float32), 'eval/episode_reward_alive_std': Array(1456.455, dtype=float32), 'eval/episode_reward_linvel_std': Array(1177.094, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(222.776, dtype=float32), 'eval/episode_x_position_std': Array(13811.274, dtype=float32), 'eval/episode_x_velocity_std': Array(941.679, dtype=float32), 'eval/episode_y_position_std': Array(1899.052, dtype=float32), 'eval/episode_y_velocity_std': Array(153.723, dtype=float32), 'eval/avg_episode_length': Array(587.336, dtype=float32), 'eval/epoch_eval_time': 10.256481647491455, 'eval/sps': 12479.913131937054}
{'eval/walltime': 79.29063582420349, 'training/sps': 48800.7227284417, 'training/walltime': 235.0791573524475, 'training/entropy_loss': Array(0.003, dtype=float32), 'training/policy_loss': Array(-0.006, dtype=float32), 'training/total_loss': Array(0.166, dtype=float32), 'training/v_loss': Array(0.169, dtype=float32), 'eval/episode_distance_from_origin': Array(26344.414, dtype=float32), 'eval/episode_forward_reward': Array(3238.766, dtype=float32), 'eval/episode_reward': Array(6163.723, dtype=float32), 'eval/episode_reward_alive': Array(3469.805, dtype=float32), 'eval/episode_reward_linvel': Array(3238.766, dtype=float32), 'eval/episode_reward_quadctrl': Array(-544.845, dtype=float32), 'eval/episode_x_position': Array(26217.16, dtype=float32), 'eval/episode_x_velocity': Array(2591.02, dtype=float32), 'eval/episode_y_position': Array(1769.648, dtype=float32), 'eval/episode_y_velocity': Array(179.873, dtype=float32), 'eval/episode_distance_from_origin_std': Array(19120.527, dtype=float32), 'eval/episode_forward_reward_std': Array(1605.339, dtype=float32), 'eval/episode_reward_std': Array(2962.223, dtype=float32), 'eval/episode_reward_alive_std': Array(1613.652, dtype=float32), 'eval/episode_reward_linvel_std': Array(1605.339, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(256.246, dtype=float32), 'eval/episode_x_position_std': Array(19065.807, dtype=float32), 'eval/episode_x_velocity_std': Array(1284.276, dtype=float32), 'eval/episode_y_position_std': Array(1715.298, dtype=float32), 'eval/episode_y_velocity_std': Array(125.27, dtype=float32), 'eval/avg_episode_length': Array(693.961, dtype=float32), 'eval/epoch_eval_time': 10.413374185562134, 'eval/sps': 12291.88519677595}
time to jit: 0:00:51.004497
time to train: 0:04:36.448649

--------

Iteration4-Robot1

--------

{'eval/walltime': 49.45796847343445, 'training/sps': 34877.39576034607, 'training/walltime': 75.16157507896423, 'training/entropy_loss': Array(-0.005, dtype=float32), 'training/policy_loss': Array(0.018, dtype=float32), 'training/total_loss': Array(0.165, dtype=float32), 'training/v_loss': Array(0.152, dtype=float32), 'eval/episode_distance_from_origin': Array(52.513, dtype=float32), 'eval/episode_forward_reward': Array(15.492, dtype=float32), 'eval/episode_reward': Array(263.445, dtype=float32), 'eval/episode_reward_alive': Array(278.555, dtype=float32), 'eval/episode_reward_linvel': Array(15.492, dtype=float32), 'eval/episode_reward_quadctrl': Array(-30.601, dtype=float32), 'eval/episode_x_position': Array(7.294, dtype=float32), 'eval/episode_x_velocity': Array(12.394, dtype=float32), 'eval/episode_y_position': Array(2.279, dtype=float32), 'eval/episode_y_velocity': Array(3.782, dtype=float32), 'eval/episode_distance_from_origin_std': Array(10.287, dtype=float32), 'eval/episode_forward_reward_std': Array(14.085, dtype=float32), 'eval/episode_reward_std': Array(51.998, dtype=float32), 'eval/episode_reward_alive_std': Array(53.587, dtype=float32), 'eval/episode_reward_linvel_std': Array(14.085, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(6.406, dtype=float32), 'eval/episode_x_position_std': Array(5.225, dtype=float32), 'eval/episode_x_velocity_std': Array(11.268, dtype=float32), 'eval/episode_y_position_std': Array(5.225, dtype=float32), 'eval/episode_y_velocity_std': Array(13.25, dtype=float32), 'eval/avg_episode_length': Array(55.711, dtype=float32), 'eval/epoch_eval_time': 10.31388783454895, 'eval/sps': 12410.451039736145}
{'eval/walltime': 59.86060357093811, 'training/sps': 48893.819288318344, 'training/walltime': 128.77653169631958, 'training/entropy_loss': Array(-0.005, dtype=float32), 'training/policy_loss': Array(-0.012, dtype=float32), 'training/total_loss': Array(0.039, dtype=float32), 'training/v_loss': Array(0.055, dtype=float32), 'eval/episode_distance_from_origin': Array(75.214, dtype=float32), 'eval/episode_forward_reward': Array(43.349, dtype=float32), 'eval/episode_reward': Array(364.822, dtype=float32), 'eval/episode_reward_alive': Array(359.922, dtype=float32), 'eval/episode_reward_linvel': Array(43.349, dtype=float32), 'eval/episode_reward_quadctrl': Array(-38.449, dtype=float32), 'eval/episode_x_position': Array(22.787, dtype=float32), 'eval/episode_x_velocity': Array(34.679, dtype=float32), 'eval/episode_y_position': Array(3.997, dtype=float32), 'eval/episode_y_velocity': Array(3.216, dtype=float32), 'eval/episode_distance_from_origin_std': Array(24.608, dtype=float32), 'eval/episode_forward_reward_std': Array(33.178, dtype=float32), 'eval/episode_reward_std': Array(90.081, dtype=float32), 'eval/episode_reward_alive_std': Array(77.371, dtype=float32), 'eval/episode_reward_linvel_std': Array(33.178, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(9.448, dtype=float32), 'eval/episode_x_position_std': Array(20.679, dtype=float32), 'eval/episode_x_velocity_std': Array(26.542, dtype=float32), 'eval/episode_y_position_std': Array(10.355, dtype=float32), 'eval/episode_y_velocity_std': Array(18.769, dtype=float32), 'eval/avg_episode_length': Array(71.984, dtype=float32), 'eval/epoch_eval_time': 10.402635097503662, 'eval/sps': 12304.574639046637}
time to jit: 0:00:52.658110
time to train: 0:02:29.596888

--------

Iteration4-Robot2

--------

{'eval/walltime': 51.94993448257446, 'training/sps': 28601.961960318356, 'training/walltime': 91.65245389938354, 'training/entropy_loss': Array(-0.001, dtype=float32), 'training/policy_loss': Array(-0.017, dtype=float32), 'training/total_loss': Array(0.302, dtype=float32), 'training/v_loss': Array(0.32, dtype=float32), 'eval/episode_distance_from_origin': Array(1054.596, dtype=float32), 'eval/episode_forward_reward': Array(513.34, dtype=float32), 'eval/episode_reward': Array(1277.258, dtype=float32), 'eval/episode_reward_alive': Array(877.109, dtype=float32), 'eval/episode_reward_linvel': Array(513.34, dtype=float32), 'eval/episode_reward_quadctrl': Array(-113.192, dtype=float32), 'eval/episode_x_position': Array(1002.475, dtype=float32), 'eval/episode_x_velocity': Array(410.672, dtype=float32), 'eval/episode_y_position': Array(12.708, dtype=float32), 'eval/episode_y_velocity': Array(-16.572, dtype=float32), 'eval/episode_distance_from_origin_std': Array(1766.848, dtype=float32), 'eval/episode_forward_reward_std': Array(334.854, dtype=float32), 'eval/episode_reward_std': Array(707.413, dtype=float32), 'eval/episode_reward_alive_std': Array(433.137, dtype=float32), 'eval/episode_reward_linvel_std': Array(334.854, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(59.706, dtype=float32), 'eval/episode_x_position_std': Array(1757.957, dtype=float32), 'eval/episode_x_velocity_std': Array(267.884, dtype=float32), 'eval/episode_y_position_std': Array(189.999, dtype=float32), 'eval/episode_y_velocity_std': Array(57.055, dtype=float32), 'eval/avg_episode_length': Array(175.422, dtype=float32), 'eval/epoch_eval_time': 10.5719473361969, 'eval/sps': 12107.513964030593}
{'eval/walltime': 62.38872194290161, 'training/sps': 25665.134280441547, 'training/walltime': 193.79257798194885, 'training/entropy_loss': Array(0.002, dtype=float32), 'training/policy_loss': Array(-0.009, dtype=float32), 'training/total_loss': Array(0.205, dtype=float32), 'training/v_loss': Array(0.212, dtype=float32), 'eval/episode_distance_from_origin': Array(29532.668, dtype=float32), 'eval/episode_forward_reward': Array(3319.694, dtype=float32), 'eval/episode_reward': Array(6878.129, dtype=float32), 'eval/episode_reward_alive': Array(4115.664, dtype=float32), 'eval/episode_reward_linvel': Array(3319.694, dtype=float32), 'eval/episode_reward_quadctrl': Array(-557.226, dtype=float32), 'eval/episode_x_position': Array(29444.344, dtype=float32), 'eval/episode_x_velocity': Array(2655.763, dtype=float32), 'eval/episode_y_position': Array(-29.698, dtype=float32), 'eval/episode_y_velocity': Array(-28.252, dtype=float32), 'eval/episode_distance_from_origin_std': Array(14801.226, dtype=float32), 'eval/episode_forward_reward_std': Array(1207.173, dtype=float32), 'eval/episode_reward_std': Array(2432.662, dtype=float32), 'eval/episode_reward_alive_std': Array(1419.027, dtype=float32), 'eval/episode_reward_linvel_std': Array(1207.173, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(192.894, dtype=float32), 'eval/episode_x_position_std': Array(14785.87, dtype=float32), 'eval/episode_x_velocity_std': Array(965.743, dtype=float32), 'eval/episode_y_position_std': Array(1303.89, dtype=float32), 'eval/episode_y_velocity_std': Array(96.111, dtype=float32), 'eval/avg_episode_length': Array(823.133, dtype=float32), 'eval/epoch_eval_time': 10.438787460327148, 'eval/sps': 12261.960547282617}
{'eval/walltime': 72.9956111907959, 'training/sps': 48381.62269595189, 'training/walltime': 247.97513437271118, 'training/entropy_loss': Array(0.004, dtype=float32), 'training/policy_loss': Array(-0.005, dtype=float32), 'training/total_loss': Array(0.079, dtype=float32), 'training/v_loss': Array(0.08, dtype=float32), 'eval/episode_distance_from_origin': Array(35811.938, dtype=float32), 'eval/episode_forward_reward': Array(4008.676, dtype=float32), 'eval/episode_reward': Array(7642.482, dtype=float32), 'eval/episode_reward_alive': Array(4223.164, dtype=float32), 'eval/episode_reward_linvel': Array(4008.676, dtype=float32), 'eval/episode_reward_quadctrl': Array(-589.355, dtype=float32), 'eval/episode_x_position': Array(35713.2, dtype=float32), 'eval/episode_x_velocity': Array(3206.951, dtype=float32), 'eval/episode_y_position': Array(-1378.296, dtype=float32), 'eval/episode_y_velocity': Array(-144.679, dtype=float32), 'eval/episode_distance_from_origin_std': Array(15982.185, dtype=float32), 'eval/episode_forward_reward_std': Array(1322.931, dtype=float32), 'eval/episode_reward_std': Array(2461.641, dtype=float32), 'eval/episode_reward_alive_std': Array(1324.483, dtype=float32), 'eval/episode_reward_linvel_std': Array(1322.931, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(185.496, dtype=float32), 'eval/episode_x_position_std': Array(15958.118, dtype=float32), 'eval/episode_x_velocity_std': Array(1058.349, dtype=float32), 'eval/episode_y_position_std': Array(1406.879, dtype=float32), 'eval/episode_y_velocity_std': Array(97.337, dtype=float32), 'eval/avg_episode_length': Array(844.633, dtype=float32), 'eval/epoch_eval_time': 10.606889247894287, 'eval/sps': 12067.628595764867}
{'eval/walltime': 83.36376905441284, 'training/sps': 48482.20736472261, 'training/walltime': 302.04527974128723, 'training/entropy_loss': Array(0.006, dtype=float32), 'training/policy_loss': Array(-0.002, dtype=float32), 'training/total_loss': Array(0.105, dtype=float32), 'training/v_loss': Array(0.101, dtype=float32), 'eval/episode_distance_from_origin': Array(37092.39, dtype=float32), 'eval/episode_forward_reward': Array(4251.198, dtype=float32), 'eval/episode_reward': Array(7702.813, dtype=float32), 'eval/episode_reward_alive': Array(4041.797, dtype=float32), 'eval/episode_reward_linvel': Array(4251.198, dtype=float32), 'eval/episode_reward_quadctrl': Array(-590.179, dtype=float32), 'eval/episode_x_position': Array(36962.31, dtype=float32), 'eval/episode_x_velocity': Array(3400.968, dtype=float32), 'eval/episode_y_position': Array(-2215.453, dtype=float32), 'eval/episode_y_velocity': Array(-221.236, dtype=float32), 'eval/episode_distance_from_origin_std': Array(19020.314, dtype=float32), 'eval/episode_forward_reward_std': Array(1575.201, dtype=float32), 'eval/episode_reward_std': Array(2775.006, dtype=float32), 'eval/episode_reward_alive_std': Array(1408.5, dtype=float32), 'eval/episode_reward_linvel_std': Array(1575.201, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(208.377, dtype=float32), 'eval/episode_x_position_std': Array(18973.121, dtype=float32), 'eval/episode_x_velocity_std': Array(1260.166, dtype=float32), 'eval/episode_y_position_std': Array(1637.491, dtype=float32), 'eval/episode_y_velocity_std': Array(114.166, dtype=float32), 'eval/avg_episode_length': Array(808.359, dtype=float32), 'eval/epoch_eval_time': 10.368157863616943, 'eval/sps': 12345.491039364544}
time to jit: 0:00:55.292094
time to train: 0:05:44.189893

--------

Iteration4-Robot3

--------

{'eval/walltime': 85.89951515197754, 'training/sps': 18730.205329838118, 'training/walltime': 139.9578890800476, 'training/entropy_loss': Array(-0.006, dtype=float32), 'training/policy_loss': Array(0.015, dtype=float32), 'training/total_loss': Array(0.131, dtype=float32), 'training/v_loss': Array(0.122, dtype=float32), 'eval/episode_distance_from_origin': Array(39.669, dtype=float32), 'eval/episode_forward_reward': Array(21.64, dtype=float32), 'eval/episode_reward': Array(218.838, dtype=float32), 'eval/episode_reward_alive': Array(220.742, dtype=float32), 'eval/episode_reward_linvel': Array(21.64, dtype=float32), 'eval/episode_reward_quadctrl': Array(-23.544, dtype=float32), 'eval/episode_x_position': Array(6.988, dtype=float32), 'eval/episode_x_velocity': Array(17.312, dtype=float32), 'eval/episode_y_position': Array(-1.252, dtype=float32), 'eval/episode_y_velocity': Array(-3.157, dtype=float32), 'eval/episode_distance_from_origin_std': Array(8.72, dtype=float32), 'eval/episode_forward_reward_std': Array(11.029, dtype=float32), 'eval/episode_reward_std': Array(46.288, dtype=float32), 'eval/episode_reward_alive_std': Array(44.577, dtype=float32), 'eval/episode_reward_linvel_std': Array(11.029, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(5.085, dtype=float32), 'eval/episode_x_position_std': Array(4.367, dtype=float32), 'eval/episode_x_velocity_std': Array(8.823, dtype=float32), 'eval/episode_y_position_std': Array(3.557, dtype=float32), 'eval/episode_y_velocity_std': Array(10.242, dtype=float32), 'eval/avg_episode_length': Array(44.148, dtype=float32), 'eval/epoch_eval_time': 45.6465950012207, 'eval/sps': 2804.152204487037}
time to jit: 0:00:55.860036
time to train: 0:03:05.689027
5

--------

Iteration5-Robot0

--------

{'eval/walltime': 51.14033365249634, 'training/sps': 34046.480741715066, 'training/walltime': 76.9959168434143, 'training/entropy_loss': Array(-0.002, dtype=float32), 'training/policy_loss': Array(0.024, dtype=float32), 'training/total_loss': Array(0.296, dtype=float32), 'training/v_loss': Array(0.274, dtype=float32), 'eval/episode_distance_from_origin': Array(83.136, dtype=float32), 'eval/episode_forward_reward': Array(102.112, dtype=float32), 'eval/episode_reward': Array(376.42, dtype=float32), 'eval/episode_reward_alive': Array(313.945, dtype=float32), 'eval/episode_reward_linvel': Array(102.112, dtype=float32), 'eval/episode_reward_quadctrl': Array(-39.637, dtype=float32), 'eval/episode_x_position': Array(49.292, dtype=float32), 'eval/episode_x_velocity': Array(81.69, dtype=float32), 'eval/episode_y_position': Array(-10.404, dtype=float32), 'eval/episode_y_velocity': Array(-19.446, dtype=float32), 'eval/episode_distance_from_origin_std': Array(35.769, dtype=float32), 'eval/episode_forward_reward_std': Array(40.672, dtype=float32), 'eval/episode_reward_std': Array(100.536, dtype=float32), 'eval/episode_reward_alive_std': Array(72.17, dtype=float32), 'eval/episode_reward_linvel_std': Array(40.672, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(9.419, dtype=float32), 'eval/episode_x_position_std': Array(31.455, dtype=float32), 'eval/episode_x_velocity_std': Array(32.538, dtype=float32), 'eval/episode_y_position_std': Array(13.967, dtype=float32), 'eval/episode_y_velocity_std': Array(22.419, dtype=float32), 'eval/avg_episode_length': Array(62.789, dtype=float32), 'eval/epoch_eval_time': 10.509374618530273, 'eval/sps': 12179.601988334172}
{'eval/walltime': 61.66618609428406, 'training/sps': 48518.42825042298, 'training/walltime': 131.02569675445557, 'training/entropy_loss': Array(0., dtype=float32), 'training/policy_loss': Array(-0.01, dtype=float32), 'training/total_loss': Array(0.285, dtype=float32), 'training/v_loss': Array(0.295, dtype=float32), 'eval/episode_distance_from_origin': Array(331.633, dtype=float32), 'eval/episode_forward_reward': Array(276.393, dtype=float32), 'eval/episode_reward': Array(767.555, dtype=float32), 'eval/episode_reward_alive': Array(564.961, dtype=float32), 'eval/episode_reward_linvel': Array(276.393, dtype=float32), 'eval/episode_reward_quadctrl': Array(-73.799, dtype=float32), 'eval/episode_x_position': Array(285.863, dtype=float32), 'eval/episode_x_velocity': Array(221.115, dtype=float32), 'eval/episode_y_position': Array(-59.906, dtype=float32), 'eval/episode_y_velocity': Array(-53.306, dtype=float32), 'eval/episode_distance_from_origin_std': Array(257.981, dtype=float32), 'eval/episode_forward_reward_std': Array(117.141, dtype=float32), 'eval/episode_reward_std': Array(274.432, dtype=float32), 'eval/episode_reward_alive_std': Array(186.834, dtype=float32), 'eval/episode_reward_linvel_std': Array(117.141, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(23.755, dtype=float32), 'eval/episode_x_position_std': Array(241.554, dtype=float32), 'eval/episode_x_velocity_std': Array(93.713, dtype=float32), 'eval/episode_y_position_std': Array(92.659, dtype=float32), 'eval/episode_y_velocity_std': Array(52.099, dtype=float32), 'eval/avg_episode_length': Array(112.992, dtype=float32), 'eval/epoch_eval_time': 10.52585244178772, 'eval/sps': 12160.535282808922}
{'eval/walltime': 72.24266767501831, 'training/sps': 48411.55583971965, 'training/walltime': 185.17475175857544, 'training/entropy_loss': Array(0.002, dtype=float32), 'training/policy_loss': Array(-0.008, dtype=float32), 'training/total_loss': Array(0.294, dtype=float32), 'training/v_loss': Array(0.3, dtype=float32), 'eval/episode_distance_from_origin': Array(4991.457, dtype=float32), 'eval/episode_forward_reward': Array(1157.968, dtype=float32), 'eval/episode_reward': Array(2565.733, dtype=float32), 'eval/episode_reward_alive': Array(1629.922, dtype=float32), 'eval/episode_reward_linvel': Array(1157.968, dtype=float32), 'eval/episode_reward_quadctrl': Array(-222.157, dtype=float32), 'eval/episode_x_position': Array(4809.04, dtype=float32), 'eval/episode_x_velocity': Array(926.376, dtype=float32), 'eval/episode_y_position': Array(-1098.446, dtype=float32), 'eval/episode_y_velocity': Array(-208.092, dtype=float32), 'eval/episode_distance_from_origin_std': Array(7023.508, dtype=float32), 'eval/episode_forward_reward_std': Array(747.196, dtype=float32), 'eval/episode_reward_std': Array(1604.464, dtype=float32), 'eval/episode_reward_alive_std': Array(993.41, dtype=float32), 'eval/episode_reward_linvel_std': Array(747.196, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(131.214, dtype=float32), 'eval/episode_x_position_std': Array(6765.324, dtype=float32), 'eval/episode_x_velocity_std': Array(597.759, dtype=float32), 'eval/episode_y_position_std': Array(1905.678, dtype=float32), 'eval/episode_y_velocity_std': Array(181.798, dtype=float32), 'eval/avg_episode_length': Array(325.984, dtype=float32), 'eval/epoch_eval_time': 10.576481580734253, 'eval/sps': 12102.323350438232}
{'eval/walltime': 82.67386937141418, 'training/sps': 48362.949624356035, 'training/walltime': 239.37822818756104, 'training/entropy_loss': Array(0.004, dtype=float32), 'training/policy_loss': Array(-0.005, dtype=float32), 'training/total_loss': Array(0.21, dtype=float32), 'training/v_loss': Array(0.211, dtype=float32), 'eval/episode_distance_from_origin': Array(23449.967, dtype=float32), 'eval/episode_forward_reward': Array(3016.93, dtype=float32), 'eval/episode_reward': Array(5754.476, dtype=float32), 'eval/episode_reward_alive': Array(3209.961, dtype=float32), 'eval/episode_reward_linvel': Array(3016.93, dtype=float32), 'eval/episode_reward_quadctrl': Array(-472.413, dtype=float32), 'eval/episode_x_position': Array(23043.414, dtype=float32), 'eval/episode_x_velocity': Array(2413.55, dtype=float32), 'eval/episode_y_position': Array(-3985.522, dtype=float32), 'eval/episode_y_velocity': Array(-441.856, dtype=float32), 'eval/episode_distance_from_origin_std': Array(18729.984, dtype=float32), 'eval/episode_forward_reward_std': Array(1541.811, dtype=float32), 'eval/episode_reward_std': Array(2878.363, dtype=float32), 'eval/episode_reward_alive_std': Array(1566.384, dtype=float32), 'eval/episode_reward_linvel_std': Array(1541.811, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(228.614, dtype=float32), 'eval/episode_x_position_std': Array(18418.516, dtype=float32), 'eval/episode_x_velocity_std': Array(1233.454, dtype=float32), 'eval/episode_y_position_std': Array(3462.227, dtype=float32), 'eval/episode_y_velocity_std': Array(263.153, dtype=float32), 'eval/avg_episode_length': Array(641.992, dtype=float32), 'eval/epoch_eval_time': 10.431201696395874, 'eval/sps': 12270.877673108918}
time to jit: 0:00:55.015362
time to train: 0:04:41.569571

--------

Iteration5-Robot1

--------

{'eval/walltime': 92.72783136367798, 'training/sps': 18313.96321617343, 'training/walltime': 143.13887000083923, 'training/entropy_loss': Array(-0.007, dtype=float32), 'training/policy_loss': Array(0.019, dtype=float32), 'training/total_loss': Array(0.152, dtype=float32), 'training/v_loss': Array(0.14, dtype=float32), 'eval/episode_distance_from_origin': Array(42.261, dtype=float32), 'eval/episode_forward_reward': Array(24.941, dtype=float32), 'eval/episode_reward': Array(224.689, dtype=float32), 'eval/episode_reward_alive': Array(224.023, dtype=float32), 'eval/episode_reward_linvel': Array(24.941, dtype=float32), 'eval/episode_reward_quadctrl': Array(-24.275, dtype=float32), 'eval/episode_x_position': Array(9.08, dtype=float32), 'eval/episode_x_velocity': Array(19.953, dtype=float32), 'eval/episode_y_position': Array(-1.119, dtype=float32), 'eval/episode_y_velocity': Array(-1.542, dtype=float32), 'eval/episode_distance_from_origin_std': Array(9.391, dtype=float32), 'eval/episode_forward_reward_std': Array(15.995, dtype=float32), 'eval/episode_reward_std': Array(48.543, dtype=float32), 'eval/episode_reward_alive_std': Array(47.162, dtype=float32), 'eval/episode_reward_linvel_std': Array(15.995, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(5.011, dtype=float32), 'eval/episode_x_position_std': Array(5.629, dtype=float32), 'eval/episode_x_velocity_std': Array(12.796, dtype=float32), 'eval/episode_y_position_std': Array(4.161, dtype=float32), 'eval/episode_y_velocity_std': Array(13.148, dtype=float32), 'eval/avg_episode_length': Array(44.805, dtype=float32), 'eval/epoch_eval_time': 44.968852043151855, 'eval/sps': 2846.4146666935576}
{'eval/walltime': 119.30154824256897, 'training/sps': 24785.422787660216, 'training/walltime': 248.9042637348175, 'training/entropy_loss': Array(-0.006, dtype=float32), 'training/policy_loss': Array(-0.012, dtype=float32), 'training/total_loss': Array(0.085, dtype=float32), 'training/v_loss': Array(0.103, dtype=float32), 'eval/episode_distance_from_origin': Array(59.852, dtype=float32), 'eval/episode_forward_reward': Array(64.023, dtype=float32), 'eval/episode_reward': Array(310.634, dtype=float32), 'eval/episode_reward_alive': Array(275.469, dtype=float32), 'eval/episode_reward_linvel': Array(64.023, dtype=float32), 'eval/episode_reward_quadctrl': Array(-28.857, dtype=float32), 'eval/episode_x_position': Array(25.193, dtype=float32), 'eval/episode_x_velocity': Array(51.218, dtype=float32), 'eval/episode_y_position': Array(-3.443, dtype=float32), 'eval/episode_y_velocity': Array(-5.214, dtype=float32), 'eval/episode_distance_from_origin_std': Array(12.121, dtype=float32), 'eval/episode_forward_reward_std': Array(22.524, dtype=float32), 'eval/episode_reward_std': Array(50.575, dtype=float32), 'eval/episode_reward_alive_std': Array(36.225, dtype=float32), 'eval/episode_reward_linvel_std': Array(22.524, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(4.12, dtype=float32), 'eval/episode_x_position_std': Array(10.967, dtype=float32), 'eval/episode_x_velocity_std': Array(18.019, dtype=float32), 'eval/episode_y_position_std': Array(5.798, dtype=float32), 'eval/episode_y_velocity_std': Array(15.561, dtype=float32), 'eval/avg_episode_length': Array(55.094, dtype=float32), 'eval/epoch_eval_time': 26.57371687889099, 'eval/sps': 4816.789483509461}
time to jit: 0:01:06.592989
time to train: 0:05:45.356309

--------

Iteration5-Robot2

--------

{'eval/walltime': 92.29182195663452, 'training/sps': 21921.58550509932, 'training/walltime': 119.58259129524231, 'training/entropy_loss': Array(-0.002, dtype=float32), 'training/policy_loss': Array(0.006, dtype=float32), 'training/total_loss': Array(0.117, dtype=float32), 'training/v_loss': Array(0.113, dtype=float32), 'eval/episode_distance_from_origin': Array(48.249, dtype=float32), 'eval/episode_forward_reward': Array(37.028, dtype=float32), 'eval/episode_reward': Array(269.089, dtype=float32), 'eval/episode_reward_alive': Array(256.133, dtype=float32), 'eval/episode_reward_linvel': Array(37.028, dtype=float32), 'eval/episode_reward_quadctrl': Array(-24.072, dtype=float32), 'eval/episode_x_position': Array(13.753, dtype=float32), 'eval/episode_x_velocity': Array(29.622, dtype=float32), 'eval/episode_y_position': Array(1.169, dtype=float32), 'eval/episode_y_velocity': Array(5.726, dtype=float32), 'eval/episode_distance_from_origin_std': Array(9.572, dtype=float32), 'eval/episode_forward_reward_std': Array(10.874, dtype=float32), 'eval/episode_reward_std': Array(45.95, dtype=float32), 'eval/episode_reward_alive_std': Array(43.031, dtype=float32), 'eval/episode_reward_linvel_std': Array(10.874, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(3.888, dtype=float32), 'eval/episode_x_position_std': Array(6.218, dtype=float32), 'eval/episode_x_velocity_std': Array(8.699, dtype=float32), 'eval/episode_y_position_std': Array(4.667, dtype=float32), 'eval/episode_y_velocity_std': Array(12.783, dtype=float32), 'eval/avg_episode_length': Array(51.227, dtype=float32), 'eval/epoch_eval_time': 10.380965232849121, 'eval/sps': 12330.259964166125}
0

--------

Iteration0-Robot0

--------

{'eval/walltime': 46.31344032287598, 'training/sps': 35815.20506564643, 'training/walltime': 73.19349408149719, 'training/entropy_loss': Array(-0.013, dtype=float32), 'training/policy_loss': Array(0.003, dtype=float32), 'training/total_loss': Array(0.11, dtype=float32), 'training/v_loss': Array(0.121, dtype=float32), 'eval/episode_distance_from_origin': Array(37.918, dtype=float32), 'eval/episode_forward_reward': Array(7.435, dtype=float32), 'eval/episode_reward': Array(201.78, dtype=float32), 'eval/episode_reward_alive': Array(218.867, dtype=float32), 'eval/episode_reward_linvel': Array(7.435, dtype=float32), 'eval/episode_reward_quadctrl': Array(-24.522, dtype=float32), 'eval/episode_x_position': Array(2.86, dtype=float32), 'eval/episode_x_velocity': Array(5.948, dtype=float32), 'eval/episode_y_position': Array(1.795, dtype=float32), 'eval/episode_y_velocity': Array(4.114, dtype=float32), 'eval/episode_distance_from_origin_std': Array(7.593, dtype=float32), 'eval/episode_forward_reward_std': Array(11.406, dtype=float32), 'eval/episode_reward_std': Array(36.522, dtype=float32), 'eval/episode_reward_alive_std': Array(43.581, dtype=float32), 'eval/episode_reward_linvel_std': Array(11.406, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(5.097, dtype=float32), 'eval/episode_x_position_std': Array(2.746, dtype=float32), 'eval/episode_x_velocity_std': Array(9.125, dtype=float32), 'eval/episode_y_position_std': Array(3.472, dtype=float32), 'eval/episode_y_velocity_std': Array(10.129, dtype=float32), 'eval/avg_episode_length': Array(43.773, dtype=float32), 'eval/epoch_eval_time': 10.289448261260986, 'eval/sps': 12439.928434443911}
{'eval/walltime': 56.65238094329834, 'training/sps': 49654.55440173802, 'training/walltime': 125.9870400428772, 'training/entropy_loss': Array(-0.012, dtype=float32), 'training/policy_loss': Array(-0.014, dtype=float32), 'training/total_loss': Array(0.032, dtype=float32), 'training/v_loss': Array(0.057, dtype=float32), 'eval/episode_distance_from_origin': Array(48.614, dtype=float32), 'eval/episode_forward_reward': Array(15.1, dtype=float32), 'eval/episode_reward': Array(260.792, dtype=float32), 'eval/episode_reward_alive': Array(274.688, dtype=float32), 'eval/episode_reward_linvel': Array(15.1, dtype=float32), 'eval/episode_reward_quadctrl': Array(-28.995, dtype=float32), 'eval/episode_x_position': Array(6.681, dtype=float32), 'eval/episode_x_velocity': Array(12.08, dtype=float32), 'eval/episode_y_position': Array(0.718, dtype=float32), 'eval/episode_y_velocity': Array(1.008, dtype=float32), 'eval/episode_distance_from_origin_std': Array(9.669, dtype=float32), 'eval/episode_forward_reward_std': Array(13.336, dtype=float32), 'eval/episode_reward_std': Array(49.697, dtype=float32), 'eval/episode_reward_alive_std': Array(54.967, dtype=float32), 'eval/episode_reward_linvel_std': Array(13.336, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(6.031, dtype=float32), 'eval/episode_x_position_std': Array(3.929, dtype=float32), 'eval/episode_x_velocity_std': Array(10.669, dtype=float32), 'eval/episode_y_position_std': Array(4.516, dtype=float32), 'eval/episode_y_velocity_std': Array(10.489, dtype=float32), 'eval/avg_episode_length': Array(54.938, dtype=float32), 'eval/epoch_eval_time': 10.338940620422363, 'eval/sps': 12380.378677014878}
{'eval/walltime': 66.97486734390259, 'training/sps': 49375.000952619834, 'training/walltime': 179.07949471473694, 'training/entropy_loss': Array(-0.011, dtype=float32), 'training/policy_loss': Array(-0.011, dtype=float32), 'training/total_loss': Array(0.023, dtype=float32), 'training/v_loss': Array(0.045, dtype=float32), 'eval/episode_distance_from_origin': Array(57.398, dtype=float32), 'eval/episode_forward_reward': Array(22.381, dtype=float32), 'eval/episode_reward': Array(309.009, dtype=float32), 'eval/episode_reward_alive': Array(318.203, dtype=float32), 'eval/episode_reward_linvel': Array(22.381, dtype=float32), 'eval/episode_reward_quadctrl': Array(-31.575, dtype=float32), 'eval/episode_x_position': Array(11.553, dtype=float32), 'eval/episode_x_velocity': Array(17.904, dtype=float32), 'eval/episode_y_position': Array(3.409, dtype=float32), 'eval/episode_y_velocity': Array(4.83, dtype=float32), 'eval/episode_distance_from_origin_std': Array(12.471, dtype=float32), 'eval/episode_forward_reward_std': Array(13.756, dtype=float32), 'eval/episode_reward_std': Array(61.524, dtype=float32), 'eval/episode_reward_alive_std': Array(65.85, dtype=float32), 'eval/episode_reward_linvel_std': Array(13.756, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(6.969, dtype=float32), 'eval/episode_x_position_std': Array(5.647, dtype=float32), 'eval/episode_x_velocity_std': Array(11.005, dtype=float32), 'eval/episode_y_position_std': Array(5.638, dtype=float32), 'eval/episode_y_velocity_std': Array(10.42, dtype=float32), 'eval/avg_episode_length': Array(63.641, dtype=float32), 'eval/epoch_eval_time': 10.322486400604248, 'eval/sps': 12400.11321230777}
{'eval/walltime': 77.30903625488281, 'training/sps': 49140.03338286942, 'training/walltime': 232.42581582069397, 'training/entropy_loss': Array(-0.009, dtype=float32), 'training/policy_loss': Array(-0.011, dtype=float32), 'training/total_loss': Array(0.023, dtype=float32), 'training/v_loss': Array(0.043, dtype=float32), 'eval/episode_distance_from_origin': Array(75.379, dtype=float32), 'eval/episode_forward_reward': Array(29.21, dtype=float32), 'eval/episode_reward': Array(393.493, dtype=float32), 'eval/episode_reward_alive': Array(403.594, dtype=float32), 'eval/episode_reward_linvel': Array(29.21, dtype=float32), 'eval/episode_reward_quadctrl': Array(-39.31, dtype=float32), 'eval/episode_x_position': Array(21.595, dtype=float32), 'eval/episode_x_velocity': Array(23.368, dtype=float32), 'eval/episode_y_position': Array(4.773, dtype=float32), 'eval/episode_y_velocity': Array(5.467, dtype=float32), 'eval/episode_distance_from_origin_std': Array(15.996, dtype=float32), 'eval/episode_forward_reward_std': Array(14.224, dtype=float32), 'eval/episode_reward_std': Array(71.392, dtype=float32), 'eval/episode_reward_alive_std': Array(80.635, dtype=float32), 'eval/episode_reward_linvel_std': Array(14.224, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(8.132, dtype=float32), 'eval/episode_x_position_std': Array(8.92, dtype=float32), 'eval/episode_x_velocity_std': Array(11.379, dtype=float32), 'eval/episode_y_position_std': Array(8.271, dtype=float32), 'eval/episode_y_velocity_std': Array(14.761, dtype=float32), 'eval/avg_episode_length': Array(80.719, dtype=float32), 'eval/epoch_eval_time': 10.334168910980225, 'eval/sps': 12386.095205391688}
time to jit: 0:00:50.243303
time to train: 0:04:33.928977

--------

Iteration0-Robot1

--------

{'eval/walltime': 63.47223782539368, 'training/sps': 32901.939586860506, 'training/walltime': 79.67433023452759, 'training/entropy_loss': Array(-0.013, dtype=float32), 'training/policy_loss': Array(0.001, dtype=float32), 'training/total_loss': Array(0.099, dtype=float32), 'training/v_loss': Array(0.111, dtype=float32), 'eval/episode_distance_from_origin': Array(36.846, dtype=float32), 'eval/episode_forward_reward': Array(9.267, dtype=float32), 'eval/episode_reward': Array(200.187, dtype=float32), 'eval/episode_reward_alive': Array(215.312, dtype=float32), 'eval/episode_reward_linvel': Array(9.267, dtype=float32), 'eval/episode_reward_quadctrl': Array(-24.393, dtype=float32), 'eval/episode_x_position': Array(3.152, dtype=float32), 'eval/episode_x_velocity': Array(7.414, dtype=float32), 'eval/episode_y_position': Array(1.112, dtype=float32), 'eval/episode_y_velocity': Array(2.17, dtype=float32), 'eval/episode_distance_from_origin_std': Array(8.203, dtype=float32), 'eval/episode_forward_reward_std': Array(10.4, dtype=float32), 'eval/episode_reward_std': Array(41.06, dtype=float32), 'eval/episode_reward_alive_std': Array(48.262, dtype=float32), 'eval/episode_reward_linvel_std': Array(10.4, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(5.714, dtype=float32), 'eval/episode_x_position_std': Array(2.373, dtype=float32), 'eval/episode_x_velocity_std': Array(8.32, dtype=float32), 'eval/episode_y_position_std': Array(3.314, dtype=float32), 'eval/episode_y_velocity_std': Array(9.701, dtype=float32), 'eval/avg_episode_length': Array(43.062, dtype=float32), 'eval/epoch_eval_time': 26.059921503067017, 'eval/sps': 4911.756928543916}
{'eval/walltime': 73.89283728599548, 'training/sps': 32631.97250402982, 'training/walltime': 160.00781297683716, 'training/entropy_loss': Array(-0.012, dtype=float32), 'training/policy_loss': Array(-0.014, dtype=float32), 'training/total_loss': Array(0.023, dtype=float32), 'training/v_loss': Array(0.048, dtype=float32), 'eval/episode_distance_from_origin': Array(47.917, dtype=float32), 'eval/episode_forward_reward': Array(14.132, dtype=float32), 'eval/episode_reward': Array(259.32, dtype=float32), 'eval/episode_reward_alive': Array(274.18, dtype=float32), 'eval/episode_reward_linvel': Array(14.132, dtype=float32), 'eval/episode_reward_quadctrl': Array(-28.992, dtype=float32), 'eval/episode_x_position': Array(6.055, dtype=float32), 'eval/episode_x_velocity': Array(11.305, dtype=float32), 'eval/episode_y_position': Array(0.36, dtype=float32), 'eval/episode_y_velocity': Array(0.314, dtype=float32), 'eval/episode_distance_from_origin_std': Array(8.682, dtype=float32), 'eval/episode_forward_reward_std': Array(15.832, dtype=float32), 'eval/episode_reward_std': Array(45.515, dtype=float32), 'eval/episode_reward_alive_std': Array(49.477, dtype=float32), 'eval/episode_reward_linvel_std': Array(15.832, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(5.522, dtype=float32), 'eval/episode_x_position_std': Array(4.311, dtype=float32), 'eval/episode_x_velocity_std': Array(12.665, dtype=float32), 'eval/episode_y_position_std': Array(3.965, dtype=float32), 'eval/episode_y_velocity_std': Array(9.518, dtype=float32), 'eval/avg_episode_length': Array(54.836, dtype=float32), 'eval/epoch_eval_time': 10.420599460601807, 'eval/sps': 12283.362438403115}
{'eval/walltime': 100.06080627441406, 'training/sps': 45064.126671522776, 'training/walltime': 218.1791388988495, 'training/entropy_loss': Array(-0.011, dtype=float32), 'training/policy_loss': Array(-0.01, dtype=float32), 'training/total_loss': Array(0.019, dtype=float32), 'training/v_loss': Array(0.04, dtype=float32), 'eval/episode_distance_from_origin': Array(53.566, dtype=float32), 'eval/episode_forward_reward': Array(20.631, dtype=float32), 'eval/episode_reward': Array(294.935, dtype=float32), 'eval/episode_reward_alive': Array(304.023, dtype=float32), 'eval/episode_reward_linvel': Array(20.631, dtype=float32), 'eval/episode_reward_quadctrl': Array(-29.72, dtype=float32), 'eval/episode_x_position': Array(8.791, dtype=float32), 'eval/episode_x_velocity': Array(16.505, dtype=float32), 'eval/episode_y_position': Array(1.211, dtype=float32), 'eval/episode_y_velocity': Array(1.672, dtype=float32), 'eval/episode_distance_from_origin_std': Array(9.592, dtype=float32), 'eval/episode_forward_reward_std': Array(15.179, dtype=float32), 'eval/episode_reward_std': Array(47.483, dtype=float32), 'eval/episode_reward_alive_std': Array(55.446, dtype=float32), 'eval/episode_reward_linvel_std': Array(15.179, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(5.907, dtype=float32), 'eval/episode_x_position_std': Array(4.091, dtype=float32), 'eval/episode_x_velocity_std': Array(12.143, dtype=float32), 'eval/episode_y_position_std': Array(4.203, dtype=float32), 'eval/episode_y_velocity_std': Array(8.635, dtype=float32), 'eval/avg_episode_length': Array(60.805, dtype=float32), 'eval/epoch_eval_time': 26.16796898841858, 'eval/sps': 4891.476295185547}
{'eval/walltime': 142.9237515926361, 'training/sps': 30156.079628893087, 'training/walltime': 305.1082103252411, 'training/entropy_loss': Array(-0.009, dtype=float32), 'training/policy_loss': Array(-0.011, dtype=float32), 'training/total_loss': Array(0.012, dtype=float32), 'training/v_loss': Array(0.033, dtype=float32), 'eval/episode_distance_from_origin': Array(70.125, dtype=float32), 'eval/episode_forward_reward': Array(22.835, dtype=float32), 'eval/episode_reward': Array(379.964, dtype=float32), 'eval/episode_reward_alive': Array(394.023, dtype=float32), 'eval/episode_reward_linvel': Array(22.835, dtype=float32), 'eval/episode_reward_quadctrl': Array(-36.894, dtype=float32), 'eval/episode_x_position': Array(13.536, dtype=float32), 'eval/episode_x_velocity': Array(18.268, dtype=float32), 'eval/episode_y_position': Array(1.618, dtype=float32), 'eval/episode_y_velocity': Array(1.544, dtype=float32), 'eval/episode_distance_from_origin_std': Array(16.546, dtype=float32), 'eval/episode_forward_reward_std': Array(17.19, dtype=float32), 'eval/episode_reward_std': Array(83.125, dtype=float32), 'eval/episode_reward_alive_std': Array(89.957, dtype=float32), 'eval/episode_reward_linvel_std': Array(17.19, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(9.063, dtype=float32), 'eval/episode_x_position_std': Array(8.435, dtype=float32), 'eval/episode_x_velocity_std': Array(13.752, dtype=float32), 'eval/episode_y_position_std': Array(5.28, dtype=float32), 'eval/episode_y_velocity_std': Array(10.555, dtype=float32), 'eval/avg_episode_length': Array(78.805, dtype=float32), 'eval/epoch_eval_time': 42.862945318222046, 'eval/sps': 2986.2623543413893}
time to jit: 0:00:50.655098
time to train: 0:06:50.882909

--------

Iteration0-Robot2

--------

{'eval/walltime': 112.7787823677063, 'training/sps': 34695.85620982549, 'training/walltime': 75.55484390258789, 'training/entropy_loss': Array(-0.013, dtype=float32), 'training/policy_loss': Array(0.001, dtype=float32), 'training/total_loss': Array(0.11, dtype=float32), 'training/v_loss': Array(0.122, dtype=float32), 'eval/episode_distance_from_origin': Array(36.994, dtype=float32), 'eval/episode_forward_reward': Array(8.226, dtype=float32), 'eval/episode_reward': Array(197.315, dtype=float32), 'eval/episode_reward_alive': Array(213.359, dtype=float32), 'eval/episode_reward_linvel': Array(8.226, dtype=float32), 'eval/episode_reward_quadctrl': Array(-24.27, dtype=float32), 'eval/episode_x_position': Array(2.972, dtype=float32), 'eval/episode_x_velocity': Array(6.58, dtype=float32), 'eval/episode_y_position': Array(1.101, dtype=float32), 'eval/episode_y_velocity': Array(2.564, dtype=float32), 'eval/episode_distance_from_origin_std': Array(7.591, dtype=float32), 'eval/episode_forward_reward_std': Array(10.07, dtype=float32), 'eval/episode_reward_std': Array(38.84, dtype=float32), 'eval/episode_reward_alive_std': Array(43.333, dtype=float32), 'eval/episode_reward_linvel_std': Array(10.07, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(5.146, dtype=float32), 'eval/episode_x_position_std': Array(2.553, dtype=float32), 'eval/episode_x_velocity_std': Array(8.056, dtype=float32), 'eval/episode_y_position_std': Array(3.345, dtype=float32), 'eval/episode_y_velocity_std': Array(10.094, dtype=float32), 'eval/avg_episode_length': Array(42.672, dtype=float32), 'eval/epoch_eval_time': 40.80516338348389, 'eval/sps': 3136.8579215592285}
{'eval/walltime': 191.13720273971558, 'training/sps': 25579.04263003992, 'training/walltime': 178.0387420654297, 'training/entropy_loss': Array(-0.011, dtype=float32), 'training/policy_loss': Array(-0.015, dtype=float32), 'training/total_loss': Array(0.025, dtype=float32), 'training/v_loss': Array(0.051, dtype=float32), 'eval/episode_distance_from_origin': Array(48.865, dtype=float32), 'eval/episode_forward_reward': Array(13.892, dtype=float32), 'eval/episode_reward': Array(260.536, dtype=float32), 'eval/episode_reward_alive': Array(276.094, dtype=float32), 'eval/episode_reward_linvel': Array(13.892, dtype=float32), 'eval/episode_reward_quadctrl': Array(-29.45, dtype=float32), 'eval/episode_x_position': Array(5.992, dtype=float32), 'eval/episode_x_velocity': Array(11.114, dtype=float32), 'eval/episode_y_position': Array(1.229, dtype=float32), 'eval/episode_y_velocity': Array(1.283, dtype=float32), 'eval/episode_distance_from_origin_std': Array(9.776, dtype=float32), 'eval/episode_forward_reward_std': Array(13.947, dtype=float32), 'eval/episode_reward_std': Array(49.68, dtype=float32), 'eval/episode_reward_alive_std': Array(54.583, dtype=float32), 'eval/episode_reward_linvel_std': Array(13.947, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(6.529, dtype=float32), 'eval/episode_x_position_std': Array(4.161, dtype=float32), 'eval/episode_x_velocity_std': Array(11.158, dtype=float32), 'eval/episode_y_position_std': Array(4.69, dtype=float32), 'eval/episode_y_velocity_std': Array(10.728, dtype=float32), 'eval/avg_episode_length': Array(55.219, dtype=float32), 'eval/epoch_eval_time': 78.35842037200928, 'eval/sps': 1633.519402156343}
{'eval/walltime': 269.6043863296509, 'training/sps': 14896.294251502533, 'training/walltime': 354.018079996109, 'training/entropy_loss': Array(-0.01, dtype=float32), 'training/policy_loss': Array(-0.011, dtype=float32), 'training/total_loss': Array(0.023, dtype=float32), 'training/v_loss': Array(0.045, dtype=float32), 'eval/episode_distance_from_origin': Array(57.521, dtype=float32), 'eval/episode_forward_reward': Array(19.219, dtype=float32), 'eval/episode_reward': Array(308.961, dtype=float32), 'eval/episode_reward_alive': Array(321.758, dtype=float32), 'eval/episode_reward_linvel': Array(19.219, dtype=float32), 'eval/episode_reward_quadctrl': Array(-32.016, dtype=float32), 'eval/episode_x_position': Array(9.177, dtype=float32), 'eval/episode_x_velocity': Array(15.375, dtype=float32), 'eval/episode_y_position': Array(3.514, dtype=float32), 'eval/episode_y_velocity': Array(5.468, dtype=float32), 'eval/episode_distance_from_origin_std': Array(11.123, dtype=float32), 'eval/episode_forward_reward_std': Array(14.471, dtype=float32), 'eval/episode_reward_std': Array(55.836, dtype=float32), 'eval/episode_reward_alive_std': Array(60.894, dtype=float32), 'eval/episode_reward_linvel_std': Array(14.471, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(6.659, dtype=float32), 'eval/episode_x_position_std': Array(5.199, dtype=float32), 'eval/episode_x_velocity_std': Array(11.577, dtype=float32), 'eval/episode_y_position_std': Array(5.073, dtype=float32), 'eval/episode_y_velocity_std': Array(10.074, dtype=float32), 'eval/avg_episode_length': Array(64.352, dtype=float32), 'eval/epoch_eval_time': 78.4671835899353, 'eval/sps': 1631.2551839367673}
{'eval/walltime': 348.15486884117126, 'training/sps': 14891.861259092593, 'training/walltime': 530.0498032569885, 'training/entropy_loss': Array(-0.009, dtype=float32), 'training/policy_loss': Array(-0.01, dtype=float32), 'training/total_loss': Array(0.023, dtype=float32), 'training/v_loss': Array(0.042, dtype=float32), 'eval/episode_distance_from_origin': Array(75.705, dtype=float32), 'eval/episode_forward_reward': Array(27.775, dtype=float32), 'eval/episode_reward': Array(397.329, dtype=float32), 'eval/episode_reward_alive': Array(409.023, dtype=float32), 'eval/episode_reward_linvel': Array(27.775, dtype=float32), 'eval/episode_reward_quadctrl': Array(-39.469, dtype=float32), 'eval/episode_x_position': Array(18.389, dtype=float32), 'eval/episode_x_velocity': Array(22.22, dtype=float32), 'eval/episode_y_position': Array(6.643, dtype=float32), 'eval/episode_y_velocity': Array(5.416, dtype=float32), 'eval/episode_distance_from_origin_std': Array(24.326, dtype=float32), 'eval/episode_forward_reward_std': Array(13.762, dtype=float32), 'eval/episode_reward_std': Array(104.453, dtype=float32), 'eval/episode_reward_alive_std': Array(112.135, dtype=float32), 'eval/episode_reward_linvel_std': Array(13.762, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(11.788, dtype=float32), 'eval/episode_x_position_std': Array(13.734, dtype=float32), 'eval/episode_x_velocity_std': Array(11.01, dtype=float32), 'eval/episode_y_position_std': Array(11.064, dtype=float32), 'eval/episode_y_velocity_std': Array(12.574, dtype=float32), 'eval/avg_episode_length': Array(81.805, dtype=float32), 'eval/epoch_eval_time': 78.55048251152039, 'eval/sps': 1629.5253180809837}
time to jit: 0:01:25.893083
time to train: 0:13:26.549003

--------

Iteration0-Robot3

--------

{'eval/walltime': 50.29528784751892, 'training/sps': 27535.248383369948, 'training/walltime': 95.20306348800659, 'training/entropy_loss': Array(-0.013, dtype=float32), 'training/policy_loss': Array(-0., dtype=float32), 'training/total_loss': Array(0.105, dtype=float32), 'training/v_loss': Array(0.118, dtype=float32), 'eval/episode_distance_from_origin': Array(35.964, dtype=float32), 'eval/episode_forward_reward': Array(8.149, dtype=float32), 'eval/episode_reward': Array(200.28, dtype=float32), 'eval/episode_reward_alive': Array(216.406, dtype=float32), 'eval/episode_reward_linvel': Array(8.149, dtype=float32), 'eval/episode_reward_quadctrl': Array(-24.276, dtype=float32), 'eval/episode_x_position': Array(3.159, dtype=float32), 'eval/episode_x_velocity': Array(6.52, dtype=float32), 'eval/episode_y_position': Array(0.838, dtype=float32), 'eval/episode_y_velocity': Array(1.388, dtype=float32), 'eval/episode_distance_from_origin_std': Array(7.653, dtype=float32), 'eval/episode_forward_reward_std': Array(9.863, dtype=float32), 'eval/episode_reward_std': Array(40.395, dtype=float32), 'eval/episode_reward_alive_std': Array(45.77, dtype=float32), 'eval/episode_reward_linvel_std': Array(9.863, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(5.367, dtype=float32), 'eval/episode_x_position_std': Array(2.527, dtype=float32), 'eval/episode_x_velocity_std': Array(7.89, dtype=float32), 'eval/episode_y_position_std': Array(3.227, dtype=float32), 'eval/episode_y_velocity_std': Array(9.077, dtype=float32), 'eval/avg_episode_length': Array(43.281, dtype=float32), 'eval/epoch_eval_time': 10.421798944473267, 'eval/sps': 12281.948700217352}
{'eval/walltime': 60.69901776313782, 'training/sps': 48399.27539528133, 'training/walltime': 149.36585783958435, 'training/entropy_loss': Array(-0.011, dtype=float32), 'training/policy_loss': Array(-0.014, dtype=float32), 'training/total_loss': Array(0.029, dtype=float32), 'training/v_loss': Array(0.055, dtype=float32), 'eval/episode_distance_from_origin': Array(47.637, dtype=float32), 'eval/episode_forward_reward': Array(14.177, dtype=float32), 'eval/episode_reward': Array(268.128, dtype=float32), 'eval/episode_reward_alive': Array(283.672, dtype=float32), 'eval/episode_reward_linvel': Array(14.177, dtype=float32), 'eval/episode_reward_quadctrl': Array(-29.721, dtype=float32), 'eval/episode_x_position': Array(6.157, dtype=float32), 'eval/episode_x_velocity': Array(11.342, dtype=float32), 'eval/episode_y_position': Array(0.643, dtype=float32), 'eval/episode_y_velocity': Array(0.318, dtype=float32), 'eval/episode_distance_from_origin_std': Array(9.538, dtype=float32), 'eval/episode_forward_reward_std': Array(12.633, dtype=float32), 'eval/episode_reward_std': Array(51.604, dtype=float32), 'eval/episode_reward_alive_std': Array(58.768, dtype=float32), 'eval/episode_reward_linvel_std': Array(12.633, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(6.584, dtype=float32), 'eval/episode_x_position_std': Array(3.222, dtype=float32), 'eval/episode_x_velocity_std': Array(10.107, dtype=float32), 'eval/episode_y_position_std': Array(3.773, dtype=float32), 'eval/episode_y_velocity_std': Array(8.552, dtype=float32), 'eval/avg_episode_length': Array(56.734, dtype=float32), 'eval/epoch_eval_time': 10.403729915618896, 'eval/sps': 12303.279788899205}
{'eval/walltime': 71.1031060218811, 'training/sps': 48373.91017971032, 'training/walltime': 203.55705285072327, 'training/entropy_loss': Array(-0.01, dtype=float32), 'training/policy_loss': Array(-0.01, dtype=float32), 'training/total_loss': Array(0.022, dtype=float32), 'training/v_loss': Array(0.043, dtype=float32), 'eval/episode_distance_from_origin': Array(52.408, dtype=float32), 'eval/episode_forward_reward': Array(18.624, dtype=float32), 'eval/episode_reward': Array(297.878, dtype=float32), 'eval/episode_reward_alive': Array(308.984, dtype=float32), 'eval/episode_reward_linvel': Array(18.624, dtype=float32), 'eval/episode_reward_quadctrl': Array(-29.73, dtype=float32), 'eval/episode_x_position': Array(8.381, dtype=float32), 'eval/episode_x_velocity': Array(14.899, dtype=float32), 'eval/episode_y_position': Array(0.816, dtype=float32), 'eval/episode_y_velocity': Array(0.523, dtype=float32), 'eval/episode_distance_from_origin_std': Array(10.998, dtype=float32), 'eval/episode_forward_reward_std': Array(13.628, dtype=float32), 'eval/episode_reward_std': Array(59.904, dtype=float32), 'eval/episode_reward_alive_std': Array(65.964, dtype=float32), 'eval/episode_reward_linvel_std': Array(13.628, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(6.904, dtype=float32), 'eval/episode_x_position_std': Array(4.371, dtype=float32), 'eval/episode_x_velocity_std': Array(10.903, dtype=float32), 'eval/episode_y_position_std': Array(3.795, dtype=float32), 'eval/episode_y_velocity_std': Array(7.194, dtype=float32), 'eval/avg_episode_length': Array(61.797, dtype=float32), 'eval/epoch_eval_time': 10.404088258743286, 'eval/sps': 12302.85603281312}
{'eval/walltime': 81.51548361778259, 'training/sps': 48384.68258520705, 'training/walltime': 257.73618268966675, 'training/entropy_loss': Array(-0.009, dtype=float32), 'training/policy_loss': Array(-0.011, dtype=float32), 'training/total_loss': Array(0.014, dtype=float32), 'training/v_loss': Array(0.034, dtype=float32), 'eval/episode_distance_from_origin': Array(65.887, dtype=float32), 'eval/episode_forward_reward': Array(23.209, dtype=float32), 'eval/episode_reward': Array(376.159, dtype=float32), 'eval/episode_reward_alive': Array(388.359, dtype=float32), 'eval/episode_reward_linvel': Array(23.209, dtype=float32), 'eval/episode_reward_quadctrl': Array(-35.409, dtype=float32), 'eval/episode_x_position': Array(12.815, dtype=float32), 'eval/episode_x_velocity': Array(18.567, dtype=float32), 'eval/episode_y_position': Array(0.089, dtype=float32), 'eval/episode_y_velocity': Array(-1.972, dtype=float32), 'eval/episode_distance_from_origin_std': Array(13.154, dtype=float32), 'eval/episode_forward_reward_std': Array(13.298, dtype=float32), 'eval/episode_reward_std': Array(70.984, dtype=float32), 'eval/episode_reward_alive_std': Array(80.049, dtype=float32), 'eval/episode_reward_linvel_std': Array(13.298, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(8.215, dtype=float32), 'eval/episode_x_position_std': Array(5.125, dtype=float32), 'eval/episode_x_velocity_std': Array(10.639, dtype=float32), 'eval/episode_y_position_std': Array(3.684, dtype=float32), 'eval/episode_y_velocity_std': Array(6.477, dtype=float32), 'eval/avg_episode_length': Array(77.672, dtype=float32), 'eval/epoch_eval_time': 10.41237759590149, 'eval/sps': 12293.061677899892}
time to jit: 0:00:56.050584
time to train: 0:04:59.504868
1

--------

Iteration1-Robot0

--------

{'eval/walltime': 58.18452715873718, 'training/sps': 23609.7881400697, 'training/walltime': 111.03191542625427, 'training/entropy_loss': Array(-0.011, dtype=float32), 'training/policy_loss': Array(0.012, dtype=float32), 'training/total_loss': Array(0.111, dtype=float32), 'training/v_loss': Array(0.11, dtype=float32), 'eval/episode_distance_from_origin': Array(43.906, dtype=float32), 'eval/episode_forward_reward': Array(14.425, dtype=float32), 'eval/episode_reward': Array(249.143, dtype=float32), 'eval/episode_reward_alive': Array(264.844, dtype=float32), 'eval/episode_reward_linvel': Array(14.425, dtype=float32), 'eval/episode_reward_quadctrl': Array(-30.126, dtype=float32), 'eval/episode_x_position': Array(6.267, dtype=float32), 'eval/episode_x_velocity': Array(11.54, dtype=float32), 'eval/episode_y_position': Array(-0.18, dtype=float32), 'eval/episode_y_velocity': Array(-1.884, dtype=float32), 'eval/episode_distance_from_origin_std': Array(8.533, dtype=float32), 'eval/episode_forward_reward_std': Array(11.21, dtype=float32), 'eval/episode_reward_std': Array(46.87, dtype=float32), 'eval/episode_reward_alive_std': Array(51.833, dtype=float32), 'eval/episode_reward_linvel_std': Array(11.21, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(6.466, dtype=float32), 'eval/episode_x_position_std': Array(3.523, dtype=float32), 'eval/episode_x_velocity_std': Array(8.968, dtype=float32), 'eval/episode_y_position_std': Array(3.815, dtype=float32), 'eval/episode_y_velocity_std': Array(8.837, dtype=float32), 'eval/avg_episode_length': Array(52.969, dtype=float32), 'eval/epoch_eval_time': 19.88267683982849, 'eval/sps': 6437.764946397637}
{'eval/walltime': 68.57278990745544, 'training/sps': 48546.468012293415, 'training/walltime': 165.03048849105835, 'training/entropy_loss': Array(-0.01, dtype=float32), 'training/policy_loss': Array(-0.013, dtype=float32), 'training/total_loss': Array(0.035, dtype=float32), 'training/v_loss': Array(0.058, dtype=float32), 'eval/episode_distance_from_origin': Array(60.472, dtype=float32), 'eval/episode_forward_reward': Array(26.78, dtype=float32), 'eval/episode_reward': Array(341.253, dtype=float32), 'eval/episode_reward_alive': Array(352.188, dtype=float32), 'eval/episode_reward_linvel': Array(26.78, dtype=float32), 'eval/episode_reward_quadctrl': Array(-37.714, dtype=float32), 'eval/episode_x_position': Array(14.604, dtype=float32), 'eval/episode_x_velocity': Array(21.424, dtype=float32), 'eval/episode_y_position': Array(-0.896, dtype=float32), 'eval/episode_y_velocity': Array(-3.588, dtype=float32), 'eval/episode_distance_from_origin_std': Array(14.343, dtype=float32), 'eval/episode_forward_reward_std': Array(15.39, dtype=float32), 'eval/episode_reward_std': Array(75.647, dtype=float32), 'eval/episode_reward_alive_std': Array(82.267, dtype=float32), 'eval/episode_reward_linvel_std': Array(15.39, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(9.384, dtype=float32), 'eval/episode_x_position_std': Array(7.027, dtype=float32), 'eval/episode_x_velocity_std': Array(12.312, dtype=float32), 'eval/episode_y_position_std': Array(5.498, dtype=float32), 'eval/episode_y_velocity_std': Array(9.6, dtype=float32), 'eval/avg_episode_length': Array(70.438, dtype=float32), 'eval/epoch_eval_time': 10.388262748718262, 'eval/sps': 12321.598239878276}
{'eval/walltime': 78.96166205406189, 'training/sps': 48610.15026571499, 'training/walltime': 218.95832014083862, 'training/entropy_loss': Array(-0.009, dtype=float32), 'training/policy_loss': Array(-0.014, dtype=float32), 'training/total_loss': Array(0.026, dtype=float32), 'training/v_loss': Array(0.049, dtype=float32), 'eval/episode_distance_from_origin': Array(99.141, dtype=float32), 'eval/episode_forward_reward': Array(44.306, dtype=float32), 'eval/episode_reward': Array(516.11, dtype=float32), 'eval/episode_reward_alive': Array(526.641, dtype=float32), 'eval/episode_reward_linvel': Array(44.306, dtype=float32), 'eval/episode_reward_quadctrl': Array(-54.837, dtype=float32), 'eval/episode_x_position': Array(40.375, dtype=float32), 'eval/episode_x_velocity': Array(35.445, dtype=float32), 'eval/episode_y_position': Array(-1.526, dtype=float32), 'eval/episode_y_velocity': Array(-2.318, dtype=float32), 'eval/episode_distance_from_origin_std': Array(33.264, dtype=float32), 'eval/episode_forward_reward_std': Array(26.626, dtype=float32), 'eval/episode_reward_std': Array(144.432, dtype=float32), 'eval/episode_reward_alive_std': Array(142.182, dtype=float32), 'eval/episode_reward_linvel_std': Array(26.626, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(14.627, dtype=float32), 'eval/episode_x_position_std': Array(25.091, dtype=float32), 'eval/episode_x_velocity_std': Array(21.301, dtype=float32), 'eval/episode_y_position_std': Array(9.508, dtype=float32), 'eval/episode_y_velocity_std': Array(11.555, dtype=float32), 'eval/avg_episode_length': Array(105.328, dtype=float32), 'eval/epoch_eval_time': 10.388872146606445, 'eval/sps': 12320.87547076143}
{'eval/walltime': 89.37599515914917, 'training/sps': 48596.373362655446, 'training/walltime': 272.9014401435852, 'training/entropy_loss': Array(-0.008, dtype=float32), 'training/policy_loss': Array(-0.013, dtype=float32), 'training/total_loss': Array(0.037, dtype=float32), 'training/v_loss': Array(0.059, dtype=float32), 'eval/episode_distance_from_origin': Array(549.22, dtype=float32), 'eval/episode_forward_reward': Array(144.918, dtype=float32), 'eval/episode_reward': Array(1363.655, dtype=float32), 'eval/episode_reward_alive': Array(1355.664, dtype=float32), 'eval/episode_reward_linvel': Array(144.918, dtype=float32), 'eval/episode_reward_quadctrl': Array(-136.927, dtype=float32), 'eval/episode_x_position': Array(459.339, dtype=float32), 'eval/episode_x_velocity': Array(115.934, dtype=float32), 'eval/episode_y_position': Array(-70.106, dtype=float32), 'eval/episode_y_velocity': Array(-16.176, dtype=float32), 'eval/episode_distance_from_origin_std': Array(714.58, dtype=float32), 'eval/episode_forward_reward_std': Array(92.276, dtype=float32), 'eval/episode_reward_std': Array(788.768, dtype=float32), 'eval/episode_reward_alive_std': Array(778.667, dtype=float32), 'eval/episode_reward_linvel_std': Array(92.276, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(76.583, dtype=float32), 'eval/episode_x_position_std': Array(690.761, dtype=float32), 'eval/episode_x_velocity_std': Array(73.821, dtype=float32), 'eval/episode_y_position_std': Array(130.626, dtype=float32), 'eval/episode_y_velocity_std': Array(21.249, dtype=float32), 'eval/avg_episode_length': Array(271.133, dtype=float32), 'eval/epoch_eval_time': 10.41433310508728, 'eval/sps': 12290.7533980715}
time to jit: 0:00:51.425611
time to train: 0:05:24.104332

--------

Iteration1-Robot1

--------

{'eval/walltime': 47.403685569763184, 'training/sps': 35712.53546095467, 'training/walltime': 73.40391731262207, 'training/entropy_loss': Array(-0.012, dtype=float32), 'training/policy_loss': Array(0.001, dtype=float32), 'training/total_loss': Array(0.109, dtype=float32), 'training/v_loss': Array(0.121, dtype=float32), 'eval/episode_distance_from_origin': Array(45.893, dtype=float32), 'eval/episode_forward_reward': Array(10.237, dtype=float32), 'eval/episode_reward': Array(253.682, dtype=float32), 'eval/episode_reward_alive': Array(270.703, dtype=float32), 'eval/episode_reward_linvel': Array(10.237, dtype=float32), 'eval/episode_reward_quadctrl': Array(-27.258, dtype=float32), 'eval/episode_x_position': Array(4.068, dtype=float32), 'eval/episode_x_velocity': Array(8.19, dtype=float32), 'eval/episode_y_position': Array(1.015, dtype=float32), 'eval/episode_y_velocity': Array(2.464, dtype=float32), 'eval/episode_distance_from_origin_std': Array(10.088, dtype=float32), 'eval/episode_forward_reward_std': Array(12.904, dtype=float32), 'eval/episode_reward_std': Array(51.012, dtype=float32), 'eval/episode_reward_alive_std': Array(58.369, dtype=float32), 'eval/episode_reward_linvel_std': Array(12.904, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(6.345, dtype=float32), 'eval/episode_x_position_std': Array(3.159, dtype=float32), 'eval/episode_x_velocity_std': Array(10.323, dtype=float32), 'eval/episode_y_position_std': Array(5.035, dtype=float32), 'eval/episode_y_velocity_std': Array(10.815, dtype=float32), 'eval/avg_episode_length': Array(54.141, dtype=float32), 'eval/epoch_eval_time': 10.376658916473389, 'eval/sps': 12335.377025527412}
{'eval/walltime': 57.73685026168823, 'training/sps': 48802.66091646167, 'training/walltime': 127.11902117729187, 'training/entropy_loss': Array(-0.01, dtype=float32), 'training/policy_loss': Array(-0.015, dtype=float32), 'training/total_loss': Array(0.025, dtype=float32), 'training/v_loss': Array(0.05, dtype=float32), 'eval/episode_distance_from_origin': Array(65.484, dtype=float32), 'eval/episode_forward_reward': Array(20.412, dtype=float32), 'eval/episode_reward': Array(358.213, dtype=float32), 'eval/episode_reward_alive': Array(374.062, dtype=float32), 'eval/episode_reward_linvel': Array(20.412, dtype=float32), 'eval/episode_reward_quadctrl': Array(-36.262, dtype=float32), 'eval/episode_x_position': Array(12.095, dtype=float32), 'eval/episode_x_velocity': Array(16.33, dtype=float32), 'eval/episode_y_position': Array(5.973, dtype=float32), 'eval/episode_y_velocity': Array(7.487, dtype=float32), 'eval/episode_distance_from_origin_std': Array(16.678, dtype=float32), 'eval/episode_forward_reward_std': Array(17.325, dtype=float32), 'eval/episode_reward_std': Array(82.176, dtype=float32), 'eval/episode_reward_alive_std': Array(88.434, dtype=float32), 'eval/episode_reward_linvel_std': Array(17.325, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(8.866, dtype=float32), 'eval/episode_x_position_std': Array(9.578, dtype=float32), 'eval/episode_x_velocity_std': Array(13.86, dtype=float32), 'eval/episode_y_position_std': Array(7.115, dtype=float32), 'eval/episode_y_velocity_std': Array(9.953, dtype=float32), 'eval/avg_episode_length': Array(74.812, dtype=float32), 'eval/epoch_eval_time': 10.333164691925049, 'eval/sps': 12387.298936600404}
{'eval/walltime': 68.12728333473206, 'training/sps': 48575.646245021206, 'training/walltime': 181.08515858650208, 'training/entropy_loss': Array(-0.009, dtype=float32), 'training/policy_loss': Array(-0.013, dtype=float32), 'training/total_loss': Array(0.028, dtype=float32), 'training/v_loss': Array(0.049, dtype=float32), 'eval/episode_distance_from_origin': Array(91.826, dtype=float32), 'eval/episode_forward_reward': Array(34.572, dtype=float32), 'eval/episode_reward': Array(471.671, dtype=float32), 'eval/episode_reward_alive': Array(482.656, dtype=float32), 'eval/episode_reward_linvel': Array(34.572, dtype=float32), 'eval/episode_reward_quadctrl': Array(-45.557, dtype=float32), 'eval/episode_x_position': Array(31.811, dtype=float32), 'eval/episode_x_velocity': Array(27.657, dtype=float32), 'eval/episode_y_position': Array(12.451, dtype=float32), 'eval/episode_y_velocity': Array(10.093, dtype=float32), 'eval/episode_distance_from_origin_std': Array(34.753, dtype=float32), 'eval/episode_forward_reward_std': Array(20.065, dtype=float32), 'eval/episode_reward_std': Array(138.687, dtype=float32), 'eval/episode_reward_alive_std': Array(144.548, dtype=float32), 'eval/episode_reward_linvel_std': Array(20.065, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(13.764, dtype=float32), 'eval/episode_x_position_std': Array(24.014, dtype=float32), 'eval/episode_x_velocity_std': Array(16.052, dtype=float32), 'eval/episode_y_position_std': Array(12.931, dtype=float32), 'eval/episode_y_velocity_std': Array(13.918, dtype=float32), 'eval/avg_episode_length': Array(96.531, dtype=float32), 'eval/epoch_eval_time': 10.390433073043823, 'eval/sps': 12319.024539224818}
{'eval/walltime': 147.67981505393982, 'training/sps': 36021.44629612814, 'training/walltime': 253.85958242416382, 'training/entropy_loss': Array(-0.007, dtype=float32), 'training/policy_loss': Array(-0.011, dtype=float32), 'training/total_loss': Array(0.04, dtype=float32), 'training/v_loss': Array(0.059, dtype=float32), 'eval/episode_distance_from_origin': Array(247.591, dtype=float32), 'eval/episode_forward_reward': Array(75.621, dtype=float32), 'eval/episode_reward': Array(866.15, dtype=float32), 'eval/episode_reward_alive': Array(872.227, dtype=float32), 'eval/episode_reward_linvel': Array(75.621, dtype=float32), 'eval/episode_reward_quadctrl': Array(-81.698, dtype=float32), 'eval/episode_x_position': Array(167.142, dtype=float32), 'eval/episode_x_velocity': Array(60.497, dtype=float32), 'eval/episode_y_position': Array(51.723, dtype=float32), 'eval/episode_y_velocity': Array(11.536, dtype=float32), 'eval/episode_distance_from_origin_std': Array(298.203, dtype=float32), 'eval/episode_forward_reward_std': Array(47.205, dtype=float32), 'eval/episode_reward_std': Array(497.086, dtype=float32), 'eval/episode_reward_alive_std': Array(501.556, dtype=float32), 'eval/episode_reward_linvel_std': Array(47.205, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(46.626, dtype=float32), 'eval/episode_x_position_std': Array(266.547, dtype=float32), 'eval/episode_x_velocity_std': Array(37.764, dtype=float32), 'eval/episode_y_position_std': Array(102.518, dtype=float32), 'eval/episode_y_velocity_std': Array(20.462, dtype=float32), 'eval/avg_episode_length': Array(174.445, dtype=float32), 'eval/epoch_eval_time': 79.55253171920776, 'eval/sps': 1608.9997041425988}
time to jit: 0:00:51.028128
time to train: 0:06:04.686826

--------

Iteration1-Robot2

--------

{'eval/walltime': 118.31375050544739, 'training/sps': 18697.116176857653, 'training/walltime': 140.2055790424347, 'training/entropy_loss': Array(-0.011, dtype=float32), 'training/policy_loss': Array(0.021, dtype=float32), 'training/total_loss': Array(0.104, dtype=float32), 'training/v_loss': Array(0.094, dtype=float32), 'eval/episode_distance_from_origin': Array(51.489, dtype=float32), 'eval/episode_forward_reward': Array(16.465, dtype=float32), 'eval/episode_reward': Array(289.243, dtype=float32), 'eval/episode_reward_alive': Array(304.023, dtype=float32), 'eval/episode_reward_linvel': Array(16.465, dtype=float32), 'eval/episode_reward_quadctrl': Array(-31.246, dtype=float32), 'eval/episode_x_position': Array(8.165, dtype=float32), 'eval/episode_x_velocity': Array(13.172, dtype=float32), 'eval/episode_y_position': Array(0.43, dtype=float32), 'eval/episode_y_velocity': Array(-1.144, dtype=float32), 'eval/episode_distance_from_origin_std': Array(11.634, dtype=float32), 'eval/episode_forward_reward_std': Array(12.657, dtype=float32), 'eval/episode_reward_std': Array(63.661, dtype=float32), 'eval/episode_reward_alive_std': Array(70.373, dtype=float32), 'eval/episode_reward_linvel_std': Array(12.657, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(7.794, dtype=float32), 'eval/episode_x_position_std': Array(4.502, dtype=float32), 'eval/episode_x_velocity_std': Array(10.126, dtype=float32), 'eval/episode_y_position_std': Array(4.612, dtype=float32), 'eval/episode_y_velocity_std': Array(9.536, dtype=float32), 'eval/avg_episode_length': Array(60.805, dtype=float32), 'eval/epoch_eval_time': 10.542634010314941, 'eval/sps': 12141.178369159401}
{'eval/walltime': 128.74565815925598, 'training/sps': 48088.82794651872, 'training/walltime': 194.71803259849548, 'training/entropy_loss': Array(-0.009, dtype=float32), 'training/policy_loss': Array(-0.013, dtype=float32), 'training/total_loss': Array(0.032, dtype=float32), 'training/v_loss': Array(0.055, dtype=float32), 'eval/episode_distance_from_origin': Array(69.617, dtype=float32), 'eval/episode_forward_reward': Array(31.091, dtype=float32), 'eval/episode_reward': Array(388.077, dtype=float32), 'eval/episode_reward_alive': Array(395.586, dtype=float32), 'eval/episode_reward_linvel': Array(31.091, dtype=float32), 'eval/episode_reward_quadctrl': Array(-38.6, dtype=float32), 'eval/episode_x_position': Array(17.774, dtype=float32), 'eval/episode_x_velocity': Array(24.873, dtype=float32), 'eval/episode_y_position': Array(0.232, dtype=float32), 'eval/episode_y_velocity': Array(-2.637, dtype=float32), 'eval/episode_distance_from_origin_std': Array(17.122, dtype=float32), 'eval/episode_forward_reward_std': Array(16.575, dtype=float32), 'eval/episode_reward_std': Array(88.092, dtype=float32), 'eval/episode_reward_alive_std': Array(91.953, dtype=float32), 'eval/episode_reward_linvel_std': Array(16.575, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(9.161, dtype=float32), 'eval/episode_x_position_std': Array(9.228, dtype=float32), 'eval/episode_x_velocity_std': Array(13.26, dtype=float32), 'eval/episode_y_position_std': Array(6.038, dtype=float32), 'eval/episode_y_velocity_std': Array(8.636, dtype=float32), 'eval/avg_episode_length': Array(79.117, dtype=float32), 'eval/epoch_eval_time': 10.431907653808594, 'eval/sps': 12270.047267267399}
{'eval/walltime': 139.11547827720642, 'training/sps': 48230.08609801793, 'training/walltime': 249.07082796096802, 'training/entropy_loss': Array(-0.008, dtype=float32), 'training/policy_loss': Array(-0.014, dtype=float32), 'training/total_loss': Array(0.038, dtype=float32), 'training/v_loss': Array(0.06, dtype=float32), 'eval/episode_distance_from_origin': Array(139.586, dtype=float32), 'eval/episode_forward_reward': Array(68.963, dtype=float32), 'eval/episode_reward': Array(654.029, dtype=float32), 'eval/episode_reward_alive': Array(647.773, dtype=float32), 'eval/episode_reward_linvel': Array(68.963, dtype=float32), 'eval/episode_reward_quadctrl': Array(-62.707, dtype=float32), 'eval/episode_x_position': Array(74.474, dtype=float32), 'eval/episode_x_velocity': Array(55.17, dtype=float32), 'eval/episode_y_position': Array(0.241, dtype=float32), 'eval/episode_y_velocity': Array(-3.482, dtype=float32), 'eval/episode_distance_from_origin_std': Array(64.461, dtype=float32), 'eval/episode_forward_reward_std': Array(30.712, dtype=float32), 'eval/episode_reward_std': Array(212.995, dtype=float32), 'eval/episode_reward_alive_std': Array(213.583, dtype=float32), 'eval/episode_reward_linvel_std': Array(30.712, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(20.324, dtype=float32), 'eval/episode_x_position_std': Array(54.315, dtype=float32), 'eval/episode_x_velocity_std': Array(24.569, dtype=float32), 'eval/episode_y_position_std': Array(14.33, dtype=float32), 'eval/episode_y_velocity_std': Array(13.814, dtype=float32), 'eval/avg_episode_length': Array(129.555, dtype=float32), 'eval/epoch_eval_time': 10.36982011795044, 'eval/sps': 12343.51209028482}
{'eval/walltime': 149.53020024299622, 'training/sps': 48319.924023513006, 'training/walltime': 303.3225688934326, 'training/entropy_loss': Array(-0.007, dtype=float32), 'training/policy_loss': Array(-0.012, dtype=float32), 'training/total_loss': Array(0.051, dtype=float32), 'training/v_loss': Array(0.07, dtype=float32), 'eval/episode_distance_from_origin': Array(463.367, dtype=float32), 'eval/episode_forward_reward': Array(155.038, dtype=float32), 'eval/episode_reward': Array(1175.659, dtype=float32), 'eval/episode_reward_alive': Array(1129.609, dtype=float32), 'eval/episode_reward_linvel': Array(155.038, dtype=float32), 'eval/episode_reward_quadctrl': Array(-108.989, dtype=float32), 'eval/episode_x_position': Array(388.785, dtype=float32), 'eval/episode_x_velocity': Array(124.031, dtype=float32), 'eval/episode_y_position': Array(-24.877, dtype=float32), 'eval/episode_y_velocity': Array(-14.626, dtype=float32), 'eval/episode_distance_from_origin_std': Array(493.044, dtype=float32), 'eval/episode_forward_reward_std': Array(82.44, dtype=float32), 'eval/episode_reward_std': Array(596.693, dtype=float32), 'eval/episode_reward_alive_std': Array(576.357, dtype=float32), 'eval/episode_reward_linvel_std': Array(82.44, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(55.313, dtype=float32), 'eval/episode_x_position_std': Array(476.738, dtype=float32), 'eval/episode_x_velocity_std': Array(65.952, dtype=float32), 'eval/episode_y_position_std': Array(89.279, dtype=float32), 'eval/episode_y_velocity_std': Array(26.791, dtype=float32), 'eval/avg_episode_length': Array(225.922, dtype=float32), 'eval/epoch_eval_time': 10.414721965789795, 'eval/sps': 12290.294490861446}
time to jit: 0:02:02.279653
time to train: 0:05:45.217176

--------

Iteration1-Robot3

--------

{'eval/walltime': 119.18608951568604, 'training/sps': 27775.18890657298, 'training/walltime': 94.38063621520996, 'training/entropy_loss': Array(-0.013, dtype=float32), 'training/policy_loss': Array(-0.002, dtype=float32), 'training/total_loss': Array(0.086, dtype=float32), 'training/v_loss': Array(0.101, dtype=float32), 'eval/episode_distance_from_origin': Array(37.783, dtype=float32), 'eval/episode_forward_reward': Array(10.527, dtype=float32), 'eval/episode_reward': Array(201.043, dtype=float32), 'eval/episode_reward_alive': Array(216.797, dtype=float32), 'eval/episode_reward_linvel': Array(10.527, dtype=float32), 'eval/episode_reward_quadctrl': Array(-26.28, dtype=float32), 'eval/episode_x_position': Array(3.409, dtype=float32), 'eval/episode_x_velocity': Array(8.422, dtype=float32), 'eval/episode_y_position': Array(0.891, dtype=float32), 'eval/episode_y_velocity': Array(2.729, dtype=float32), 'eval/episode_distance_from_origin_std': Array(6.999, dtype=float32), 'eval/episode_forward_reward_std': Array(10.278, dtype=float32), 'eval/episode_reward_std': Array(34.4, dtype=float32), 'eval/episode_reward_alive_std': Array(39.274, dtype=float32), 'eval/episode_reward_linvel_std': Array(10.278, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(4.931, dtype=float32), 'eval/episode_x_position_std': Array(2.264, dtype=float32), 'eval/episode_x_velocity_std': Array(8.222, dtype=float32), 'eval/episode_y_position_std': Array(2.962, dtype=float32), 'eval/episode_y_velocity_std': Array(8.765, dtype=float32), 'eval/avg_episode_length': Array(43.359, dtype=float32), 'eval/epoch_eval_time': 10.526472330093384, 'eval/sps': 12159.819166965355}
{'eval/walltime': 131.92201590538025, 'training/sps': 32691.800266517268, 'training/walltime': 174.5671043395996, 'training/entropy_loss': Array(-0.012, dtype=float32), 'training/policy_loss': Array(-0.016, dtype=float32), 'training/total_loss': Array(0.025, dtype=float32), 'training/v_loss': Array(0.052, dtype=float32), 'eval/episode_distance_from_origin': Array(54.124, dtype=float32), 'eval/episode_forward_reward': Array(20.522, dtype=float32), 'eval/episode_reward': Array(288.782, dtype=float32), 'eval/episode_reward_alive': Array(302.617, dtype=float32), 'eval/episode_reward_linvel': Array(20.522, dtype=float32), 'eval/episode_reward_quadctrl': Array(-34.357, dtype=float32), 'eval/episode_x_position': Array(9.401, dtype=float32), 'eval/episode_x_velocity': Array(16.418, dtype=float32), 'eval/episode_y_position': Array(0.579, dtype=float32), 'eval/episode_y_velocity': Array(3.054, dtype=float32), 'eval/episode_distance_from_origin_std': Array(10.404, dtype=float32), 'eval/episode_forward_reward_std': Array(16.852, dtype=float32), 'eval/episode_reward_std': Array(53.747, dtype=float32), 'eval/episode_reward_alive_std': Array(56.125, dtype=float32), 'eval/episode_reward_linvel_std': Array(16.852, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(6.432, dtype=float32), 'eval/episode_x_position_std': Array(5.339, dtype=float32), 'eval/episode_x_velocity_std': Array(13.482, dtype=float32), 'eval/episode_y_position_std': Array(4.187, dtype=float32), 'eval/episode_y_velocity_std': Array(8.995, dtype=float32), 'eval/avg_episode_length': Array(60.523, dtype=float32), 'eval/epoch_eval_time': 12.735926389694214, 'eval/sps': 10050.309344091085}
{'eval/walltime': 211.9391577243805, 'training/sps': 18593.132801518652, 'training/walltime': 315.55679273605347, 'training/entropy_loss': Array(-0.01, dtype=float32), 'training/policy_loss': Array(-0.014, dtype=float32), 'training/total_loss': Array(0.03, dtype=float32), 'training/v_loss': Array(0.055, dtype=float32), 'eval/episode_distance_from_origin': Array(73.748, dtype=float32), 'eval/episode_forward_reward': Array(38.532, dtype=float32), 'eval/episode_reward': Array(380.049, dtype=float32), 'eval/episode_reward_alive': Array(383.867, dtype=float32), 'eval/episode_reward_linvel': Array(38.532, dtype=float32), 'eval/episode_reward_quadctrl': Array(-42.35, dtype=float32), 'eval/episode_x_position': Array(23.96, dtype=float32), 'eval/episode_x_velocity': Array(30.826, dtype=float32), 'eval/episode_y_position': Array(2.502, dtype=float32), 'eval/episode_y_velocity': Array(6.406, dtype=float32), 'eval/episode_distance_from_origin_std': Array(19.44, dtype=float32), 'eval/episode_forward_reward_std': Array(23.479, dtype=float32), 'eval/episode_reward_std': Array(83.184, dtype=float32), 'eval/episode_reward_alive_std': Array(82.128, dtype=float32), 'eval/episode_reward_linvel_std': Array(23.479, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(9.09, dtype=float32), 'eval/episode_x_position_std': Array(14.57, dtype=float32), 'eval/episode_x_velocity_std': Array(18.783, dtype=float32), 'eval/episode_y_position_std': Array(6.782, dtype=float32), 'eval/episode_y_velocity_std': Array(11.872, dtype=float32), 'eval/avg_episode_length': Array(76.773, dtype=float32), 'eval/epoch_eval_time': 80.01714181900024, 'eval/sps': 1599.6572370647475}
time to jit: 0:02:03.843959
time to train: 0:06:59.006926
2

--------

Iteration2-Robot0

--------

{'eval/walltime': 49.069199085235596, 'training/sps': 35714.16388153812, 'training/walltime': 73.40057039260864, 'training/entropy_loss': Array(-0.009, dtype=float32), 'training/policy_loss': Array(0.016, dtype=float32), 'training/total_loss': Array(0.119, dtype=float32), 'training/v_loss': Array(0.112, dtype=float32), 'eval/episode_distance_from_origin': Array(66.176, dtype=float32), 'eval/episode_forward_reward': Array(29.644, dtype=float32), 'eval/episode_reward': Array(354.014, dtype=float32), 'eval/episode_reward_alive': Array(372.812, dtype=float32), 'eval/episode_reward_linvel': Array(29.644, dtype=float32), 'eval/episode_reward_quadctrl': Array(-48.443, dtype=float32), 'eval/episode_x_position': Array(20.539, dtype=float32), 'eval/episode_x_velocity': Array(23.715, dtype=float32), 'eval/episode_y_position': Array(-7.098, dtype=float32), 'eval/episode_y_velocity': Array(-10.228, dtype=float32), 'eval/episode_distance_from_origin_std': Array(21.76, dtype=float32), 'eval/episode_forward_reward_std': Array(20.836, dtype=float32), 'eval/episode_reward_std': Array(93.895, dtype=float32), 'eval/episode_reward_alive_std': Array(96.107, dtype=float32), 'eval/episode_reward_linvel_std': Array(20.836, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(12.605, dtype=float32), 'eval/episode_x_position_std': Array(16.08, dtype=float32), 'eval/episode_x_velocity_std': Array(16.669, dtype=float32), 'eval/episode_y_position_std': Array(7.529, dtype=float32), 'eval/episode_y_velocity_std': Array(10.812, dtype=float32), 'eval/avg_episode_length': Array(74.562, dtype=float32), 'eval/epoch_eval_time': 10.44939661026001, 'eval/sps': 12249.511122424035}
{'eval/walltime': 59.45300483703613, 'training/sps': 48616.54313733857, 'training/walltime': 127.3213107585907, 'training/entropy_loss': Array(-0.008, dtype=float32), 'training/policy_loss': Array(-0.014, dtype=float32), 'training/total_loss': Array(0.078, dtype=float32), 'training/v_loss': Array(0.1, dtype=float32), 'eval/episode_distance_from_origin': Array(273.128, dtype=float32), 'eval/episode_forward_reward': Array(116.059, dtype=float32), 'eval/episode_reward': Array(841.479, dtype=float32), 'eval/episode_reward_alive': Array(828.867, dtype=float32), 'eval/episode_reward_linvel': Array(116.059, dtype=float32), 'eval/episode_reward_quadctrl': Array(-103.447, dtype=float32), 'eval/episode_x_position': Array(207.263, dtype=float32), 'eval/episode_x_velocity': Array(92.847, dtype=float32), 'eval/episode_y_position': Array(-55.526, dtype=float32), 'eval/episode_y_velocity': Array(-24.8, dtype=float32), 'eval/episode_distance_from_origin_std': Array(233.88, dtype=float32), 'eval/episode_forward_reward_std': Array(70.322, dtype=float32), 'eval/episode_reward_std': Array(399.156, dtype=float32), 'eval/episode_reward_alive_std': Array(383.451, dtype=float32), 'eval/episode_reward_linvel_std': Array(70.322, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(47.205, dtype=float32), 'eval/episode_x_position_std': Array(217.024, dtype=float32), 'eval/episode_x_velocity_std': Array(56.257, dtype=float32), 'eval/episode_y_position_std': Array(65.397, dtype=float32), 'eval/episode_y_velocity_std': Array(22.132, dtype=float32), 'eval/avg_episode_length': Array(165.773, dtype=float32), 'eval/epoch_eval_time': 10.383805751800537, 'eval/sps': 12326.886987249833}
{'eval/walltime': 69.80615091323853, 'training/sps': 48557.804848648026, 'training/walltime': 181.30727672576904, 'training/entropy_loss': Array(-0.007, dtype=float32), 'training/policy_loss': Array(-0.011, dtype=float32), 'training/total_loss': Array(0.062, dtype=float32), 'training/v_loss': Array(0.08, dtype=float32), 'eval/episode_distance_from_origin': Array(3064.604, dtype=float32), 'eval/episode_forward_reward': Array(467.106, dtype=float32), 'eval/episode_reward': Array(2599.6, dtype=float32), 'eval/episode_reward_alive': Array(2423.945, dtype=float32), 'eval/episode_reward_linvel': Array(467.106, dtype=float32), 'eval/episode_reward_quadctrl': Array(-291.451, dtype=float32), 'eval/episode_x_position': Array(2932.108, dtype=float32), 'eval/episode_x_velocity': Array(373.686, dtype=float32), 'eval/episode_y_position': Array(-592.135, dtype=float32), 'eval/episode_y_velocity': Array(-76.764, dtype=float32), 'eval/episode_distance_from_origin_std': Array(3174.785, dtype=float32), 'eval/episode_forward_reward_std': Array(285.418, dtype=float32), 'eval/episode_reward_std': Array(1557.211, dtype=float32), 'eval/episode_reward_alive_std': Array(1446.403, dtype=float32), 'eval/episode_reward_linvel_std': Array(285.418, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(171.564, dtype=float32), 'eval/episode_x_position_std': Array(3097.717, dtype=float32), 'eval/episode_x_velocity_std': Array(228.335, dtype=float32), 'eval/episode_y_position_std': Array(663.808, dtype=float32), 'eval/episode_y_velocity_std': Array(55.836, dtype=float32), 'eval/avg_episode_length': Array(484.789, dtype=float32), 'eval/epoch_eval_time': 10.353146076202393, 'eval/sps': 12363.391674171307}
{'eval/walltime': 80.18164777755737, 'training/sps': 48480.84071440999, 'training/walltime': 235.3789463043213, 'training/entropy_loss': Array(-0.005, dtype=float32), 'training/policy_loss': Array(-0.01, dtype=float32), 'training/total_loss': Array(0.029, dtype=float32), 'training/v_loss': Array(0.045, dtype=float32), 'eval/episode_distance_from_origin': Array(7124.355, dtype=float32), 'eval/episode_forward_reward': Array(897.954, dtype=float32), 'eval/episode_reward': Array(3736.523, dtype=float32), 'eval/episode_reward_alive': Array(3226.641, dtype=float32), 'eval/episode_reward_linvel': Array(897.954, dtype=float32), 'eval/episode_reward_quadctrl': Array(-388.071, dtype=float32), 'eval/episode_x_position': Array(7016.343, dtype=float32), 'eval/episode_x_velocity': Array(718.365, dtype=float32), 'eval/episode_y_position': Array(-772.868, dtype=float32), 'eval/episode_y_velocity': Array(-83.169, dtype=float32), 'eval/episode_distance_from_origin_std': Array(5543.601, dtype=float32), 'eval/episode_forward_reward_std': Array(459.645, dtype=float32), 'eval/episode_reward_std': Array(1902.927, dtype=float32), 'eval/episode_reward_alive_std': Array(1638.156, dtype=float32), 'eval/episode_reward_linvel_std': Array(459.645, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(193.807, dtype=float32), 'eval/episode_x_position_std': Array(5497.999, dtype=float32), 'eval/episode_x_velocity_std': Array(367.718, dtype=float32), 'eval/episode_y_position_std': Array(679.612, dtype=float32), 'eval/episode_y_velocity_std': Array(52.214, dtype=float32), 'eval/avg_episode_length': Array(645.328, dtype=float32), 'eval/epoch_eval_time': 10.375496864318848, 'eval/sps': 12336.758583599958}
time to jit: 0:00:52.717485
time to train: 0:04:37.056075

--------

Iteration2-Robot1

--------

{'eval/walltime': 48.30380892753601, 'training/sps': 35953.26152165752, 'training/walltime': 72.91243934631348, 'training/entropy_loss': Array(nan, dtype=float32), 'training/policy_loss': Array(nan, dtype=float32), 'training/total_loss': Array(nan, dtype=float32), 'training/v_loss': Array(nan, dtype=float32), 'eval/episode_distance_from_origin': Array(nan, dtype=float32), 'eval/episode_forward_reward': Array(nan, dtype=float32), 'eval/episode_reward': Array(nan, dtype=float32), 'eval/episode_reward_alive': Array(5000., dtype=float32), 'eval/episode_reward_linvel': Array(nan, dtype=float32), 'eval/episode_reward_quadctrl': Array(nan, dtype=float32), 'eval/episode_x_position': Array(nan, dtype=float32), 'eval/episode_x_velocity': Array(nan, dtype=float32), 'eval/episode_y_position': Array(nan, dtype=float32), 'eval/episode_y_velocity': Array(nan, dtype=float32), 'eval/episode_distance_from_origin_std': Array(nan, dtype=float32), 'eval/episode_forward_reward_std': Array(nan, dtype=float32), 'eval/episode_reward_std': Array(nan, dtype=float32), 'eval/episode_reward_alive_std': Array(0., dtype=float32), 'eval/episode_reward_linvel_std': Array(nan, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(nan, dtype=float32), 'eval/episode_x_position_std': Array(nan, dtype=float32), 'eval/episode_x_velocity_std': Array(nan, dtype=float32), 'eval/episode_y_position_std': Array(nan, dtype=float32), 'eval/episode_y_velocity_std': Array(nan, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/epoch_eval_time': 10.356812000274658, 'eval/sps': 12359.01549594658}
{'eval/walltime': 58.81334042549133, 'training/sps': 49836.32477114139, 'training/walltime': 125.5134289264679, 'training/entropy_loss': Array(nan, dtype=float32), 'training/policy_loss': Array(nan, dtype=float32), 'training/total_loss': Array(nan, dtype=float32), 'training/v_loss': Array(nan, dtype=float32), 'eval/episode_distance_from_origin': Array(nan, dtype=float32), 'eval/episode_forward_reward': Array(nan, dtype=float32), 'eval/episode_reward': Array(nan, dtype=float32), 'eval/episode_reward_alive': Array(5000., dtype=float32), 'eval/episode_reward_linvel': Array(nan, dtype=float32), 'eval/episode_reward_quadctrl': Array(nan, dtype=float32), 'eval/episode_x_position': Array(nan, dtype=float32), 'eval/episode_x_velocity': Array(nan, dtype=float32), 'eval/episode_y_position': Array(nan, dtype=float32), 'eval/episode_y_velocity': Array(nan, dtype=float32), 'eval/episode_distance_from_origin_std': Array(nan, dtype=float32), 'eval/episode_forward_reward_std': Array(nan, dtype=float32), 'eval/episode_reward_std': Array(nan, dtype=float32), 'eval/episode_reward_alive_std': Array(0., dtype=float32), 'eval/episode_reward_linvel_std': Array(nan, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(nan, dtype=float32), 'eval/episode_x_position_std': Array(nan, dtype=float32), 'eval/episode_x_velocity_std': Array(nan, dtype=float32), 'eval/episode_y_position_std': Array(nan, dtype=float32), 'eval/episode_y_velocity_std': Array(nan, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/epoch_eval_time': 10.509531497955322, 'eval/sps': 12179.42017918715}
{'eval/walltime': 69.12482118606567, 'training/sps': 49708.714659122146, 'training/walltime': 178.2494535446167, 'training/entropy_loss': Array(nan, dtype=float32), 'training/policy_loss': Array(nan, dtype=float32), 'training/total_loss': Array(nan, dtype=float32), 'training/v_loss': Array(nan, dtype=float32), 'eval/episode_distance_from_origin': Array(nan, dtype=float32), 'eval/episode_forward_reward': Array(nan, dtype=float32), 'eval/episode_reward': Array(nan, dtype=float32), 'eval/episode_reward_alive': Array(5000., dtype=float32), 'eval/episode_reward_linvel': Array(nan, dtype=float32), 'eval/episode_reward_quadctrl': Array(nan, dtype=float32), 'eval/episode_x_position': Array(nan, dtype=float32), 'eval/episode_x_velocity': Array(nan, dtype=float32), 'eval/episode_y_position': Array(nan, dtype=float32), 'eval/episode_y_velocity': Array(nan, dtype=float32), 'eval/episode_distance_from_origin_std': Array(nan, dtype=float32), 'eval/episode_forward_reward_std': Array(nan, dtype=float32), 'eval/episode_reward_std': Array(nan, dtype=float32), 'eval/episode_reward_alive_std': Array(0., dtype=float32), 'eval/episode_reward_linvel_std': Array(nan, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(nan, dtype=float32), 'eval/episode_x_position_std': Array(nan, dtype=float32), 'eval/episode_x_velocity_std': Array(nan, dtype=float32), 'eval/episode_y_position_std': Array(nan, dtype=float32), 'eval/episode_y_velocity_std': Array(nan, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/epoch_eval_time': 10.31148076057434, 'eval/sps': 12413.348089578407}
{'eval/walltime': 79.57870125770569, 'training/sps': 49642.98056969456, 'training/walltime': 231.05530786514282, 'training/entropy_loss': Array(nan, dtype=float32), 'training/policy_loss': Array(nan, dtype=float32), 'training/total_loss': Array(nan, dtype=float32), 'training/v_loss': Array(nan, dtype=float32), 'eval/episode_distance_from_origin': Array(nan, dtype=float32), 'eval/episode_forward_reward': Array(nan, dtype=float32), 'eval/episode_reward': Array(nan, dtype=float32), 'eval/episode_reward_alive': Array(5000., dtype=float32), 'eval/episode_reward_linvel': Array(nan, dtype=float32), 'eval/episode_reward_quadctrl': Array(nan, dtype=float32), 'eval/episode_x_position': Array(nan, dtype=float32), 'eval/episode_x_velocity': Array(nan, dtype=float32), 'eval/episode_y_position': Array(nan, dtype=float32), 'eval/episode_y_velocity': Array(nan, dtype=float32), 'eval/episode_distance_from_origin_std': Array(nan, dtype=float32), 'eval/episode_forward_reward_std': Array(nan, dtype=float32), 'eval/episode_reward_std': Array(nan, dtype=float32), 'eval/episode_reward_alive_std': Array(0., dtype=float32), 'eval/episode_reward_linvel_std': Array(nan, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(nan, dtype=float32), 'eval/episode_x_position_std': Array(nan, dtype=float32), 'eval/episode_x_velocity_std': Array(nan, dtype=float32), 'eval/episode_y_position_std': Array(nan, dtype=float32), 'eval/episode_y_velocity_std': Array(nan, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/epoch_eval_time': 10.453880071640015, 'eval/sps': 12244.257550576553}

--------

Iteration2-Robot2

--------

{'eval/walltime': 48.304853439331055, 'training/sps': 35358.88874121247, 'training/walltime': 74.13807654380798, 'training/entropy_loss': Array(-0.006, dtype=float32), 'training/policy_loss': Array(0.02, dtype=float32), 'training/total_loss': Array(0.115, dtype=float32), 'training/v_loss': Array(0.102, dtype=float32), 'eval/episode_distance_from_origin': Array(82.795, dtype=float32), 'eval/episode_forward_reward': Array(28.807, dtype=float32), 'eval/episode_reward': Array(437.696, dtype=float32), 'eval/episode_reward_alive': Array(461.289, dtype=float32), 'eval/episode_reward_linvel': Array(28.807, dtype=float32), 'eval/episode_reward_quadctrl': Array(-52.4, dtype=float32), 'eval/episode_x_position': Array(21.645, dtype=float32), 'eval/episode_x_velocity': Array(23.046, dtype=float32), 'eval/episode_y_position': Array(9.981, dtype=float32), 'eval/episode_y_velocity': Array(8.729, dtype=float32), 'eval/episode_distance_from_origin_std': Array(25.904, dtype=float32), 'eval/episode_forward_reward_std': Array(21.858, dtype=float32), 'eval/episode_reward_std': Array(123.466, dtype=float32), 'eval/episode_reward_alive_std': Array(132.041, dtype=float32), 'eval/episode_reward_linvel_std': Array(21.858, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(14.567, dtype=float32), 'eval/episode_x_position_std': Array(14.842, dtype=float32), 'eval/episode_x_velocity_std': Array(17.487, dtype=float32), 'eval/episode_y_position_std': Array(10.929, dtype=float32), 'eval/episode_y_velocity_std': Array(11.062, dtype=float32), 'eval/avg_episode_length': Array(92.258, dtype=float32), 'eval/epoch_eval_time': 10.324756860733032, 'eval/sps': 12397.386372051798}
0

--------

Iteration0-Robot0

--------

Traceback (most recent call last):
  File "/home/name/Desktop/Codes/ScalingOptimization (1)/./TheAlgorithm.py", line 229, in <module>
    a,b,c,d= train_fn(environment=rob_env, progress_fn=progress, transfer_params=Candidates[k].brain, cmprsn_rewards=cmprsn_rewards)
  File "/home/name/Desktop/Codes/ScalingOptimization (1)/train.py", line 373, in tRain
    progress_fn(0, metrics, cmprsn_rewards)
  File "/home/name/Desktop/Codes/ScalingOptimization (1)/./TheAlgorithm.py", line 151, in progress
    savefig(fname = "plots/"+plot_name+".png", format = "png")
NameError: name 'savefig' is not defined
0

--------

Iteration0-Robot0

--------

{'eval/walltime': 45.994871854782104, 'training/sps': 36006.29050508475, 'training/walltime': 72.80505609512329, 'training/entropy_loss': Array(-0.013, dtype=float32), 'training/policy_loss': Array(0.001, dtype=float32), 'training/total_loss': Array(0.121, dtype=float32), 'training/v_loss': Array(0.132, dtype=float32), 'eval/episode_distance_from_origin': Array(35.324, dtype=float32), 'eval/episode_forward_reward': Array(6.563, dtype=float32), 'eval/episode_reward': Array(191.123, dtype=float32), 'eval/episode_reward_alive': Array(207.969, dtype=float32), 'eval/episode_reward_linvel': Array(6.563, dtype=float32), 'eval/episode_reward_quadctrl': Array(-23.409, dtype=float32), 'eval/episode_x_position': Array(2.617, dtype=float32), 'eval/episode_x_velocity': Array(5.251, dtype=float32), 'eval/episode_y_position': Array(0.976, dtype=float32), 'eval/episode_y_velocity': Array(1.751, dtype=float32), 'eval/episode_distance_from_origin_std': Array(8.783, dtype=float32), 'eval/episode_forward_reward_std': Array(9.313, dtype=float32), 'eval/episode_reward_std': Array(43.854, dtype=float32), 'eval/episode_reward_alive_std': Array(51.582, dtype=float32), 'eval/episode_reward_linvel_std': Array(9.313, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(6.067, dtype=float32), 'eval/episode_x_position_std': Array(2.238, dtype=float32), 'eval/episode_x_velocity_std': Array(7.451, dtype=float32), 'eval/episode_y_position_std': Array(3.236, dtype=float32), 'eval/episode_y_velocity_std': Array(8.841, dtype=float32), 'eval/avg_episode_length': Array(41.594, dtype=float32), 'eval/epoch_eval_time': 10.369933843612671, 'eval/sps': 12343.376720656826}
{'eval/walltime': 56.36502003669739, 'training/sps': 49226.4227941599, 'training/walltime': 126.05775761604309, 'training/entropy_loss': Array(-0.011, dtype=float32), 'training/policy_loss': Array(-0.014, dtype=float32), 'training/total_loss': Array(0.038, dtype=float32), 'training/v_loss': Array(0.064, dtype=float32), 'eval/episode_distance_from_origin': Array(47.051, dtype=float32), 'eval/episode_forward_reward': Array(11.527, dtype=float32), 'eval/episode_reward': Array(256.671, dtype=float32), 'eval/episode_reward_alive': Array(274.141, dtype=float32), 'eval/episode_reward_linvel': Array(11.527, dtype=float32), 'eval/episode_reward_quadctrl': Array(-28.996, dtype=float32), 'eval/episode_x_position': Array(5.226, dtype=float32), 'eval/episode_x_velocity': Array(9.222, dtype=float32), 'eval/episode_y_position': Array(1.511, dtype=float32), 'eval/episode_y_velocity': Array(2.998, dtype=float32), 'eval/episode_distance_from_origin_std': Array(8.955, dtype=float32), 'eval/episode_forward_reward_std': Array(13.515, dtype=float32), 'eval/episode_reward_std': Array(47.499, dtype=float32), 'eval/episode_reward_alive_std': Array(53.863, dtype=float32), 'eval/episode_reward_linvel_std': Array(13.515, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(6.088, dtype=float32), 'eval/episode_x_position_std': Array(3.825, dtype=float32), 'eval/episode_x_velocity_std': Array(10.812, dtype=float32), 'eval/episode_y_position_std': Array(4.147, dtype=float32), 'eval/episode_y_velocity_std': Array(9.59, dtype=float32), 'eval/avg_episode_length': Array(54.828, dtype=float32), 'eval/epoch_eval_time': 10.370148181915283, 'eval/sps': 12343.121598129317}
{'eval/walltime': 66.74090719223022, 'training/sps': 49044.23510721757, 'training/walltime': 179.5082802772522, 'training/entropy_loss': Array(-0.01, dtype=float32), 'training/policy_loss': Array(-0.011, dtype=float32), 'training/total_loss': Array(0.032, dtype=float32), 'training/v_loss': Array(0.053, dtype=float32), 'eval/episode_distance_from_origin': Array(54.676, dtype=float32), 'eval/episode_forward_reward': Array(15.371, dtype=float32), 'eval/episode_reward': Array(299.449, dtype=float32), 'eval/episode_reward_alive': Array(315.43, dtype=float32), 'eval/episode_reward_linvel': Array(15.371, dtype=float32), 'eval/episode_reward_quadctrl': Array(-31.351, dtype=float32), 'eval/episode_x_position': Array(7.795, dtype=float32), 'eval/episode_x_velocity': Array(12.296, dtype=float32), 'eval/episode_y_position': Array(3.468, dtype=float32), 'eval/episode_y_velocity': Array(5.37, dtype=float32), 'eval/episode_distance_from_origin_std': Array(11.487, dtype=float32), 'eval/episode_forward_reward_std': Array(14.395, dtype=float32), 'eval/episode_reward_std': Array(57.631, dtype=float32), 'eval/episode_reward_alive_std': Array(64.967, dtype=float32), 'eval/episode_reward_linvel_std': Array(14.395, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(7.149, dtype=float32), 'eval/episode_x_position_std': Array(5.345, dtype=float32), 'eval/episode_x_velocity_std': Array(11.516, dtype=float32), 'eval/episode_y_position_std': Array(4.694, dtype=float32), 'eval/episode_y_velocity_std': Array(8.561, dtype=float32), 'eval/avg_episode_length': Array(63.086, dtype=float32), 'eval/epoch_eval_time': 10.375887155532837, 'eval/sps': 12336.294533787917}
{'eval/walltime': 77.12585973739624, 'training/sps': 48873.1086151858, 'training/walltime': 233.14595699310303, 'training/entropy_loss': Array(-0.009, dtype=float32), 'training/policy_loss': Array(-0.01, dtype=float32), 'training/total_loss': Array(0.026, dtype=float32), 'training/v_loss': Array(0.045, dtype=float32), 'eval/episode_distance_from_origin': Array(70.761, dtype=float32), 'eval/episode_forward_reward': Array(21.88, dtype=float32), 'eval/episode_reward': Array(386.282, dtype=float32), 'eval/episode_reward_alive': Array(402.188, dtype=float32), 'eval/episode_reward_linvel': Array(21.88, dtype=float32), 'eval/episode_reward_quadctrl': Array(-37.786, dtype=float32), 'eval/episode_x_position': Array(12.785, dtype=float32), 'eval/episode_x_velocity': Array(17.504, dtype=float32), 'eval/episode_y_position': Array(7.071, dtype=float32), 'eval/episode_y_velocity': Array(7.08, dtype=float32), 'eval/episode_distance_from_origin_std': Array(17.017, dtype=float32), 'eval/episode_forward_reward_std': Array(14.953, dtype=float32), 'eval/episode_reward_std': Array(84.996, dtype=float32), 'eval/episode_reward_alive_std': Array(92.816, dtype=float32), 'eval/episode_reward_linvel_std': Array(14.953, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(9.238, dtype=float32), 'eval/episode_x_position_std': Array(6.698, dtype=float32), 'eval/episode_x_velocity_std': Array(11.963, dtype=float32), 'eval/episode_y_position_std': Array(7.68, dtype=float32), 'eval/episode_y_velocity_std': Array(9.658, dtype=float32), 'eval/avg_episode_length': Array(80.438, dtype=float32), 'eval/epoch_eval_time': 10.384952545166016, 'eval/sps': 12325.525749232374}
time to jit: 0:00:49.735058
time to train: 0:04:35.200626

--------

Iteration0-Robot1

--------

{'eval/walltime': 46.78485679626465, 'training/sps': 35930.95935990041, 'training/walltime': 72.95769572257996, 'training/entropy_loss': Array(-0.013, dtype=float32), 'training/policy_loss': Array(0.002, dtype=float32), 'training/total_loss': Array(0.117, dtype=float32), 'training/v_loss': Array(0.127, dtype=float32), 'eval/episode_distance_from_origin': Array(35.749, dtype=float32), 'eval/episode_forward_reward': Array(7.082, dtype=float32), 'eval/episode_reward': Array(191.353, dtype=float32), 'eval/episode_reward_alive': Array(207.617, dtype=float32), 'eval/episode_reward_linvel': Array(7.082, dtype=float32), 'eval/episode_reward_quadctrl': Array(-23.346, dtype=float32), 'eval/episode_x_position': Array(2.679, dtype=float32), 'eval/episode_x_velocity': Array(5.665, dtype=float32), 'eval/episode_y_position': Array(0.999, dtype=float32), 'eval/episode_y_velocity': Array(2.253, dtype=float32), 'eval/episode_distance_from_origin_std': Array(6.363, dtype=float32), 'eval/episode_forward_reward_std': Array(10.262, dtype=float32), 'eval/episode_reward_std': Array(32.672, dtype=float32), 'eval/episode_reward_alive_std': Array(37.002, dtype=float32), 'eval/episode_reward_linvel_std': Array(10.262, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(4.463, dtype=float32), 'eval/episode_x_position_std': Array(2.42, dtype=float32), 'eval/episode_x_velocity_std': Array(8.21, dtype=float32), 'eval/episode_y_position_std': Array(2.913, dtype=float32), 'eval/episode_y_velocity_std': Array(8.909, dtype=float32), 'eval/avg_episode_length': Array(41.523, dtype=float32), 'eval/epoch_eval_time': 10.376904487609863, 'eval/sps': 12335.085106819031}
{'eval/walltime': 57.171807527542114, 'training/sps': 48929.3748668057, 'training/walltime': 126.53369188308716, 'training/entropy_loss': Array(-0.011, dtype=float32), 'training/policy_loss': Array(-0.015, dtype=float32), 'training/total_loss': Array(0.028, dtype=float32), 'training/v_loss': Array(0.054, dtype=float32), 'eval/episode_distance_from_origin': Array(47.781, dtype=float32), 'eval/episode_forward_reward': Array(11.707, dtype=float32), 'eval/episode_reward': Array(255.295, dtype=float32), 'eval/episode_reward_alive': Array(272.344, dtype=float32), 'eval/episode_reward_linvel': Array(11.707, dtype=float32), 'eval/episode_reward_quadctrl': Array(-28.756, dtype=float32), 'eval/episode_x_position': Array(5.441, dtype=float32), 'eval/episode_x_velocity': Array(9.366, dtype=float32), 'eval/episode_y_position': Array(0.455, dtype=float32), 'eval/episode_y_velocity': Array(-0.164, dtype=float32), 'eval/episode_distance_from_origin_std': Array(9.015, dtype=float32), 'eval/episode_forward_reward_std': Array(13.384, dtype=float32), 'eval/episode_reward_std': Array(45.646, dtype=float32), 'eval/episode_reward_alive_std': Array(51.12, dtype=float32), 'eval/episode_reward_linvel_std': Array(13.384, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(5.791, dtype=float32), 'eval/episode_x_position_std': Array(3.598, dtype=float32), 'eval/episode_x_velocity_std': Array(10.707, dtype=float32), 'eval/episode_y_position_std': Array(4.962, dtype=float32), 'eval/episode_y_velocity_std': Array(10.188, dtype=float32), 'eval/avg_episode_length': Array(54.469, dtype=float32), 'eval/epoch_eval_time': 10.386950731277466, 'eval/sps': 12323.154630411691}
{'eval/walltime': 67.57076573371887, 'training/sps': 48759.75920587933, 'training/walltime': 180.29605746269226, 'training/entropy_loss': Array(-0.01, dtype=float32), 'training/policy_loss': Array(-0.011, dtype=float32), 'training/total_loss': Array(0.026, dtype=float32), 'training/v_loss': Array(0.047, dtype=float32), 'eval/episode_distance_from_origin': Array(53.542, dtype=float32), 'eval/episode_forward_reward': Array(17.576, dtype=float32), 'eval/episode_reward': Array(290.38, dtype=float32), 'eval/episode_reward_alive': Array(302.539, dtype=float32), 'eval/episode_reward_linvel': Array(17.576, dtype=float32), 'eval/episode_reward_quadctrl': Array(-29.735, dtype=float32), 'eval/episode_x_position': Array(8.234, dtype=float32), 'eval/episode_x_velocity': Array(14.061, dtype=float32), 'eval/episode_y_position': Array(2.084, dtype=float32), 'eval/episode_y_velocity': Array(3.978, dtype=float32), 'eval/episode_distance_from_origin_std': Array(10.422, dtype=float32), 'eval/episode_forward_reward_std': Array(15.039, dtype=float32), 'eval/episode_reward_std': Array(53.343, dtype=float32), 'eval/episode_reward_alive_std': Array(58.763, dtype=float32), 'eval/episode_reward_linvel_std': Array(15.039, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(6.269, dtype=float32), 'eval/episode_x_position_std': Array(4.487, dtype=float32), 'eval/episode_x_velocity_std': Array(12.031, dtype=float32), 'eval/episode_y_position_std': Array(3.878, dtype=float32), 'eval/episode_y_velocity_std': Array(8.211, dtype=float32), 'eval/avg_episode_length': Array(60.508, dtype=float32), 'eval/epoch_eval_time': 10.398958206176758, 'eval/sps': 12308.925323305055}
{'eval/walltime': 77.9428129196167, 'training/sps': 48669.36498411273, 'training/walltime': 234.15827655792236, 'training/entropy_loss': Array(-0.009, dtype=float32), 'training/policy_loss': Array(-0.01, dtype=float32), 'training/total_loss': Array(0.02, dtype=float32), 'training/v_loss': Array(0.04, dtype=float32), 'eval/episode_distance_from_origin': Array(69.182, dtype=float32), 'eval/episode_forward_reward': Array(20.719, dtype=float32), 'eval/episode_reward': Array(370.171, dtype=float32), 'eval/episode_reward_alive': Array(385.742, dtype=float32), 'eval/episode_reward_linvel': Array(20.719, dtype=float32), 'eval/episode_reward_quadctrl': Array(-36.291, dtype=float32), 'eval/episode_x_position': Array(12.676, dtype=float32), 'eval/episode_x_velocity': Array(16.575, dtype=float32), 'eval/episode_y_position': Array(3.968, dtype=float32), 'eval/episode_y_velocity': Array(4.645, dtype=float32), 'eval/episode_distance_from_origin_std': Array(15.847, dtype=float32), 'eval/episode_forward_reward_std': Array(17.104, dtype=float32), 'eval/episode_reward_std': Array(78.095, dtype=float32), 'eval/episode_reward_alive_std': Array(83.946, dtype=float32), 'eval/episode_reward_linvel_std': Array(17.104, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(8.497, dtype=float32), 'eval/episode_x_position_std': Array(7.818, dtype=float32), 'eval/episode_x_velocity_std': Array(13.683, dtype=float32), 'eval/episode_y_position_std': Array(7.435, dtype=float32), 'eval/episode_y_velocity_std': Array(11.456, dtype=float32), 'eval/avg_episode_length': Array(77.148, dtype=float32), 'eval/epoch_eval_time': 10.372047185897827, 'eval/sps': 12340.86171281914}
time to jit: 0:00:49.264966
time to train: 0:04:35.970175

--------

Iteration0-Robot2

--------

{'eval/walltime': 49.573997497558594, 'training/sps': 35240.23694227054, 'training/walltime': 74.38769507408142, 'training/entropy_loss': Array(-0.013, dtype=float32), 'training/policy_loss': Array(0.001, dtype=float32), 'training/total_loss': Array(0.102, dtype=float32), 'training/v_loss': Array(0.113, dtype=float32), 'eval/episode_distance_from_origin': Array(39.455, dtype=float32), 'eval/episode_forward_reward': Array(8.5, dtype=float32), 'eval/episode_reward': Array(205.967, dtype=float32), 'eval/episode_reward_alive': Array(222.617, dtype=float32), 'eval/episode_reward_linvel': Array(8.5, dtype=float32), 'eval/episode_reward_quadctrl': Array(-25.15, dtype=float32), 'eval/episode_x_position': Array(2.975, dtype=float32), 'eval/episode_x_velocity': Array(6.8, dtype=float32), 'eval/episode_y_position': Array(0.229, dtype=float32), 'eval/episode_y_velocity': Array(0.154, dtype=float32), 'eval/episode_distance_from_origin_std': Array(7.226, dtype=float32), 'eval/episode_forward_reward_std': Array(11.598, dtype=float32), 'eval/episode_reward_std': Array(34.01, dtype=float32), 'eval/episode_reward_alive_std': Array(39.956, dtype=float32), 'eval/episode_reward_linvel_std': Array(11.598, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(4.785, dtype=float32), 'eval/episode_x_position_std': Array(2.797, dtype=float32), 'eval/episode_x_velocity_std': Array(9.278, dtype=float32), 'eval/episode_y_position_std': Array(3.824, dtype=float32), 'eval/episode_y_velocity_std': Array(11.208, dtype=float32), 'eval/avg_episode_length': Array(44.523, dtype=float32), 'eval/epoch_eval_time': 10.388670921325684, 'eval/sps': 12321.114122235196}
{'eval/walltime': 59.96402359008789, 'training/sps': 48930.31073400355, 'training/walltime': 127.96266651153564, 'training/entropy_loss': Array(-0.011, dtype=float32), 'training/policy_loss': Array(-0.014, dtype=float32), 'training/total_loss': Array(0.02, dtype=float32), 'training/v_loss': Array(0.046, dtype=float32), 'eval/episode_distance_from_origin': Array(49.719, dtype=float32), 'eval/episode_forward_reward': Array(14.014, dtype=float32), 'eval/episode_reward': Array(261.38, dtype=float32), 'eval/episode_reward_alive': Array(276.211, dtype=float32), 'eval/episode_reward_linvel': Array(14.014, dtype=float32), 'eval/episode_reward_quadctrl': Array(-28.845, dtype=float32), 'eval/episode_x_position': Array(6.128, dtype=float32), 'eval/episode_x_velocity': Array(11.211, dtype=float32), 'eval/episode_y_position': Array(0.739, dtype=float32), 'eval/episode_y_velocity': Array(0.994, dtype=float32), 'eval/episode_distance_from_origin_std': Array(8.954, dtype=float32), 'eval/episode_forward_reward_std': Array(15.371, dtype=float32), 'eval/episode_reward_std': Array(46.015, dtype=float32), 'eval/episode_reward_alive_std': Array(50.108, dtype=float32), 'eval/episode_reward_linvel_std': Array(15.371, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(5.483, dtype=float32), 'eval/episode_x_position_std': Array(4.079, dtype=float32), 'eval/episode_x_velocity_std': Array(12.297, dtype=float32), 'eval/episode_y_position_std': Array(4.129, dtype=float32), 'eval/episode_y_velocity_std': Array(10.698, dtype=float32), 'eval/avg_episode_length': Array(55.242, dtype=float32), 'eval/epoch_eval_time': 10.390026092529297, 'eval/sps': 12319.507079201214}
{'eval/walltime': 70.34736037254333, 'training/sps': 48735.984237703844, 'training/walltime': 181.75125908851624, 'training/entropy_loss': Array(-0.01, dtype=float32), 'training/policy_loss': Array(-0.012, dtype=float32), 'training/total_loss': Array(0.014, dtype=float32), 'training/v_loss': Array(0.036, dtype=float32), 'eval/episode_distance_from_origin': Array(60.182, dtype=float32), 'eval/episode_forward_reward': Array(18.965, dtype=float32), 'eval/episode_reward': Array(319.771, dtype=float32), 'eval/episode_reward_alive': Array(332.852, dtype=float32), 'eval/episode_reward_linvel': Array(18.965, dtype=float32), 'eval/episode_reward_quadctrl': Array(-32.046, dtype=float32), 'eval/episode_x_position': Array(9.897, dtype=float32), 'eval/episode_x_velocity': Array(15.172, dtype=float32), 'eval/episode_y_position': Array(2.558, dtype=float32), 'eval/episode_y_velocity': Array(3.727, dtype=float32), 'eval/episode_distance_from_origin_std': Array(12.143, dtype=float32), 'eval/episode_forward_reward_std': Array(17.649, dtype=float32), 'eval/episode_reward_std': Array(61.583, dtype=float32), 'eval/episode_reward_alive_std': Array(68.05, dtype=float32), 'eval/episode_reward_linvel_std': Array(17.649, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(7.094, dtype=float32), 'eval/episode_x_position_std': Array(5.691, dtype=float32), 'eval/episode_x_velocity_std': Array(14.12, dtype=float32), 'eval/episode_y_position_std': Array(3.777, dtype=float32), 'eval/episode_y_velocity_std': Array(8.615, dtype=float32), 'eval/avg_episode_length': Array(66.57, dtype=float32), 'eval/epoch_eval_time': 10.383336782455444, 'eval/sps': 12327.443738151644}
{'eval/walltime': 80.69541788101196, 'training/sps': 48694.506029310345, 'training/walltime': 235.58566904067993, 'training/entropy_loss': Array(-0.009, dtype=float32), 'training/policy_loss': Array(-0.011, dtype=float32), 'training/total_loss': Array(0.012, dtype=float32), 'training/v_loss': Array(0.032, dtype=float32), 'eval/episode_distance_from_origin': Array(71.447, dtype=float32), 'eval/episode_forward_reward': Array(26.296, dtype=float32), 'eval/episode_reward': Array(380.379, dtype=float32), 'eval/episode_reward_alive': Array(389.609, dtype=float32), 'eval/episode_reward_linvel': Array(26.296, dtype=float32), 'eval/episode_reward_quadctrl': Array(-35.526, dtype=float32), 'eval/episode_x_position': Array(14.567, dtype=float32), 'eval/episode_x_velocity': Array(21.037, dtype=float32), 'eval/episode_y_position': Array(1.969, dtype=float32), 'eval/episode_y_velocity': Array(1.172, dtype=float32), 'eval/episode_distance_from_origin_std': Array(13.915, dtype=float32), 'eval/episode_forward_reward_std': Array(17.85, dtype=float32), 'eval/episode_reward_std': Array(69.845, dtype=float32), 'eval/episode_reward_alive_std': Array(78.739, dtype=float32), 'eval/episode_reward_linvel_std': Array(17.85, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(7.809, dtype=float32), 'eval/episode_x_position_std': Array(6.79, dtype=float32), 'eval/episode_x_velocity_std': Array(14.28, dtype=float32), 'eval/episode_y_position_std': Array(5.168, dtype=float32), 'eval/episode_y_velocity_std': Array(10.073, dtype=float32), 'eval/avg_episode_length': Array(77.922, dtype=float32), 'eval/epoch_eval_time': 10.348057508468628, 'eval/sps': 12369.471265041535}
time to jit: 0:00:52.638074
time to train: 0:04:37.319378

--------

Iteration0-Robot3

--------

{'eval/walltime': 47.24413776397705, 'training/sps': 35986.19577504274, 'training/walltime': 72.84571051597595, 'training/entropy_loss': Array(-0.013, dtype=float32), 'training/policy_loss': Array(-0.001, dtype=float32), 'training/total_loss': Array(0.098, dtype=float32), 'training/v_loss': Array(0.111, dtype=float32), 'eval/episode_distance_from_origin': Array(39.479, dtype=float32), 'eval/episode_forward_reward': Array(8.349, dtype=float32), 'eval/episode_reward': Array(208.018, dtype=float32), 'eval/episode_reward_alive': Array(224.766, dtype=float32), 'eval/episode_reward_linvel': Array(8.349, dtype=float32), 'eval/episode_reward_quadctrl': Array(-25.096, dtype=float32), 'eval/episode_x_position': Array(3.042, dtype=float32), 'eval/episode_x_velocity': Array(6.679, dtype=float32), 'eval/episode_y_position': Array(0.556, dtype=float32), 'eval/episode_y_velocity': Array(0.986, dtype=float32), 'eval/episode_distance_from_origin_std': Array(9.252, dtype=float32), 'eval/episode_forward_reward_std': Array(11.964, dtype=float32), 'eval/episode_reward_std': Array(44.233, dtype=float32), 'eval/episode_reward_alive_std': Array(51.595, dtype=float32), 'eval/episode_reward_linvel_std': Array(11.964, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(6.003, dtype=float32), 'eval/episode_x_position_std': Array(2.829, dtype=float32), 'eval/episode_x_velocity_std': Array(9.571, dtype=float32), 'eval/episode_y_position_std': Array(3.99, dtype=float32), 'eval/episode_y_velocity_std': Array(11.163, dtype=float32), 'eval/avg_episode_length': Array(44.953, dtype=float32), 'eval/epoch_eval_time': 10.36375617980957, 'eval/sps': 12350.734403552126}
{'eval/walltime': 57.6123685836792, 'training/sps': 48900.57363773449, 'training/walltime': 126.45326161384583, 'training/entropy_loss': Array(-0.012, dtype=float32), 'training/policy_loss': Array(-0.014, dtype=float32), 'training/total_loss': Array(0.025, dtype=float32), 'training/v_loss': Array(0.051, dtype=float32), 'eval/episode_distance_from_origin': Array(49.622, dtype=float32), 'eval/episode_forward_reward': Array(16.012, dtype=float32), 'eval/episode_reward': Array(263.323, dtype=float32), 'eval/episode_reward_alive': Array(275.977, dtype=float32), 'eval/episode_reward_linvel': Array(16.012, dtype=float32), 'eval/episode_reward_quadctrl': Array(-28.666, dtype=float32), 'eval/episode_x_position': Array(6.722, dtype=float32), 'eval/episode_x_velocity': Array(12.81, dtype=float32), 'eval/episode_y_position': Array(0.121, dtype=float32), 'eval/episode_y_velocity': Array(-0.125, dtype=float32), 'eval/episode_distance_from_origin_std': Array(10.109, dtype=float32), 'eval/episode_forward_reward_std': Array(14.576, dtype=float32), 'eval/episode_reward_std': Array(51.434, dtype=float32), 'eval/episode_reward_alive_std': Array(54.993, dtype=float32), 'eval/episode_reward_linvel_std': Array(14.576, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(5.946, dtype=float32), 'eval/episode_x_position_std': Array(4.553, dtype=float32), 'eval/episode_x_velocity_std': Array(11.661, dtype=float32), 'eval/episode_y_position_std': Array(4.086, dtype=float32), 'eval/episode_y_velocity_std': Array(9.532, dtype=float32), 'eval/avg_episode_length': Array(55.195, dtype=float32), 'eval/epoch_eval_time': 10.368230819702148, 'eval/sps': 12345.404170282263}
{'eval/walltime': 67.99302554130554, 'training/sps': 48770.877663803236, 'training/walltime': 180.20337080955505, 'training/entropy_loss': Array(-0.011, dtype=float32), 'training/policy_loss': Array(-0.011, dtype=float32), 'training/total_loss': Array(0.019, dtype=float32), 'training/v_loss': Array(0.041, dtype=float32), 'eval/episode_distance_from_origin': Array(54.992, dtype=float32), 'eval/episode_forward_reward': Array(22.773, dtype=float32), 'eval/episode_reward': Array(296.498, dtype=float32), 'eval/episode_reward_alive': Array(302.539, dtype=float32), 'eval/episode_reward_linvel': Array(22.773, dtype=float32), 'eval/episode_reward_quadctrl': Array(-28.815, dtype=float32), 'eval/episode_x_position': Array(9.627, dtype=float32), 'eval/episode_x_velocity': Array(18.218, dtype=float32), 'eval/episode_y_position': Array(1.163, dtype=float32), 'eval/episode_y_velocity': Array(2.549, dtype=float32), 'eval/episode_distance_from_origin_std': Array(9.65, dtype=float32), 'eval/episode_forward_reward_std': Array(17.364, dtype=float32), 'eval/episode_reward_std': Array(49.319, dtype=float32), 'eval/episode_reward_alive_std': Array(52.589, dtype=float32), 'eval/episode_reward_linvel_std': Array(17.364, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(5.307, dtype=float32), 'eval/episode_x_position_std': Array(4.893, dtype=float32), 'eval/episode_x_velocity_std': Array(13.891, dtype=float32), 'eval/episode_y_position_std': Array(3.035, dtype=float32), 'eval/episode_y_velocity_std': Array(6.899, dtype=float32), 'eval/avg_episode_length': Array(60.508, dtype=float32), 'eval/epoch_eval_time': 10.380656957626343, 'eval/sps': 12330.626136909612}
{'eval/walltime': 78.37524032592773, 'training/sps': 48701.45651276561, 'training/walltime': 234.0300977230072, 'training/entropy_loss': Array(-0.009, dtype=float32), 'training/policy_loss': Array(-0.012, dtype=float32), 'training/total_loss': Array(0.017, dtype=float32), 'training/v_loss': Array(0.039, dtype=float32), 'eval/episode_distance_from_origin': Array(74.699, dtype=float32), 'eval/episode_forward_reward': Array(32.246, dtype=float32), 'eval/episode_reward': Array(394.601, dtype=float32), 'eval/episode_reward_alive': Array(399.219, dtype=float32), 'eval/episode_reward_linvel': Array(32.246, dtype=float32), 'eval/episode_reward_quadctrl': Array(-36.863, dtype=float32), 'eval/episode_x_position': Array(18.791, dtype=float32), 'eval/episode_x_velocity': Array(25.796, dtype=float32), 'eval/episode_y_position': Array(1.165, dtype=float32), 'eval/episode_y_velocity': Array(-0.494, dtype=float32), 'eval/episode_distance_from_origin_std': Array(17.392, dtype=float32), 'eval/episode_forward_reward_std': Array(21.617, dtype=float32), 'eval/episode_reward_std': Array(83.644, dtype=float32), 'eval/episode_reward_alive_std': Array(87.452, dtype=float32), 'eval/episode_reward_linvel_std': Array(21.617, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(8.177, dtype=float32), 'eval/episode_x_position_std': Array(9.831, dtype=float32), 'eval/episode_x_velocity_std': Array(17.293, dtype=float32), 'eval/episode_y_position_std': Array(5.705, dtype=float32), 'eval/episode_y_velocity_std': Array(10.824, dtype=float32), 'eval/avg_episode_length': Array(79.844, dtype=float32), 'eval/epoch_eval_time': 10.382214784622192, 'eval/sps': 12328.775955356803}
time to jit: 0:00:50.112435
time to train: 0:04:35.737865
1

--------

Iteration1-Robot0

--------

{'eval/walltime': 46.31572222709656, 'training/sps': 36261.21650382051, 'training/walltime': 72.29321718215942, 'training/entropy_loss': Array(-0.012, dtype=float32), 'training/policy_loss': Array(0.011, dtype=float32), 'training/total_loss': Array(0.08, dtype=float32), 'training/v_loss': Array(0.081, dtype=float32), 'eval/episode_distance_from_origin': Array(44.49, dtype=float32), 'eval/episode_forward_reward': Array(15.589, dtype=float32), 'eval/episode_reward': Array(242.001, dtype=float32), 'eval/episode_reward_alive': Array(254.414, dtype=float32), 'eval/episode_reward_linvel': Array(15.589, dtype=float32), 'eval/episode_reward_quadctrl': Array(-28.001, dtype=float32), 'eval/episode_x_position': Array(6.283, dtype=float32), 'eval/episode_x_velocity': Array(12.471, dtype=float32), 'eval/episode_y_position': Array(-0.05, dtype=float32), 'eval/episode_y_velocity': Array(0.351, dtype=float32), 'eval/episode_distance_from_origin_std': Array(10.659, dtype=float32), 'eval/episode_forward_reward_std': Array(13.701, dtype=float32), 'eval/episode_reward_std': Array(55.704, dtype=float32), 'eval/episode_reward_alive_std': Array(61.163, dtype=float32), 'eval/episode_reward_linvel_std': Array(13.701, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(6.919, dtype=float32), 'eval/episode_x_position_std': Array(3.954, dtype=float32), 'eval/episode_x_velocity_std': Array(10.961, dtype=float32), 'eval/episode_y_position_std': Array(3.465, dtype=float32), 'eval/episode_y_velocity_std': Array(9.301, dtype=float32), 'eval/avg_episode_length': Array(50.883, dtype=float32), 'eval/epoch_eval_time': 10.320116758346558, 'eval/sps': 12402.96045066331}
{'eval/walltime': 56.690534591674805, 'training/sps': 49048.491095047306, 'training/walltime': 125.73910188674927, 'training/entropy_loss': Array(-0.011, dtype=float32), 'training/policy_loss': Array(-0.016, dtype=float32), 'training/total_loss': Array(0.034, dtype=float32), 'training/v_loss': Array(0.06, dtype=float32), 'eval/episode_distance_from_origin': Array(62.716, dtype=float32), 'eval/episode_forward_reward': Array(32.476, dtype=float32), 'eval/episode_reward': Array(336.593, dtype=float32), 'eval/episode_reward_alive': Array(340.469, dtype=float32), 'eval/episode_reward_linvel': Array(32.476, dtype=float32), 'eval/episode_reward_quadctrl': Array(-36.352, dtype=float32), 'eval/episode_x_position': Array(17.608, dtype=float32), 'eval/episode_x_velocity': Array(25.981, dtype=float32), 'eval/episode_y_position': Array(0.861, dtype=float32), 'eval/episode_y_velocity': Array(3.526, dtype=float32), 'eval/episode_distance_from_origin_std': Array(16.502, dtype=float32), 'eval/episode_forward_reward_std': Array(21.939, dtype=float32), 'eval/episode_reward_std': Array(77.457, dtype=float32), 'eval/episode_reward_alive_std': Array(79.874, dtype=float32), 'eval/episode_reward_linvel_std': Array(21.939, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(8.362, dtype=float32), 'eval/episode_x_position_std': Array(10.653, dtype=float32), 'eval/episode_x_velocity_std': Array(17.551, dtype=float32), 'eval/episode_y_position_std': Array(5.627, dtype=float32), 'eval/episode_y_velocity_std': Array(10.239, dtype=float32), 'eval/avg_episode_length': Array(68.094, dtype=float32), 'eval/epoch_eval_time': 10.374812364578247, 'eval/sps': 12337.572526807178}
{'eval/walltime': 67.04269933700562, 'training/sps': 48889.448580740536, 'training/walltime': 179.35885167121887, 'training/entropy_loss': Array(-0.009, dtype=float32), 'training/policy_loss': Array(-0.014, dtype=float32), 'training/total_loss': Array(0.063, dtype=float32), 'training/v_loss': Array(0.086, dtype=float32), 'eval/episode_distance_from_origin': Array(117.159, dtype=float32), 'eval/episode_forward_reward': Array(80.218, dtype=float32), 'eval/episode_reward': Array(521.024, dtype=float32), 'eval/episode_reward_alive': Array(493.398, dtype=float32), 'eval/episode_reward_linvel': Array(80.218, dtype=float32), 'eval/episode_reward_quadctrl': Array(-52.593, dtype=float32), 'eval/episode_x_position': Array(66.195, dtype=float32), 'eval/episode_x_velocity': Array(64.174, dtype=float32), 'eval/episode_y_position': Array(6.415, dtype=float32), 'eval/episode_y_velocity': Array(7.675, dtype=float32), 'eval/episode_distance_from_origin_std': Array(53.028, dtype=float32), 'eval/episode_forward_reward_std': Array(34.28, dtype=float32), 'eval/episode_reward_std': Array(143.681, dtype=float32), 'eval/episode_reward_alive_std': Array(131.454, dtype=float32), 'eval/episode_reward_linvel_std': Array(34.28, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(13.936, dtype=float32), 'eval/episode_x_position_std': Array(47.584, dtype=float32), 'eval/episode_x_velocity_std': Array(27.424, dtype=float32), 'eval/episode_y_position_std': Array(14.449, dtype=float32), 'eval/episode_y_velocity_std': Array(17.084, dtype=float32), 'eval/avg_episode_length': Array(98.68, dtype=float32), 'eval/epoch_eval_time': 10.35216474533081, 'eval/sps': 12364.563658797306}
{'eval/walltime': 77.41463661193848, 'training/sps': 48781.98777474644, 'training/walltime': 233.09671926498413, 'training/entropy_loss': Array(-0.008, dtype=float32), 'training/policy_loss': Array(-0.012, dtype=float32), 'training/total_loss': Array(0.093, dtype=float32), 'training/v_loss': Array(0.113, dtype=float32), 'eval/episode_distance_from_origin': Array(354.844, dtype=float32), 'eval/episode_forward_reward': Array(171.69, dtype=float32), 'eval/episode_reward': Array(910.536, dtype=float32), 'eval/episode_reward_alive': Array(827.695, dtype=float32), 'eval/episode_reward_linvel': Array(171.69, dtype=float32), 'eval/episode_reward_quadctrl': Array(-88.849, dtype=float32), 'eval/episode_x_position': Array(296.058, dtype=float32), 'eval/episode_x_velocity': Array(137.352, dtype=float32), 'eval/episode_y_position': Array(25.059, dtype=float32), 'eval/episode_y_velocity': Array(15.118, dtype=float32), 'eval/episode_distance_from_origin_std': Array(323.262, dtype=float32), 'eval/episode_forward_reward_std': Array(89.202, dtype=float32), 'eval/episode_reward_std': Array(380.634, dtype=float32), 'eval/episode_reward_alive_std': Array(332.908, dtype=float32), 'eval/episode_reward_linvel_std': Array(89.202, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(36.798, dtype=float32), 'eval/episode_x_position_std': Array(316.235, dtype=float32), 'eval/episode_x_velocity_std': Array(71.362, dtype=float32), 'eval/episode_y_position_std': Array(43.833, dtype=float32), 'eval/episode_y_velocity_std': Array(27.491, dtype=float32), 'eval/avg_episode_length': Array(165.539, dtype=float32), 'eval/epoch_eval_time': 10.371937274932861, 'eval/sps': 12340.992488390126}
time to jit: 0:00:48.372949
time to train: 0:04:34.724227

--------

Iteration1-Robot1

--------

{'eval/walltime': 46.70167088508606, 'training/sps': 35556.177058899164, 'training/walltime': 73.72671127319336, 'training/entropy_loss': Array(-0.012, dtype=float32), 'training/policy_loss': Array(0.009, dtype=float32), 'training/total_loss': Array(0.098, dtype=float32), 'training/v_loss': Array(0.1, dtype=float32), 'eval/episode_distance_from_origin': Array(44.839, dtype=float32), 'eval/episode_forward_reward': Array(16.486, dtype=float32), 'eval/episode_reward': Array(240.86, dtype=float32), 'eval/episode_reward_alive': Array(253.555, dtype=float32), 'eval/episode_reward_linvel': Array(16.486, dtype=float32), 'eval/episode_reward_quadctrl': Array(-29.18, dtype=float32), 'eval/episode_x_position': Array(6.428, dtype=float32), 'eval/episode_x_velocity': Array(13.189, dtype=float32), 'eval/episode_y_position': Array(-0.915, dtype=float32), 'eval/episode_y_velocity': Array(-2.529, dtype=float32), 'eval/episode_distance_from_origin_std': Array(9.507, dtype=float32), 'eval/episode_forward_reward_std': Array(11.659, dtype=float32), 'eval/episode_reward_std': Array(47.416, dtype=float32), 'eval/episode_reward_alive_std': Array(52.534, dtype=float32), 'eval/episode_reward_linvel_std': Array(11.659, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(6.325, dtype=float32), 'eval/episode_x_position_std': Array(3.57, dtype=float32), 'eval/episode_x_velocity_std': Array(9.327, dtype=float32), 'eval/episode_y_position_std': Array(4.2, dtype=float32), 'eval/episode_y_velocity_std': Array(10.088, dtype=float32), 'eval/avg_episode_length': Array(50.711, dtype=float32), 'eval/epoch_eval_time': 10.32329249382019, 'eval/sps': 12399.14495076298}
{'eval/walltime': 57.036762952804565, 'training/sps': 49011.01160702653, 'training/walltime': 127.21346688270569, 'training/entropy_loss': Array(-0.011, dtype=float32), 'training/policy_loss': Array(-0.015, dtype=float32), 'training/total_loss': Array(0.028, dtype=float32), 'training/v_loss': Array(0.054, dtype=float32), 'eval/episode_distance_from_origin': Array(58.952, dtype=float32), 'eval/episode_forward_reward': Array(26.585, dtype=float32), 'eval/episode_reward': Array(316.117, dtype=float32), 'eval/episode_reward_alive': Array(325.078, dtype=float32), 'eval/episode_reward_linvel': Array(26.585, dtype=float32), 'eval/episode_reward_quadctrl': Array(-35.546, dtype=float32), 'eval/episode_x_position': Array(12.618, dtype=float32), 'eval/episode_x_velocity': Array(21.268, dtype=float32), 'eval/episode_y_position': Array(-0.026, dtype=float32), 'eval/episode_y_velocity': Array(1.974, dtype=float32), 'eval/episode_distance_from_origin_std': Array(13.085, dtype=float32), 'eval/episode_forward_reward_std': Array(16.17, dtype=float32), 'eval/episode_reward_std': Array(65.598, dtype=float32), 'eval/episode_reward_alive_std': Array(69.397, dtype=float32), 'eval/episode_reward_linvel_std': Array(16.17, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(7.827, dtype=float32), 'eval/episode_x_position_std': Array(6.662, dtype=float32), 'eval/episode_x_velocity_std': Array(12.936, dtype=float32), 'eval/episode_y_position_std': Array(5.164, dtype=float32), 'eval/episode_y_velocity_std': Array(10.698, dtype=float32), 'eval/avg_episode_length': Array(65.016, dtype=float32), 'eval/epoch_eval_time': 10.335092067718506, 'eval/sps': 12384.988847830968}
{'eval/walltime': 67.37713050842285, 'training/sps': 48860.31065683565, 'training/walltime': 180.86519289016724, 'training/entropy_loss': Array(-0.01, dtype=float32), 'training/policy_loss': Array(-0.015, dtype=float32), 'training/total_loss': Array(0.041, dtype=float32), 'training/v_loss': Array(0.065, dtype=float32), 'eval/episode_distance_from_origin': Array(103.307, dtype=float32), 'eval/episode_forward_reward': Array(57.236, dtype=float32), 'eval/episode_reward': Array(493.405, dtype=float32), 'eval/episode_reward_alive': Array(488.906, dtype=float32), 'eval/episode_reward_linvel': Array(57.236, dtype=float32), 'eval/episode_reward_quadctrl': Array(-52.737, dtype=float32), 'eval/episode_x_position': Array(47.45, dtype=float32), 'eval/episode_x_velocity': Array(45.789, dtype=float32), 'eval/episode_y_position': Array(0.67, dtype=float32), 'eval/episode_y_velocity': Array(3.715, dtype=float32), 'eval/episode_distance_from_origin_std': Array(41.371, dtype=float32), 'eval/episode_forward_reward_std': Array(28.339, dtype=float32), 'eval/episode_reward_std': Array(137.411, dtype=float32), 'eval/episode_reward_alive_std': Array(133.104, dtype=float32), 'eval/episode_reward_linvel_std': Array(28.339, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(14.497, dtype=float32), 'eval/episode_x_position_std': Array(35.185, dtype=float32), 'eval/episode_x_velocity_std': Array(22.671, dtype=float32), 'eval/episode_y_position_std': Array(9.787, dtype=float32), 'eval/episode_y_velocity_std': Array(15.971, dtype=float32), 'eval/avg_episode_length': Array(97.781, dtype=float32), 'eval/epoch_eval_time': 10.340367555618286, 'eval/sps': 12378.670227293138}
{'eval/walltime': 77.71436858177185, 'training/sps': 48786.594290992325, 'training/walltime': 234.59798645973206, 'training/entropy_loss': Array(-0.008, dtype=float32), 'training/policy_loss': Array(-0.013, dtype=float32), 'training/total_loss': Array(0.068, dtype=float32), 'training/v_loss': Array(0.089, dtype=float32), 'eval/episode_distance_from_origin': Array(277.088, dtype=float32), 'eval/episode_forward_reward': Array(131.368, dtype=float32), 'eval/episode_reward': Array(864.049, dtype=float32), 'eval/episode_reward_alive': Array(820.664, dtype=float32), 'eval/episode_reward_linvel': Array(131.368, dtype=float32), 'eval/episode_reward_quadctrl': Array(-87.983, dtype=float32), 'eval/episode_x_position': Array(211.667, dtype=float32), 'eval/episode_x_velocity': Array(105.095, dtype=float32), 'eval/episode_y_position': Array(16.97, dtype=float32), 'eval/episode_y_velocity': Array(14.622, dtype=float32), 'eval/episode_distance_from_origin_std': Array(201.719, dtype=float32), 'eval/episode_forward_reward_std': Array(57.314, dtype=float32), 'eval/episode_reward_std': Array(332.842, dtype=float32), 'eval/episode_reward_alive_std': Array(316.735, dtype=float32), 'eval/episode_reward_linvel_std': Array(57.314, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(33.508, dtype=float32), 'eval/episode_x_position_std': Array(190.37, dtype=float32), 'eval/episode_x_velocity_std': Array(45.851, dtype=float32), 'eval/episode_y_position_std': Array(43.133, dtype=float32), 'eval/episode_y_velocity_std': Array(22.418, dtype=float32), 'eval/avg_episode_length': Array(164.133, dtype=float32), 'eval/epoch_eval_time': 10.337238073348999, 'eval/sps': 12382.417730128886}
time to jit: 0:00:50.107675
time to train: 0:04:36.146731

--------

Iteration1-Robot2

--------

{'eval/walltime': 46.05417275428772, 'training/sps': 35857.51436264968, 'training/walltime': 73.1071310043335, 'training/entropy_loss': Array(-0.012, dtype=float32), 'training/policy_loss': Array(0.005, dtype=float32), 'training/total_loss': Array(0.1, dtype=float32), 'training/v_loss': Array(0.106, dtype=float32), 'eval/episode_distance_from_origin': Array(32.503, dtype=float32), 'eval/episode_forward_reward': Array(14.861, dtype=float32), 'eval/episode_reward': Array(180.144, dtype=float32), 'eval/episode_reward_alive': Array(186.133, dtype=float32), 'eval/episode_reward_linvel': Array(14.861, dtype=float32), 'eval/episode_reward_quadctrl': Array(-20.85, dtype=float32), 'eval/episode_x_position': Array(4.488, dtype=float32), 'eval/episode_x_velocity': Array(11.889, dtype=float32), 'eval/episode_y_position': Array(-0.304, dtype=float32), 'eval/episode_y_velocity': Array(-3.915, dtype=float32), 'eval/episode_distance_from_origin_std': Array(6.994, dtype=float32), 'eval/episode_forward_reward_std': Array(11.736, dtype=float32), 'eval/episode_reward_std': Array(37.715, dtype=float32), 'eval/episode_reward_alive_std': Array(37.459, dtype=float32), 'eval/episode_reward_linvel_std': Array(11.736, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(4.301, dtype=float32), 'eval/episode_x_position_std': Array(3.555, dtype=float32), 'eval/episode_x_velocity_std': Array(9.389, dtype=float32), 'eval/episode_y_position_std': Array(2.42, dtype=float32), 'eval/episode_y_velocity_std': Array(8.365, dtype=float32), 'eval/avg_episode_length': Array(37.227, dtype=float32), 'eval/epoch_eval_time': 10.289808750152588, 'eval/sps': 12439.492619151146}
{'eval/walltime': 56.395347118377686, 'training/sps': 48942.93384933671, 'training/walltime': 126.66828465461731, 'training/entropy_loss': Array(-0.01, dtype=float32), 'training/policy_loss': Array(-0.017, dtype=float32), 'training/total_loss': Array(0.059, dtype=float32), 'training/v_loss': Array(0.087, dtype=float32), 'eval/episode_distance_from_origin': Array(53.207, dtype=float32), 'eval/episode_forward_reward': Array(27.917, dtype=float32), 'eval/episode_reward': Array(283.5, dtype=float32), 'eval/episode_reward_alive': Array(286.562, dtype=float32), 'eval/episode_reward_linvel': Array(27.917, dtype=float32), 'eval/episode_reward_quadctrl': Array(-30.979, dtype=float32), 'eval/episode_x_position': Array(16.633, dtype=float32), 'eval/episode_x_velocity': Array(22.334, dtype=float32), 'eval/episode_y_position': Array(2.188, dtype=float32), 'eval/episode_y_velocity': Array(-0.436, dtype=float32), 'eval/episode_distance_from_origin_std': Array(12.546, dtype=float32), 'eval/episode_forward_reward_std': Array(9.796, dtype=float32), 'eval/episode_reward_std': Array(55.602, dtype=float32), 'eval/episode_reward_alive_std': Array(60.311, dtype=float32), 'eval/episode_reward_linvel_std': Array(9.796, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(6.526, dtype=float32), 'eval/episode_x_position_std': Array(7.639, dtype=float32), 'eval/episode_x_velocity_std': Array(7.837, dtype=float32), 'eval/episode_y_position_std': Array(5.959, dtype=float32), 'eval/episode_y_velocity_std': Array(13.156, dtype=float32), 'eval/avg_episode_length': Array(57.312, dtype=float32), 'eval/epoch_eval_time': 10.341174364089966, 'eval/sps': 12377.70445535507}
{'eval/walltime': 66.720210313797, 'training/sps': 48864.47766770106, 'training/walltime': 180.3154354095459, 'training/entropy_loss': Array(-0.01, dtype=float32), 'training/policy_loss': Array(-0.012, dtype=float32), 'training/total_loss': Array(0.038, dtype=float32), 'training/v_loss': Array(0.06, dtype=float32), 'eval/episode_distance_from_origin': Array(74.679, dtype=float32), 'eval/episode_forward_reward': Array(41.5, dtype=float32), 'eval/episode_reward': Array(376.01, dtype=float32), 'eval/episode_reward_alive': Array(372.578, dtype=float32), 'eval/episode_reward_linvel': Array(41.5, dtype=float32), 'eval/episode_reward_quadctrl': Array(-38.067, dtype=float32), 'eval/episode_x_position': Array(32.77, dtype=float32), 'eval/episode_x_velocity': Array(33.2, dtype=float32), 'eval/episode_y_position': Array(3.186, dtype=float32), 'eval/episode_y_velocity': Array(1.026, dtype=float32), 'eval/episode_distance_from_origin_std': Array(14.78, dtype=float32), 'eval/episode_forward_reward_std': Array(11.985, dtype=float32), 'eval/episode_reward_std': Array(61.034, dtype=float32), 'eval/episode_reward_alive_std': Array(62.728, dtype=float32), 'eval/episode_reward_linvel_std': Array(11.985, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(5.823, dtype=float32), 'eval/episode_x_position_std': Array(10.858, dtype=float32), 'eval/episode_x_velocity_std': Array(9.588, dtype=float32), 'eval/episode_y_position_std': Array(7.486, dtype=float32), 'eval/episode_y_velocity_std': Array(15.386, dtype=float32), 'eval/avg_episode_length': Array(74.516, dtype=float32), 'eval/epoch_eval_time': 10.324863195419312, 'eval/sps': 12397.258692666066}
time to jit: 0:00:49.111082
time to train: 0:03:31.445376

--------

Iteration1-Robot3

--------

{'eval/walltime': 46.4205219745636, 'training/sps': 35670.42182239799, 'training/walltime': 73.49058032035828, 'training/entropy_loss': Array(-0.012, dtype=float32), 'training/policy_loss': Array(-0.001, dtype=float32), 'training/total_loss': Array(0.087, dtype=float32), 'training/v_loss': Array(0.1, dtype=float32), 'eval/episode_distance_from_origin': Array(44.526, dtype=float32), 'eval/episode_forward_reward': Array(20.75, dtype=float32), 'eval/episode_reward': Array(235.414, dtype=float32), 'eval/episode_reward_alive': Array(243.008, dtype=float32), 'eval/episode_reward_linvel': Array(20.75, dtype=float32), 'eval/episode_reward_quadctrl': Array(-28.344, dtype=float32), 'eval/episode_x_position': Array(7.161, dtype=float32), 'eval/episode_x_velocity': Array(16.6, dtype=float32), 'eval/episode_y_position': Array(-1.207, dtype=float32), 'eval/episode_y_velocity': Array(-3.194, dtype=float32), 'eval/episode_distance_from_origin_std': Array(8.087, dtype=float32), 'eval/episode_forward_reward_std': Array(12.871, dtype=float32), 'eval/episode_reward_std': Array(38.625, dtype=float32), 'eval/episode_reward_alive_std': Array(41., dtype=float32), 'eval/episode_reward_linvel_std': Array(12.871, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(5.007, dtype=float32), 'eval/episode_x_position_std': Array(3.999, dtype=float32), 'eval/episode_x_velocity_std': Array(10.297, dtype=float32), 'eval/episode_y_position_std': Array(4.237, dtype=float32), 'eval/episode_y_velocity_std': Array(11.58, dtype=float32), 'eval/avg_episode_length': Array(48.602, dtype=float32), 'eval/epoch_eval_time': 10.329153060913086, 'eval/sps': 12392.109909220857}
{'eval/walltime': 56.77227711677551, 'training/sps': 49014.19860457361, 'training/walltime': 126.97385811805725, 'training/entropy_loss': Array(-0.011, dtype=float32), 'training/policy_loss': Array(-0.016, dtype=float32), 'training/total_loss': Array(0.018, dtype=float32), 'training/v_loss': Array(0.045, dtype=float32), 'eval/episode_distance_from_origin': Array(64.365, dtype=float32), 'eval/episode_forward_reward': Array(35.783, dtype=float32), 'eval/episode_reward': Array(332.311, dtype=float32), 'eval/episode_reward_alive': Array(333.711, dtype=float32), 'eval/episode_reward_linvel': Array(35.783, dtype=float32), 'eval/episode_reward_quadctrl': Array(-37.182, dtype=float32), 'eval/episode_x_position': Array(17.976, dtype=float32), 'eval/episode_x_velocity': Array(28.626, dtype=float32), 'eval/episode_y_position': Array(-2.568, dtype=float32), 'eval/episode_y_velocity': Array(-3.777, dtype=float32), 'eval/episode_distance_from_origin_std': Array(13.233, dtype=float32), 'eval/episode_forward_reward_std': Array(21.064, dtype=float32), 'eval/episode_reward_std': Array(60.377, dtype=float32), 'eval/episode_reward_alive_std': Array(59.57, dtype=float32), 'eval/episode_reward_linvel_std': Array(21.064, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(6.585, dtype=float32), 'eval/episode_x_position_std': Array(9.708, dtype=float32), 'eval/episode_x_velocity_std': Array(16.851, dtype=float32), 'eval/episode_y_position_std': Array(5.933, dtype=float32), 'eval/episode_y_velocity_std': Array(11.607, dtype=float32), 'eval/avg_episode_length': Array(66.742, dtype=float32), 'eval/epoch_eval_time': 10.351755142211914, 'eval/sps': 12365.052905670793}
{'eval/walltime': 67.1229259967804, 'training/sps': 49571.175891734885, 'training/walltime': 179.85620260238647, 'training/entropy_loss': Array(nan, dtype=float32), 'training/policy_loss': Array(nan, dtype=float32), 'training/total_loss': Array(nan, dtype=float32), 'training/v_loss': Array(nan, dtype=float32), 'eval/episode_distance_from_origin': Array(nan, dtype=float32), 'eval/episode_forward_reward': Array(nan, dtype=float32), 'eval/episode_reward': Array(nan, dtype=float32), 'eval/episode_reward_alive': Array(5000., dtype=float32), 'eval/episode_reward_linvel': Array(nan, dtype=float32), 'eval/episode_reward_quadctrl': Array(nan, dtype=float32), 'eval/episode_x_position': Array(nan, dtype=float32), 'eval/episode_x_velocity': Array(nan, dtype=float32), 'eval/episode_y_position': Array(nan, dtype=float32), 'eval/episode_y_velocity': Array(nan, dtype=float32), 'eval/episode_distance_from_origin_std': Array(nan, dtype=float32), 'eval/episode_forward_reward_std': Array(nan, dtype=float32), 'eval/episode_reward_std': Array(nan, dtype=float32), 'eval/episode_reward_alive_std': Array(0., dtype=float32), 'eval/episode_reward_linvel_std': Array(nan, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(nan, dtype=float32), 'eval/episode_x_position_std': Array(nan, dtype=float32), 'eval/episode_x_velocity_std': Array(nan, dtype=float32), 'eval/episode_y_position_std': Array(nan, dtype=float32), 'eval/episode_y_velocity_std': Array(nan, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/epoch_eval_time': 10.350648880004883, 'eval/sps': 12366.374464432574}
{'eval/walltime': 77.50126361846924, 'training/sps': 49789.58869620288, 'training/walltime': 232.50656723976135, 'training/entropy_loss': Array(nan, dtype=float32), 'training/policy_loss': Array(nan, dtype=float32), 'training/total_loss': Array(nan, dtype=float32), 'training/v_loss': Array(nan, dtype=float32), 'eval/episode_distance_from_origin': Array(nan, dtype=float32), 'eval/episode_forward_reward': Array(nan, dtype=float32), 'eval/episode_reward': Array(nan, dtype=float32), 'eval/episode_reward_alive': Array(5000., dtype=float32), 'eval/episode_reward_linvel': Array(nan, dtype=float32), 'eval/episode_reward_quadctrl': Array(nan, dtype=float32), 'eval/episode_x_position': Array(nan, dtype=float32), 'eval/episode_x_velocity': Array(nan, dtype=float32), 'eval/episode_y_position': Array(nan, dtype=float32), 'eval/episode_y_velocity': Array(nan, dtype=float32), 'eval/episode_distance_from_origin_std': Array(nan, dtype=float32), 'eval/episode_forward_reward_std': Array(nan, dtype=float32), 'eval/episode_reward_std': Array(nan, dtype=float32), 'eval/episode_reward_alive_std': Array(0., dtype=float32), 'eval/episode_reward_linvel_std': Array(nan, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(nan, dtype=float32), 'eval/episode_x_position_std': Array(nan, dtype=float32), 'eval/episode_x_velocity_std': Array(nan, dtype=float32), 'eval/episode_y_position_std': Array(nan, dtype=float32), 'eval/episode_y_velocity_std': Array(nan, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/epoch_eval_time': 10.378337621688843, 'eval/sps': 12333.381767471432}
2

--------

Iteration2-Robot0

--------

{'eval/walltime': 47.244444608688354, 'training/sps': 36189.5479587178, 'training/walltime': 72.4363842010498, 'training/entropy_loss': Array(-0.006, dtype=float32), 'training/policy_loss': Array(-0.014, dtype=float32), 'training/total_loss': Array(0.083, dtype=float32), 'training/v_loss': Array(0.103, dtype=float32), 'eval/episode_distance_from_origin': Array(104.556, dtype=float32), 'eval/episode_forward_reward': Array(63.005, dtype=float32), 'eval/episode_reward': Array(477.867, dtype=float32), 'eval/episode_reward_alive': Array(470.898, dtype=float32), 'eval/episode_reward_linvel': Array(63.005, dtype=float32), 'eval/episode_reward_quadctrl': Array(-56.036, dtype=float32), 'eval/episode_x_position': Array(53.725, dtype=float32), 'eval/episode_x_velocity': Array(50.404, dtype=float32), 'eval/episode_y_position': Array(12.867, dtype=float32), 'eval/episode_y_velocity': Array(14.203, dtype=float32), 'eval/episode_distance_from_origin_std': Array(57.395, dtype=float32), 'eval/episode_forward_reward_std': Array(36.254, dtype=float32), 'eval/episode_reward_std': Array(161.767, dtype=float32), 'eval/episode_reward_alive_std': Array(152.301, dtype=float32), 'eval/episode_reward_linvel_std': Array(36.254, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(18.57, dtype=float32), 'eval/episode_x_position_std': Array(49.223, dtype=float32), 'eval/episode_x_velocity_std': Array(29.003, dtype=float32), 'eval/episode_y_position_std': Array(21.205, dtype=float32), 'eval/episode_y_velocity_std': Array(18.783, dtype=float32), 'eval/avg_episode_length': Array(94.18, dtype=float32), 'eval/epoch_eval_time': 10.340132236480713, 'eval/sps': 12378.951939164474}
{'eval/walltime': 57.592562437057495, 'training/sps': 49180.59367518937, 'training/walltime': 125.73870944976807, 'training/entropy_loss': Array(-0.005, dtype=float32), 'training/policy_loss': Array(-0.012, dtype=float32), 'training/total_loss': Array(0.107, dtype=float32), 'training/v_loss': Array(0.123, dtype=float32), 'eval/episode_distance_from_origin': Array(534.09, dtype=float32), 'eval/episode_forward_reward': Array(201.795, dtype=float32), 'eval/episode_reward': Array(1070.843, dtype=float32), 'eval/episode_reward_alive': Array(986.562, dtype=float32), 'eval/episode_reward_linvel': Array(201.795, dtype=float32), 'eval/episode_reward_quadctrl': Array(-117.515, dtype=float32), 'eval/episode_x_position': Array(462.338, dtype=float32), 'eval/episode_x_velocity': Array(161.436, dtype=float32), 'eval/episode_y_position': Array(91.614, dtype=float32), 'eval/episode_y_velocity': Array(29.825, dtype=float32), 'eval/episode_distance_from_origin_std': Array(660.558, dtype=float32), 'eval/episode_forward_reward_std': Array(123.576, dtype=float32), 'eval/episode_reward_std': Array(560.867, dtype=float32), 'eval/episode_reward_alive_std': Array(502.87, dtype=float32), 'eval/episode_reward_linvel_std': Array(123.576, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(61.661, dtype=float32), 'eval/episode_x_position_std': Array(631.794, dtype=float32), 'eval/episode_x_velocity_std': Array(98.861, dtype=float32), 'eval/episode_y_position_std': Array(185.428, dtype=float32), 'eval/episode_y_velocity_std': Array(40.199, dtype=float32), 'eval/avg_episode_length': Array(197.312, dtype=float32), 'eval/epoch_eval_time': 10.34811782836914, 'eval/sps': 12369.399162530868}
{'eval/walltime': 67.95595097541809, 'training/sps': 48962.87584843094, 'training/walltime': 179.27804827690125, 'training/entropy_loss': Array(-0.004, dtype=float32), 'training/policy_loss': Array(-0.009, dtype=float32), 'training/total_loss': Array(0.091, dtype=float32), 'training/v_loss': Array(0.104, dtype=float32), 'eval/episode_distance_from_origin': Array(3338.932, dtype=float32), 'eval/episode_forward_reward': Array(542.122, dtype=float32), 'eval/episode_reward': Array(2367.441, dtype=float32), 'eval/episode_reward_alive': Array(2067.734, dtype=float32), 'eval/episode_reward_linvel': Array(542.122, dtype=float32), 'eval/episode_reward_quadctrl': Array(-242.415, dtype=float32), 'eval/episode_x_position': Array(3236.501, dtype=float32), 'eval/episode_x_velocity': Array(433.698, dtype=float32), 'eval/episode_y_position': Array(430.157, dtype=float32), 'eval/episode_y_velocity': Array(55.377, dtype=float32), 'eval/episode_distance_from_origin_std': Array(4325.736, dtype=float32), 'eval/episode_forward_reward_std': Array(402.994, dtype=float32), 'eval/episode_reward_std': Array(1661.714, dtype=float32), 'eval/episode_reward_alive_std': Array(1429.862, dtype=float32), 'eval/episode_reward_linvel_std': Array(402.994, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(169.476, dtype=float32), 'eval/episode_x_position_std': Array(4262.248, dtype=float32), 'eval/episode_x_velocity_std': Array(322.396, dtype=float32), 'eval/episode_y_position_std': Array(731.318, dtype=float32), 'eval/episode_y_velocity_std': Array(63.278, dtype=float32), 'eval/avg_episode_length': Array(413.547, dtype=float32), 'eval/epoch_eval_time': 10.363388538360596, 'eval/sps': 12351.172546141801}
{'eval/walltime': 78.33359813690186, 'training/sps': 48880.309883750204, 'training/walltime': 232.90782284736633, 'training/entropy_loss': Array(-0.002, dtype=float32), 'training/policy_loss': Array(-0.009, dtype=float32), 'training/total_loss': Array(0.058, dtype=float32), 'training/v_loss': Array(0.069, dtype=float32), 'eval/episode_distance_from_origin': Array(8654.646, dtype=float32), 'eval/episode_forward_reward': Array(1078.362, dtype=float32), 'eval/episode_reward': Array(3987.729, dtype=float32), 'eval/episode_reward_alive': Array(3295.664, dtype=float32), 'eval/episode_reward_linvel': Array(1078.362, dtype=float32), 'eval/episode_reward_quadctrl': Array(-386.297, dtype=float32), 'eval/episode_x_position': Array(8482.569, dtype=float32), 'eval/episode_x_velocity': Array(862.692, dtype=float32), 'eval/episode_y_position': Array(1183.932, dtype=float32), 'eval/episode_y_velocity': Array(121.204, dtype=float32), 'eval/episode_distance_from_origin_std': Array(6493.146, dtype=float32), 'eval/episode_forward_reward_std': Array(532.512, dtype=float32), 'eval/episode_reward_std': Array(1916.456, dtype=float32), 'eval/episode_reward_alive_std': Array(1570.013, dtype=float32), 'eval/episode_reward_linvel_std': Array(532.512, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(184.755, dtype=float32), 'eval/episode_x_position_std': Array(6408.091, dtype=float32), 'eval/episode_x_velocity_std': Array(426.011, dtype=float32), 'eval/episode_y_position_std': Array(1165.554, dtype=float32), 'eval/episode_y_velocity_std': Array(91.196, dtype=float32), 'eval/avg_episode_length': Array(659.133, dtype=float32), 'eval/epoch_eval_time': 10.377647161483765, 'eval/sps': 12334.20234935979}
time to jit: 0:00:50.171976
time to train: 0:04:34.548080

--------

Iteration2-Robot1

--------

{'eval/walltime': 47.555933713912964, 'training/sps': 35833.288636740355, 'training/walltime': 73.15655636787415, 'training/entropy_loss': Array(-0.007, dtype=float32), 'training/policy_loss': Array(-0.015, dtype=float32), 'training/total_loss': Array(0.203, dtype=float32), 'training/v_loss': Array(0.225, dtype=float32), 'eval/episode_distance_from_origin': Array(114.806, dtype=float32), 'eval/episode_forward_reward': Array(103.381, dtype=float32), 'eval/episode_reward': Array(468.455, dtype=float32), 'eval/episode_reward_alive': Array(408.438, dtype=float32), 'eval/episode_reward_linvel': Array(103.381, dtype=float32), 'eval/episode_reward_quadctrl': Array(-43.364, dtype=float32), 'eval/episode_x_position': Array(73.819, dtype=float32), 'eval/episode_x_velocity': Array(82.705, dtype=float32), 'eval/episode_y_position': Array(26.017, dtype=float32), 'eval/episode_y_velocity': Array(30.717, dtype=float32), 'eval/episode_distance_from_origin_std': Array(64.178, dtype=float32), 'eval/episode_forward_reward_std': Array(41.97, dtype=float32), 'eval/episode_reward_std': Array(149.918, dtype=float32), 'eval/episode_reward_alive_std': Array(125.043, dtype=float32), 'eval/episode_reward_linvel_std': Array(41.97, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(13.794, dtype=float32), 'eval/episode_x_position_std': Array(56.455, dtype=float32), 'eval/episode_x_velocity_std': Array(33.576, dtype=float32), 'eval/episode_y_position_std': Array(24.415, dtype=float32), 'eval/episode_y_velocity_std': Array(22.987, dtype=float32), 'eval/avg_episode_length': Array(81.688, dtype=float32), 'eval/epoch_eval_time': 10.362766027450562, 'eval/sps': 12351.91450438358}
{'eval/walltime': 57.914727210998535, 'training/sps': 49036.28803818808, 'training/walltime': 126.61574149131775, 'training/entropy_loss': Array(-0.006, dtype=float32), 'training/policy_loss': Array(-0.012, dtype=float32), 'training/total_loss': Array(0.21, dtype=float32), 'training/v_loss': Array(0.227, dtype=float32), 'eval/episode_distance_from_origin': Array(436.573, dtype=float32), 'eval/episode_forward_reward': Array(242.056, dtype=float32), 'eval/episode_reward': Array(932.459, dtype=float32), 'eval/episode_reward_alive': Array(771.719, dtype=float32), 'eval/episode_reward_linvel': Array(242.056, dtype=float32), 'eval/episode_reward_quadctrl': Array(-81.317, dtype=float32), 'eval/episode_x_position': Array(376.057, dtype=float32), 'eval/episode_x_velocity': Array(193.645, dtype=float32), 'eval/episode_y_position': Array(103.532, dtype=float32), 'eval/episode_y_velocity': Array(55.168, dtype=float32), 'eval/episode_distance_from_origin_std': Array(381.376, dtype=float32), 'eval/episode_forward_reward_std': Array(117.194, dtype=float32), 'eval/episode_reward_std': Array(383.963, dtype=float32), 'eval/episode_reward_alive_std': Array(303.148, dtype=float32), 'eval/episode_reward_linvel_std': Array(117.194, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(32.714, dtype=float32), 'eval/episode_x_position_std': Array(357.654, dtype=float32), 'eval/episode_x_velocity_std': Array(93.755, dtype=float32), 'eval/episode_y_position_std': Array(128.871, dtype=float32), 'eval/episode_y_velocity_std': Array(43.036, dtype=float32), 'eval/avg_episode_length': Array(154.344, dtype=float32), 'eval/epoch_eval_time': 10.358793497085571, 'eval/sps': 12356.651383774817}
{'eval/walltime': 68.28529000282288, 'training/sps': 48942.32580588885, 'training/walltime': 180.17756056785583, 'training/entropy_loss': Array(-0.005, dtype=float32), 'training/policy_loss': Array(-0.01, dtype=float32), 'training/total_loss': Array(0.187, dtype=float32), 'training/v_loss': Array(0.201, dtype=float32), 'eval/episode_distance_from_origin': Array(3979.566, dtype=float32), 'eval/episode_forward_reward': Array(761.507, dtype=float32), 'eval/episode_reward': Array(2429.684, dtype=float32), 'eval/episode_reward_alive': Array(1867.93, dtype=float32), 'eval/episode_reward_linvel': Array(761.507, dtype=float32), 'eval/episode_reward_quadctrl': Array(-199.753, dtype=float32), 'eval/episode_x_position': Array(3823.142, dtype=float32), 'eval/episode_x_velocity': Array(609.206, dtype=float32), 'eval/episode_y_position': Array(849.417, dtype=float32), 'eval/episode_y_velocity': Array(122.808, dtype=float32), 'eval/episode_distance_from_origin_std': Array(5057.858, dtype=float32), 'eval/episode_forward_reward_std': Array(518.943, dtype=float32), 'eval/episode_reward_std': Array(1583.836, dtype=float32), 'eval/episode_reward_alive_std': Array(1195.409, dtype=float32), 'eval/episode_reward_linvel_std': Array(518.943, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(128.702, dtype=float32), 'eval/episode_x_position_std': Array(4911.381, dtype=float32), 'eval/episode_x_velocity_std': Array(415.156, dtype=float32), 'eval/episode_y_position_std': Array(1215.258, dtype=float32), 'eval/episode_y_velocity_std': Array(115.356, dtype=float32), 'eval/avg_episode_length': Array(373.586, dtype=float32), 'eval/epoch_eval_time': 10.37056279182434, 'eval/sps': 12342.62812630662}
{'eval/walltime': 78.64044618606567, 'training/sps': 48859.49970321112, 'training/walltime': 233.83017706871033, 'training/entropy_loss': Array(-0.004, dtype=float32), 'training/policy_loss': Array(-0.009, dtype=float32), 'training/total_loss': Array(0.126, dtype=float32), 'training/v_loss': Array(0.138, dtype=float32), 'eval/episode_distance_from_origin': Array(10750.784, dtype=float32), 'eval/episode_forward_reward': Array(1494.418, dtype=float32), 'eval/episode_reward': Array(4025.697, dtype=float32), 'eval/episode_reward_alive': Array(2843.789, dtype=float32), 'eval/episode_reward_linvel': Array(1494.418, dtype=float32), 'eval/episode_reward_quadctrl': Array(-312.51, dtype=float32), 'eval/episode_x_position': Array(10616.1, dtype=float32), 'eval/episode_x_velocity': Array(1195.538, dtype=float32), 'eval/episode_y_position': Array(1262.844, dtype=float32), 'eval/episode_y_velocity': Array(135.254, dtype=float32), 'eval/episode_distance_from_origin_std': Array(9823.849, dtype=float32), 'eval/episode_forward_reward_std': Array(858.391, dtype=float32), 'eval/episode_reward_std': Array(2235.492, dtype=float32), 'eval/episode_reward_alive_std': Array(1549.246, dtype=float32), 'eval/episode_reward_linvel_std': Array(858.391, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(170.751, dtype=float32), 'eval/episode_x_position_std': Array(9740.506, dtype=float32), 'eval/episode_x_velocity_std': Array(686.715, dtype=float32), 'eval/episode_y_position_std': Array(1306.909, dtype=float32), 'eval/episode_y_velocity_std': Array(103.446, dtype=float32), 'eval/avg_episode_length': Array(568.758, dtype=float32), 'eval/epoch_eval_time': 10.355156183242798, 'eval/sps': 12360.99173541541}
time to jit: 0:00:50.157546
time to train: 0:04:35.494757

--------

Iteration2-Robot2

--------

{'eval/walltime': 47.92904305458069, 'training/sps': 35950.45805846483, 'training/walltime': 72.91812515258789, 'training/entropy_loss': Array(-0.008, dtype=float32), 'training/policy_loss': Array(-0.02, dtype=float32), 'training/total_loss': Array(0.1, dtype=float32), 'training/v_loss': Array(0.127, dtype=float32), 'eval/episode_distance_from_origin': Array(170.471, dtype=float32), 'eval/episode_forward_reward': Array(114.573, dtype=float32), 'eval/episode_reward': Array(620.145, dtype=float32), 'eval/episode_reward_alive': Array(563.086, dtype=float32), 'eval/episode_reward_linvel': Array(114.573, dtype=float32), 'eval/episode_reward_quadctrl': Array(-57.513, dtype=float32), 'eval/episode_x_position': Array(115.463, dtype=float32), 'eval/episode_x_velocity': Array(91.658, dtype=float32), 'eval/episode_y_position': Array(37.058, dtype=float32), 'eval/episode_y_velocity': Array(31.823, dtype=float32), 'eval/episode_distance_from_origin_std': Array(118.542, dtype=float32), 'eval/episode_forward_reward_std': Array(50.243, dtype=float32), 'eval/episode_reward_std': Array(222.357, dtype=float32), 'eval/episode_reward_alive_std': Array(196.853, dtype=float32), 'eval/episode_reward_linvel_std': Array(50.243, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(20.184, dtype=float32), 'eval/episode_x_position_std': Array(106.358, dtype=float32), 'eval/episode_x_velocity_std': Array(40.195, dtype=float32), 'eval/episode_y_position_std': Array(44.477, dtype=float32), 'eval/episode_y_velocity_std': Array(25.981, dtype=float32), 'eval/avg_episode_length': Array(112.617, dtype=float32), 'eval/epoch_eval_time': 10.368552684783936, 'eval/sps': 12345.020938924546}
{'eval/walltime': 58.29900646209717, 'training/sps': 49033.19569217941, 'training/walltime': 126.38068175315857, 'training/entropy_loss': Array(-0.006, dtype=float32), 'training/policy_loss': Array(-0.013, dtype=float32), 'training/total_loss': Array(0.138, dtype=float32), 'training/v_loss': Array(0.157, dtype=float32), 'eval/episode_distance_from_origin': Array(1555.046, dtype=float32), 'eval/episode_forward_reward': Array(389.748, dtype=float32), 'eval/episode_reward': Array(1645.347, dtype=float32), 'eval/episode_reward_alive': Array(1402.266, dtype=float32), 'eval/episode_reward_linvel': Array(389.748, dtype=float32), 'eval/episode_reward_quadctrl': Array(-146.667, dtype=float32), 'eval/episode_x_position': Array(1446.729, dtype=float32), 'eval/episode_x_velocity': Array(311.798, dtype=float32), 'eval/episode_y_position': Array(351.188, dtype=float32), 'eval/episode_y_velocity': Array(83.352, dtype=float32), 'eval/episode_distance_from_origin_std': Array(2439.291, dtype=float32), 'eval/episode_forward_reward_std': Array(276.787, dtype=float32), 'eval/episode_reward_std': Array(1061.7, dtype=float32), 'eval/episode_reward_alive_std': Array(881.531, dtype=float32), 'eval/episode_reward_linvel_std': Array(276.787, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(94.427, dtype=float32), 'eval/episode_x_position_std': Array(2354.997, dtype=float32), 'eval/episode_x_velocity_std': Array(221.43, dtype=float32), 'eval/episode_y_position_std': Array(628.457, dtype=float32), 'eval/episode_y_velocity_std': Array(78.117, dtype=float32), 'eval/avg_episode_length': Array(280.453, dtype=float32), 'eval/epoch_eval_time': 10.36996340751648, 'eval/sps': 12343.341530715676}
{'eval/walltime': 68.66497039794922, 'training/sps': 48899.21309327672, 'training/walltime': 179.9897243976593, 'training/entropy_loss': Array(-0.005, dtype=float32), 'training/policy_loss': Array(-0.01, dtype=float32), 'training/total_loss': Array(0.09, dtype=float32), 'training/v_loss': Array(0.105, dtype=float32), 'eval/episode_distance_from_origin': Array(9212.676, dtype=float32), 'eval/episode_forward_reward': Array(1213.361, dtype=float32), 'eval/episode_reward': Array(4025.115, dtype=float32), 'eval/episode_reward_alive': Array(3151.641, dtype=float32), 'eval/episode_reward_linvel': Array(1213.361, dtype=float32), 'eval/episode_reward_quadctrl': Array(-339.886, dtype=float32), 'eval/episode_x_position': Array(9001.869, dtype=float32), 'eval/episode_x_velocity': Array(970.692, dtype=float32), 'eval/episode_y_position': Array(1533.906, dtype=float32), 'eval/episode_y_velocity': Array(165.76, dtype=float32), 'eval/episode_distance_from_origin_std': Array(7314.389, dtype=float32), 'eval/episode_forward_reward_std': Array(600.897, dtype=float32), 'eval/episode_reward_std': Array(1941.271, dtype=float32), 'eval/episode_reward_alive_std': Array(1505.135, dtype=float32), 'eval/episode_reward_linvel_std': Array(600.897, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(163.224, dtype=float32), 'eval/episode_x_position_std': Array(7174.887, dtype=float32), 'eval/episode_x_velocity_std': Array(480.72, dtype=float32), 'eval/episode_y_position_std': Array(1478.743, dtype=float32), 'eval/episode_y_velocity_std': Array(114.285, dtype=float32), 'eval/avg_episode_length': Array(630.328, dtype=float32), 'eval/epoch_eval_time': 10.36596393585205, 'eval/sps': 12348.10392859801}
{'eval/walltime': 79.01128911972046, 'training/sps': 48848.61750837846, 'training/walltime': 233.6542932987213, 'training/entropy_loss': Array(-0.003, dtype=float32), 'training/policy_loss': Array(-0.008, dtype=float32), 'training/total_loss': Array(0.082, dtype=float32), 'training/v_loss': Array(0.094, dtype=float32), 'eval/episode_distance_from_origin': Array(13883.146, dtype=float32), 'eval/episode_forward_reward': Array(1698.704, dtype=float32), 'eval/episode_reward': Array(4723.559, dtype=float32), 'eval/episode_reward_alive': Array(3399.297, dtype=float32), 'eval/episode_reward_linvel': Array(1698.704, dtype=float32), 'eval/episode_reward_quadctrl': Array(-374.441, dtype=float32), 'eval/episode_x_position': Array(13745.463, dtype=float32), 'eval/episode_x_velocity': Array(1358.967, dtype=float32), 'eval/episode_y_position': Array(1366.856, dtype=float32), 'eval/episode_y_velocity': Array(136.535, dtype=float32), 'eval/episode_distance_from_origin_std': Array(10180.835, dtype=float32), 'eval/episode_forward_reward_std': Array(865.069, dtype=float32), 'eval/episode_reward_std': Array(2332.906, dtype=float32), 'eval/episode_reward_alive_std': Array(1653.312, dtype=float32), 'eval/episode_reward_linvel_std': Array(865.069, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(183.967, dtype=float32), 'eval/episode_x_position_std': Array(10112.808, dtype=float32), 'eval/episode_x_velocity_std': Array(692.058, dtype=float32), 'eval/episode_y_position_std': Array(1270.055, dtype=float32), 'eval/episode_y_velocity_std': Array(100.689, dtype=float32), 'eval/avg_episode_length': Array(679.859, dtype=float32), 'eval/epoch_eval_time': 10.34631872177124, 'eval/sps': 12371.550059699593}
time to jit: 0:00:51.868525
time to train: 0:04:35.330887

--------

Iteration2-Robot3

--------

{'eval/walltime': 47.44020366668701, 'training/sps': 35887.201981513994, 'training/walltime': 73.04665327072144, 'training/entropy_loss': Array(-0.004, dtype=float32), 'training/policy_loss': Array(0.025, dtype=float32), 'training/total_loss': Array(0.145, dtype=float32), 'training/v_loss': Array(0.124, dtype=float32), 'eval/episode_distance_from_origin': Array(39.593, dtype=float32), 'eval/episode_forward_reward': Array(30.717, dtype=float32), 'eval/episode_reward': Array(211.191, dtype=float32), 'eval/episode_reward_alive': Array(210.352, dtype=float32), 'eval/episode_reward_linvel': Array(30.717, dtype=float32), 'eval/episode_reward_quadctrl': Array(-29.878, dtype=float32), 'eval/episode_x_position': Array(11.63, dtype=float32), 'eval/episode_x_velocity': Array(24.574, dtype=float32), 'eval/episode_y_position': Array(4.7, dtype=float32), 'eval/episode_y_velocity': Array(13.859, dtype=float32), 'eval/episode_distance_from_origin_std': Array(10.316, dtype=float32), 'eval/episode_forward_reward_std': Array(12.656, dtype=float32), 'eval/episode_reward_std': Array(44.651, dtype=float32), 'eval/episode_reward_alive_std': Array(41.871, dtype=float32), 'eval/episode_reward_linvel_std': Array(12.656, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(5.993, dtype=float32), 'eval/episode_x_position_std': Array(7.038, dtype=float32), 'eval/episode_x_velocity_std': Array(10.125, dtype=float32), 'eval/episode_y_position_std': Array(4.36, dtype=float32), 'eval/episode_y_velocity_std': Array(9.293, dtype=float32), 'eval/avg_episode_length': Array(42.07, dtype=float32), 'eval/epoch_eval_time': 10.353348731994629, 'eval/sps': 12363.14967392585}
time to jit: 0:00:51.513938
time to train: 0:01:23.472471
3

--------

Iteration3-Robot0

--------

{'eval/walltime': 46.83400893211365, 'training/sps': 35666.27701931145, 'training/walltime': 73.49912071228027, 'training/entropy_loss': Array(-0.004, dtype=float32), 'training/policy_loss': Array(-0.022, dtype=float32), 'training/total_loss': Array(0.083, dtype=float32), 'training/v_loss': Array(0.109, dtype=float32), 'eval/episode_distance_from_origin': Array(39.815, dtype=float32), 'eval/episode_forward_reward': Array(34.059, dtype=float32), 'eval/episode_reward': Array(234.777, dtype=float32), 'eval/episode_reward_alive': Array(220., dtype=float32), 'eval/episode_reward_linvel': Array(34.059, dtype=float32), 'eval/episode_reward_quadctrl': Array(-19.282, dtype=float32), 'eval/episode_x_position': Array(9.735, dtype=float32), 'eval/episode_x_velocity': Array(27.247, dtype=float32), 'eval/episode_y_position': Array(-0.293, dtype=float32), 'eval/episode_y_velocity': Array(-0.723, dtype=float32), 'eval/episode_distance_from_origin_std': Array(7.256, dtype=float32), 'eval/episode_forward_reward_std': Array(11.056, dtype=float32), 'eval/episode_reward_std': Array(39.021, dtype=float32), 'eval/episode_reward_alive_std': Array(35.767, dtype=float32), 'eval/episode_reward_linvel_std': Array(11.056, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(3.315, dtype=float32), 'eval/episode_x_position_std': Array(4.376, dtype=float32), 'eval/episode_x_velocity_std': Array(8.845, dtype=float32), 'eval/episode_y_position_std': Array(2.202, dtype=float32), 'eval/episode_y_velocity_std': Array(5.259, dtype=float32), 'eval/avg_episode_length': Array(44., dtype=float32), 'eval/epoch_eval_time': 10.331688404083252, 'eval/sps': 12389.068949215727}
{'eval/walltime': 57.17832660675049, 'training/sps': 49334.00859604264, 'training/walltime': 126.63569068908691, 'training/entropy_loss': Array(-0.003, dtype=float32), 'training/policy_loss': Array(-0.019, dtype=float32), 'training/total_loss': Array(0.128, dtype=float32), 'training/v_loss': Array(0.15, dtype=float32), 'eval/episode_distance_from_origin': Array(135.766, dtype=float32), 'eval/episode_forward_reward': Array(148.041, dtype=float32), 'eval/episode_reward': Array(539.839, dtype=float32), 'eval/episode_reward_alive': Array(436.797, dtype=float32), 'eval/episode_reward_linvel': Array(148.041, dtype=float32), 'eval/episode_reward_quadctrl': Array(-44.999, dtype=float32), 'eval/episode_x_position': Array(96.24, dtype=float32), 'eval/episode_x_velocity': Array(118.433, dtype=float32), 'eval/episode_y_position': Array(8.428, dtype=float32), 'eval/episode_y_velocity': Array(6.44, dtype=float32), 'eval/episode_distance_from_origin_std': Array(60.706, dtype=float32), 'eval/episode_forward_reward_std': Array(54.252, dtype=float32), 'eval/episode_reward_std': Array(133.656, dtype=float32), 'eval/episode_reward_alive_std': Array(91.147, dtype=float32), 'eval/episode_reward_linvel_std': Array(54.252, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(10.252, dtype=float32), 'eval/episode_x_position_std': Array(57.984, dtype=float32), 'eval/episode_x_velocity_std': Array(43.402, dtype=float32), 'eval/episode_y_position_std': Array(17.508, dtype=float32), 'eval/episode_y_velocity_std': Array(20.148, dtype=float32), 'eval/avg_episode_length': Array(87.359, dtype=float32), 'eval/epoch_eval_time': 10.34431767463684, 'eval/sps': 12373.94326296091}
{'eval/walltime': 67.51664090156555, 'training/sps': 49126.94293165915, 'training/walltime': 179.99622654914856, 'training/entropy_loss': Array(-0.002, dtype=float32), 'training/policy_loss': Array(-0.014, dtype=float32), 'training/total_loss': Array(0.253, dtype=float32), 'training/v_loss': Array(0.269, dtype=float32), 'eval/episode_distance_from_origin': Array(5884.085, dtype=float32), 'eval/episode_forward_reward': Array(1141.936, dtype=float32), 'eval/episode_reward': Array(2887.116, dtype=float32), 'eval/episode_reward_alive': Array(1966.016, dtype=float32), 'eval/episode_reward_linvel': Array(1141.936, dtype=float32), 'eval/episode_reward_quadctrl': Array(-220.834, dtype=float32), 'eval/episode_x_position': Array(5773.094, dtype=float32), 'eval/episode_x_velocity': Array(913.55, dtype=float32), 'eval/episode_y_position': Array(755.618, dtype=float32), 'eval/episode_y_velocity': Array(109.742, dtype=float32), 'eval/episode_distance_from_origin_std': Array(6968.047, dtype=float32), 'eval/episode_forward_reward_std': Array(740.437, dtype=float32), 'eval/episode_reward_std': Array(1752.159, dtype=float32), 'eval/episode_reward_alive_std': Array(1143.93, dtype=float32), 'eval/episode_reward_linvel_std': Array(740.437, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(131.628, dtype=float32), 'eval/episode_x_position_std': Array(6902.691, dtype=float32), 'eval/episode_x_velocity_std': Array(592.352, dtype=float32), 'eval/episode_y_position_std': Array(991.94, dtype=float32), 'eval/episode_y_velocity_std': Array(105.884, dtype=float32), 'eval/avg_episode_length': Array(393.203, dtype=float32), 'eval/epoch_eval_time': 10.338314294815063, 'eval/sps': 12381.128716912328}
{'eval/walltime': 77.85294938087463, 'training/sps': 48997.596985216645, 'training/walltime': 233.4976258277893, 'training/entropy_loss': Array(-0.001, dtype=float32), 'training/policy_loss': Array(-0.008, dtype=float32), 'training/total_loss': Array(0.122, dtype=float32), 'training/v_loss': Array(0.131, dtype=float32), 'eval/episode_distance_from_origin': Array(23105.092, dtype=float32), 'eval/episode_forward_reward': Array(2680.957, dtype=float32), 'eval/episode_reward': Array(5968.121, dtype=float32), 'eval/episode_reward_alive': Array(3718.516, dtype=float32), 'eval/episode_reward_linvel': Array(2680.957, dtype=float32), 'eval/episode_reward_quadctrl': Array(-431.35, dtype=float32), 'eval/episode_x_position': Array(22984.318, dtype=float32), 'eval/episode_x_velocity': Array(2144.772, dtype=float32), 'eval/episode_y_position': Array(1574.05, dtype=float32), 'eval/episode_y_velocity': Array(129.799, dtype=float32), 'eval/episode_distance_from_origin_std': Array(14419.809, dtype=float32), 'eval/episode_forward_reward_std': Array(1236.878, dtype=float32), 'eval/episode_reward_std': Array(2675.233, dtype=float32), 'eval/episode_reward_alive_std': Array(1629.009, dtype=float32), 'eval/episode_reward_linvel_std': Array(1236.878, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(189.97, dtype=float32), 'eval/episode_x_position_std': Array(14374.8, dtype=float32), 'eval/episode_x_velocity_std': Array(989.506, dtype=float32), 'eval/episode_y_position_std': Array(1371.133, dtype=float32), 'eval/episode_y_velocity_std': Array(102.718, dtype=float32), 'eval/avg_episode_length': Array(743.703, dtype=float32), 'eval/epoch_eval_time': 10.336308479309082, 'eval/sps': 12383.531340635453}
time to jit: 0:00:51.039764
time to train: 0:04:35.187385

--------

Iteration3-Robot1

--------

{'eval/walltime': 47.11040711402893, 'training/sps': 35813.82405620749, 'training/walltime': 73.1963164806366, 'training/entropy_loss': Array(-0.003, dtype=float32), 'training/policy_loss': Array(0.009, dtype=float32), 'training/total_loss': Array(0.133, dtype=float32), 'training/v_loss': Array(0.127, dtype=float32), 'eval/episode_distance_from_origin': Array(54.38, dtype=float32), 'eval/episode_forward_reward': Array(29.562, dtype=float32), 'eval/episode_reward': Array(298.243, dtype=float32), 'eval/episode_reward_alive': Array(297.031, dtype=float32), 'eval/episode_reward_linvel': Array(29.562, dtype=float32), 'eval/episode_reward_quadctrl': Array(-28.35, dtype=float32), 'eval/episode_x_position': Array(11.872, dtype=float32), 'eval/episode_x_velocity': Array(23.65, dtype=float32), 'eval/episode_y_position': Array(-0.807, dtype=float32), 'eval/episode_y_velocity': Array(-2.301, dtype=float32), 'eval/episode_distance_from_origin_std': Array(23.958, dtype=float32), 'eval/episode_forward_reward_std': Array(23.976, dtype=float32), 'eval/episode_reward_std': Array(106.146, dtype=float32), 'eval/episode_reward_alive_std': Array(98.01, dtype=float32), 'eval/episode_reward_linvel_std': Array(23.976, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(10.618, dtype=float32), 'eval/episode_x_position_std': Array(15.1, dtype=float32), 'eval/episode_x_velocity_std': Array(19.181, dtype=float32), 'eval/episode_y_position_std': Array(8.121, dtype=float32), 'eval/episode_y_velocity_std': Array(15.103, dtype=float32), 'eval/avg_episode_length': Array(59.406, dtype=float32), 'eval/epoch_eval_time': 10.33280348777771, 'eval/sps': 12387.73195981192}
{'eval/walltime': 57.45167016983032, 'training/sps': 49217.43132255701, 'training/walltime': 126.45874667167664, 'training/entropy_loss': Array(-0.003, dtype=float32), 'training/policy_loss': Array(-0.015, dtype=float32), 'training/total_loss': Array(0.228, dtype=float32), 'training/v_loss': Array(0.247, dtype=float32), 'eval/episode_distance_from_origin': Array(487.358, dtype=float32), 'eval/episode_forward_reward': Array(250.437, dtype=float32), 'eval/episode_reward': Array(950.117, dtype=float32), 'eval/episode_reward_alive': Array(787.852, dtype=float32), 'eval/episode_reward_linvel': Array(250.437, dtype=float32), 'eval/episode_reward_quadctrl': Array(-88.171, dtype=float32), 'eval/episode_x_position': Array(423.952, dtype=float32), 'eval/episode_x_velocity': Array(200.349, dtype=float32), 'eval/episode_y_position': Array(67.225, dtype=float32), 'eval/episode_y_velocity': Array(19.283, dtype=float32), 'eval/episode_distance_from_origin_std': Array(595.654, dtype=float32), 'eval/episode_forward_reward_std': Array(165.93, dtype=float32), 'eval/episode_reward_std': Array(506.58, dtype=float32), 'eval/episode_reward_alive_std': Array(388.582, dtype=float32), 'eval/episode_reward_linvel_std': Array(165.93, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(46.064, dtype=float32), 'eval/episode_x_position_std': Array(563.258, dtype=float32), 'eval/episode_x_velocity_std': Array(132.744, dtype=float32), 'eval/episode_y_position_std': Array(196.095, dtype=float32), 'eval/episode_y_velocity_std': Array(51.098, dtype=float32), 'eval/avg_episode_length': Array(157.57, dtype=float32), 'eval/epoch_eval_time': 10.341263055801392, 'eval/sps': 12377.598298129811}
{'eval/walltime': 67.8045482635498, 'training/sps': 49025.15055370881, 'training/walltime': 179.9300765991211, 'training/entropy_loss': Array(-0.002, dtype=float32), 'training/policy_loss': Array(-0.01, dtype=float32), 'training/total_loss': Array(0.186, dtype=float32), 'training/v_loss': Array(0.198, dtype=float32), 'eval/episode_distance_from_origin': Array(10225.586, dtype=float32), 'eval/episode_forward_reward': Array(1388.792, dtype=float32), 'eval/episode_reward': Array(4036.211, dtype=float32), 'eval/episode_reward_alive': Array(2999.102, dtype=float32), 'eval/episode_reward_linvel': Array(1388.792, dtype=float32), 'eval/episode_reward_quadctrl': Array(-351.682, dtype=float32), 'eval/episode_x_position': Array(10061.105, dtype=float32), 'eval/episode_x_velocity': Array(1111.036, dtype=float32), 'eval/episode_y_position': Array(1315.373, dtype=float32), 'eval/episode_y_velocity': Array(131.751, dtype=float32), 'eval/episode_distance_from_origin_std': Array(8779.113, dtype=float32), 'eval/episode_forward_reward_std': Array(751.599, dtype=float32), 'eval/episode_reward_std': Array(2096.893, dtype=float32), 'eval/episode_reward_alive_std': Array(1526.682, dtype=float32), 'eval/episode_reward_linvel_std': Array(751.599, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(180.373, dtype=float32), 'eval/episode_x_position_std': Array(8679.793, dtype=float32), 'eval/episode_x_velocity_std': Array(601.281, dtype=float32), 'eval/episode_y_position_std': Array(1406.921, dtype=float32), 'eval/episode_y_velocity_std': Array(98.326, dtype=float32), 'eval/avg_episode_length': Array(599.82, dtype=float32), 'eval/epoch_eval_time': 10.352878093719482, 'eval/sps': 12363.711698455187}
{'eval/walltime': 78.15453696250916, 'training/sps': 48908.2083139543, 'training/walltime': 233.52925944328308, 'training/entropy_loss': Array(-0.001, dtype=float32), 'training/policy_loss': Array(-0.007, dtype=float32), 'training/total_loss': Array(0.098, dtype=float32), 'training/v_loss': Array(0.106, dtype=float32), 'eval/episode_distance_from_origin': Array(16526.56, dtype=float32), 'eval/episode_forward_reward': Array(2016.086, dtype=float32), 'eval/episode_reward': Array(5063.372, dtype=float32), 'eval/episode_reward_alive': Array(3469.453, dtype=float32), 'eval/episode_reward_linvel': Array(2016.086, dtype=float32), 'eval/episode_reward_quadctrl': Array(-422.166, dtype=float32), 'eval/episode_x_position': Array(16365.808, dtype=float32), 'eval/episode_x_velocity': Array(1612.873, dtype=float32), 'eval/episode_y_position': Array(1683.527, dtype=float32), 'eval/episode_y_velocity': Array(151.637, dtype=float32), 'eval/episode_distance_from_origin_std': Array(11777.62, dtype=float32), 'eval/episode_forward_reward_std': Array(984.845, dtype=float32), 'eval/episode_reward_std': Array(2392.667, dtype=float32), 'eval/episode_reward_alive_std': Array(1604.992, dtype=float32), 'eval/episode_reward_linvel_std': Array(984.845, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(195.99, dtype=float32), 'eval/episode_x_position_std': Array(11701.042, dtype=float32), 'eval/episode_x_velocity_std': Array(787.879, dtype=float32), 'eval/episode_y_position_std': Array(1515.053, dtype=float32), 'eval/episode_y_velocity_std': Array(102.348, dtype=float32), 'eval/avg_episode_length': Array(693.891, dtype=float32), 'eval/epoch_eval_time': 10.34998869895935, 'eval/sps': 12367.163262011087}
time to jit: 0:00:49.715846
time to train: 0:04:35.242174

--------

Iteration3-Robot2

--------

{'eval/walltime': 47.559081077575684, 'training/sps': 36448.42214712126, 'training/walltime': 71.92190623283386, 'training/entropy_loss': Array(-0.006, dtype=float32), 'training/policy_loss': Array(-0.023, dtype=float32), 'training/total_loss': Array(0.059, dtype=float32), 'training/v_loss': Array(0.088, dtype=float32), 'eval/episode_distance_from_origin': Array(44.048, dtype=float32), 'eval/episode_forward_reward': Array(17.682, dtype=float32), 'eval/episode_reward': Array(248.005, dtype=float32), 'eval/episode_reward_alive': Array(251.602, dtype=float32), 'eval/episode_reward_linvel': Array(17.682, dtype=float32), 'eval/episode_reward_quadctrl': Array(-21.278, dtype=float32), 'eval/episode_x_position': Array(5.243, dtype=float32), 'eval/episode_x_velocity': Array(14.145, dtype=float32), 'eval/episode_y_position': Array(1.421, dtype=float32), 'eval/episode_y_velocity': Array(5.333, dtype=float32), 'eval/episode_distance_from_origin_std': Array(7.681, dtype=float32), 'eval/episode_forward_reward_std': Array(13.312, dtype=float32), 'eval/episode_reward_std': Array(43.723, dtype=float32), 'eval/episode_reward_alive_std': Array(44.379, dtype=float32), 'eval/episode_reward_linvel_std': Array(13.312, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(3.628, dtype=float32), 'eval/episode_x_position_std': Array(3.868, dtype=float32), 'eval/episode_x_velocity_std': Array(10.649, dtype=float32), 'eval/episode_y_position_std': Array(4.116, dtype=float32), 'eval/episode_y_velocity_std': Array(10.684, dtype=float32), 'eval/avg_episode_length': Array(50.32, dtype=float32), 'eval/epoch_eval_time': 10.346975803375244, 'eval/sps': 12370.764408112915}
{'eval/walltime': 57.96787166595459, 'training/sps': 49126.931956532004, 'training/walltime': 125.28245401382446, 'training/entropy_loss': Array(-0.005, dtype=float32), 'training/policy_loss': Array(-0.015, dtype=float32), 'training/total_loss': Array(0.057, dtype=float32), 'training/v_loss': Array(0.076, dtype=float32), 'eval/episode_distance_from_origin': Array(74.555, dtype=float32), 'eval/episode_forward_reward': Array(68.994, dtype=float32), 'eval/episode_reward': Array(399.444, dtype=float32), 'eval/episode_reward_alive': Array(362.852, dtype=float32), 'eval/episode_reward_linvel': Array(68.994, dtype=float32), 'eval/episode_reward_quadctrl': Array(-32.402, dtype=float32), 'eval/episode_x_position': Array(29.83, dtype=float32), 'eval/episode_x_velocity': Array(55.195, dtype=float32), 'eval/episode_y_position': Array(4.005, dtype=float32), 'eval/episode_y_velocity': Array(7.936, dtype=float32), 'eval/episode_distance_from_origin_std': Array(18.048, dtype=float32), 'eval/episode_forward_reward_std': Array(25.823, dtype=float32), 'eval/episode_reward_std': Array(71.032, dtype=float32), 'eval/episode_reward_alive_std': Array(57.557, dtype=float32), 'eval/episode_reward_linvel_std': Array(25.823, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(6.061, dtype=float32), 'eval/episode_x_position_std': Array(15.418, dtype=float32), 'eval/episode_x_velocity_std': Array(20.658, dtype=float32), 'eval/episode_y_position_std': Array(8.577, dtype=float32), 'eval/episode_y_velocity_std': Array(14.568, dtype=float32), 'eval/avg_episode_length': Array(72.57, dtype=float32), 'eval/epoch_eval_time': 10.408790588378906, 'eval/sps': 12297.298030273378}
{'eval/walltime': 68.33413076400757, 'training/sps': 49018.709874815206, 'training/walltime': 178.76080965995789, 'training/entropy_loss': Array(-0.003, dtype=float32), 'training/policy_loss': Array(-0.015, dtype=float32), 'training/total_loss': Array(0.158, dtype=float32), 'training/v_loss': Array(0.175, dtype=float32), 'eval/episode_distance_from_origin': Array(343.105, dtype=float32), 'eval/episode_forward_reward': Array(267.859, dtype=float32), 'eval/episode_reward': Array(851.051, dtype=float32), 'eval/episode_reward_alive': Array(650.039, dtype=float32), 'eval/episode_reward_linvel': Array(267.859, dtype=float32), 'eval/episode_reward_quadctrl': Array(-66.847, dtype=float32), 'eval/episode_x_position': Array(296.236, dtype=float32), 'eval/episode_x_velocity': Array(214.287, dtype=float32), 'eval/episode_y_position': Array(30.385, dtype=float32), 'eval/episode_y_velocity': Array(22.221, dtype=float32), 'eval/episode_distance_from_origin_std': Array(249.542, dtype=float32), 'eval/episode_forward_reward_std': Array(108.156, dtype=float32), 'eval/episode_reward_std': Array(264.812, dtype=float32), 'eval/episode_reward_alive_std': Array(177.355, dtype=float32), 'eval/episode_reward_linvel_std': Array(108.156, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(19.813, dtype=float32), 'eval/episode_x_position_std': Array(243.159, dtype=float32), 'eval/episode_x_velocity_std': Array(86.525, dtype=float32), 'eval/episode_y_position_std': Array(58.537, dtype=float32), 'eval/episode_y_velocity_std': Array(32.58, dtype=float32), 'eval/avg_episode_length': Array(130.008, dtype=float32), 'eval/epoch_eval_time': 10.366259098052979, 'eval/sps': 12347.752336620772}
time to jit: 0:00:51.605363
time to train: 0:03:30.076549

--------

Iteration3-Robot3

--------

{'eval/walltime': 47.540674448013306, 'training/sps': 36628.461551259, 'training/walltime': 71.56838941574097, 'training/entropy_loss': Array(-0.002, dtype=float32), 'training/policy_loss': Array(-0.023, dtype=float32), 'training/total_loss': Array(0.035, dtype=float32), 'training/v_loss': Array(0.06, dtype=float32), 'eval/episode_distance_from_origin': Array(31.098, dtype=float32), 'eval/episode_forward_reward': Array(14.089, dtype=float32), 'eval/episode_reward': Array(176.034, dtype=float32), 'eval/episode_reward_alive': Array(178.633, dtype=float32), 'eval/episode_reward_linvel': Array(14.089, dtype=float32), 'eval/episode_reward_quadctrl': Array(-16.688, dtype=float32), 'eval/episode_x_position': Array(3.353, dtype=float32), 'eval/episode_x_velocity': Array(11.271, dtype=float32), 'eval/episode_y_position': Array(-0.656, dtype=float32), 'eval/episode_y_velocity': Array(-1.452, dtype=float32), 'eval/episode_distance_from_origin_std': Array(6.643, dtype=float32), 'eval/episode_forward_reward_std': Array(7.163, dtype=float32), 'eval/episode_reward_std': Array(37.332, dtype=float32), 'eval/episode_reward_alive_std': Array(39.847, dtype=float32), 'eval/episode_reward_linvel_std': Array(7.163, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(3.512, dtype=float32), 'eval/episode_x_position_std': Array(1.778, dtype=float32), 'eval/episode_x_velocity_std': Array(5.731, dtype=float32), 'eval/episode_y_position_std': Array(1.491, dtype=float32), 'eval/episode_y_velocity_std': Array(4.994, dtype=float32), 'eval/avg_episode_length': Array(35.727, dtype=float32), 'eval/epoch_eval_time': 10.337807655334473, 'eval/sps': 12381.735496302252}
{'eval/walltime': 57.88023924827576, 'training/sps': 49169.5358926974, 'training/walltime': 124.8827018737793, 'training/entropy_loss': Array(-0.003, dtype=float32), 'training/policy_loss': Array(-0.016, dtype=float32), 'training/total_loss': Array(0.057, dtype=float32), 'training/v_loss': Array(0.075, dtype=float32), 'eval/episode_distance_from_origin': Array(52.963, dtype=float32), 'eval/episode_forward_reward': Array(26.395, dtype=float32), 'eval/episode_reward': Array(299.595, dtype=float32), 'eval/episode_reward_alive': Array(299.805, dtype=float32), 'eval/episode_reward_linvel': Array(26.395, dtype=float32), 'eval/episode_reward_quadctrl': Array(-26.604, dtype=float32), 'eval/episode_x_position': Array(10.061, dtype=float32), 'eval/episode_x_velocity': Array(21.116, dtype=float32), 'eval/episode_y_position': Array(-3.344, dtype=float32), 'eval/episode_y_velocity': Array(-7.114, dtype=float32), 'eval/episode_distance_from_origin_std': Array(8.805, dtype=float32), 'eval/episode_forward_reward_std': Array(16.045, dtype=float32), 'eval/episode_reward_std': Array(48.413, dtype=float32), 'eval/episode_reward_alive_std': Array(49.613, dtype=float32), 'eval/episode_reward_linvel_std': Array(16.045, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(4.412, dtype=float32), 'eval/episode_x_position_std': Array(4.913, dtype=float32), 'eval/episode_x_velocity_std': Array(12.836, dtype=float32), 'eval/episode_y_position_std': Array(3.636, dtype=float32), 'eval/episode_y_velocity_std': Array(8.487, dtype=float32), 'eval/avg_episode_length': Array(59.961, dtype=float32), 'eval/epoch_eval_time': 10.339564800262451, 'eval/sps': 12379.631297126834}
time to jit: 0:00:51.748373
time to train: 0:02:25.710849
4

--------

Iteration4-Robot0

--------

{'eval/walltime': 48.230348348617554, 'training/sps': 35575.6333598922, 'training/walltime': 73.68639016151428, 'training/entropy_loss': Array(-0.001, dtype=float32), 'training/policy_loss': Array(-0.016, dtype=float32), 'training/total_loss': Array(0.93, dtype=float32), 'training/v_loss': Array(0.947, dtype=float32), 'eval/episode_distance_from_origin': Array(5388.453, dtype=float32), 'eval/episode_forward_reward': Array(1240.46, dtype=float32), 'eval/episode_reward': Array(2675.073, dtype=float32), 'eval/episode_reward_alive': Array(1622.031, dtype=float32), 'eval/episode_reward_linvel': Array(1240.46, dtype=float32), 'eval/episode_reward_quadctrl': Array(-187.418, dtype=float32), 'eval/episode_x_position': Array(5310.115, dtype=float32), 'eval/episode_x_velocity': Array(992.369, dtype=float32), 'eval/episode_y_position': Array(556.682, dtype=float32), 'eval/episode_y_velocity': Array(99.544, dtype=float32), 'eval/episode_distance_from_origin_std': Array(7728.223, dtype=float32), 'eval/episode_forward_reward_std': Array(910.847, dtype=float32), 'eval/episode_reward_std': Array(1806.982, dtype=float32), 'eval/episode_reward_alive_std': Array(1019.394, dtype=float32), 'eval/episode_reward_linvel_std': Array(910.847, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(122.639, dtype=float32), 'eval/episode_x_position_std': Array(7689.888, dtype=float32), 'eval/episode_x_velocity_std': Array(728.68, dtype=float32), 'eval/episode_y_position_std': Array(778.047, dtype=float32), 'eval/episode_y_velocity_std': Array(75.614, dtype=float32), 'eval/avg_episode_length': Array(324.406, dtype=float32), 'eval/epoch_eval_time': 10.360024690628052, 'eval/sps': 12355.182909534196}
{'eval/walltime': 58.591681241989136, 'training/sps': 49307.689147452475, 'training/walltime': 126.85132336616516, 'training/entropy_loss': Array(0.001, dtype=float32), 'training/policy_loss': Array(-0.006, dtype=float32), 'training/total_loss': Array(0.205, dtype=float32), 'training/v_loss': Array(0.21, dtype=float32), 'eval/episode_distance_from_origin': Array(32569.848, dtype=float32), 'eval/episode_forward_reward': Array(3672.446, dtype=float32), 'eval/episode_reward': Array(7317.404, dtype=float32), 'eval/episode_reward_alive': Array(4126.172, dtype=float32), 'eval/episode_reward_linvel': Array(3672.446, dtype=float32), 'eval/episode_reward_quadctrl': Array(-481.211, dtype=float32), 'eval/episode_x_position': Array(32433.451, dtype=float32), 'eval/episode_x_velocity': Array(2937.966, dtype=float32), 'eval/episode_y_position': Array(2217.988, dtype=float32), 'eval/episode_y_velocity': Array(193.016, dtype=float32), 'eval/episode_distance_from_origin_std': Array(15816.85, dtype=float32), 'eval/episode_forward_reward_std': Array(1357.732, dtype=float32), 'eval/episode_reward_std': Array(2626.252, dtype=float32), 'eval/episode_reward_alive_std': Array(1439.12, dtype=float32), 'eval/episode_reward_linvel_std': Array(1357.732, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(170.082, dtype=float32), 'eval/episode_x_position_std': Array(15770.509, dtype=float32), 'eval/episode_x_velocity_std': Array(1086.19, dtype=float32), 'eval/episode_y_position_std': Array(1487.781, dtype=float32), 'eval/episode_y_velocity_std': Array(95.365, dtype=float32), 'eval/avg_episode_length': Array(825.234, dtype=float32), 'eval/epoch_eval_time': 10.361332893371582, 'eval/sps': 12353.622966972229}
{'eval/walltime': 68.94009828567505, 'training/sps': 49080.751913631364, 'training/walltime': 180.2620780467987, 'training/entropy_loss': Array(0.002, dtype=float32), 'training/policy_loss': Array(-0.002, dtype=float32), 'training/total_loss': Array(0.097, dtype=float32), 'training/v_loss': Array(0.097, dtype=float32), 'eval/episode_distance_from_origin': Array(36759.08, dtype=float32), 'eval/episode_forward_reward': Array(4147.056, dtype=float32), 'eval/episode_reward': Array(7758.641, dtype=float32), 'eval/episode_reward_alive': Array(4101.953, dtype=float32), 'eval/episode_reward_linvel': Array(4147.056, dtype=float32), 'eval/episode_reward_quadctrl': Array(-490.366, dtype=float32), 'eval/episode_x_position': Array(36677.695, dtype=float32), 'eval/episode_x_velocity': Array(3317.655, dtype=float32), 'eval/episode_y_position': Array(1369.295, dtype=float32), 'eval/episode_y_velocity': Array(129.685, dtype=float32), 'eval/episode_distance_from_origin_std': Array(18319.398, dtype=float32), 'eval/episode_forward_reward_std': Array(1579.655, dtype=float32), 'eval/episode_reward_std': Array(2876.338, dtype=float32), 'eval/episode_reward_alive_std': Array(1475.797, dtype=float32), 'eval/episode_reward_linvel_std': Array(1579.655, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(178.759, dtype=float32), 'eval/episode_x_position_std': Array(18301.137, dtype=float32), 'eval/episode_x_velocity_std': Array(1263.729, dtype=float32), 'eval/episode_y_position_std': Array(1222.954, dtype=float32), 'eval/episode_y_velocity_std': Array(88.983, dtype=float32), 'eval/avg_episode_length': Array(820.391, dtype=float32), 'eval/epoch_eval_time': 10.348417043685913, 'eval/sps': 12369.04151230542}
{'eval/walltime': 79.30398917198181, 'training/sps': 49013.85229046576, 'training/walltime': 233.74573373794556, 'training/entropy_loss': Array(0.004, dtype=float32), 'training/policy_loss': Array(0.001, dtype=float32), 'training/total_loss': Array(0.138, dtype=float32), 'training/v_loss': Array(0.133, dtype=float32), 'eval/episode_distance_from_origin': Array(45845.203, dtype=float32), 'eval/episode_forward_reward': Array(4940.442, dtype=float32), 'eval/episode_reward': Array(8945.357, dtype=float32), 'eval/episode_reward_alive': Array(4551.875, dtype=float32), 'eval/episode_reward_linvel': Array(4940.442, dtype=float32), 'eval/episode_reward_quadctrl': Array(-546.956, dtype=float32), 'eval/episode_x_position': Array(45724.156, dtype=float32), 'eval/episode_x_velocity': Array(3952.366, dtype=float32), 'eval/episode_y_position': Array(2559.154, dtype=float32), 'eval/episode_y_velocity': Array(220.67, dtype=float32), 'eval/episode_distance_from_origin_std': Array(14876.18, dtype=float32), 'eval/episode_forward_reward_std': Array(1331.161, dtype=float32), 'eval/episode_reward_std': Array(2362.504, dtype=float32), 'eval/episode_reward_alive_std': Array(1174.392, dtype=float32), 'eval/episode_reward_linvel_std': Array(1331.161, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(142.71, dtype=float32), 'eval/episode_x_position_std': Array(14846.433, dtype=float32), 'eval/episode_x_velocity_std': Array(1064.933, dtype=float32), 'eval/episode_y_position_std': Array(1215.455, dtype=float32), 'eval/episode_y_velocity_std': Array(82.599, dtype=float32), 'eval/avg_episode_length': Array(910.375, dtype=float32), 'eval/epoch_eval_time': 10.363890886306763, 'eval/sps': 12350.573872706373}
time to jit: 0:00:50.782713
time to train: 0:04:35.430429

--------

Iteration4-Robot1

--------

{'eval/walltime': 48.64795160293579, 'training/sps': 35411.330501599106, 'training/walltime': 74.02828311920166, 'training/entropy_loss': Array(-0.002, dtype=float32), 'training/policy_loss': Array(-0.027, dtype=float32), 'training/total_loss': Array(0.067, dtype=float32), 'training/v_loss': Array(0.096, dtype=float32), 'eval/episode_distance_from_origin': Array(34.575, dtype=float32), 'eval/episode_forward_reward': Array(21.293, dtype=float32), 'eval/episode_reward': Array(199.774, dtype=float32), 'eval/episode_reward_alive': Array(194.258, dtype=float32), 'eval/episode_reward_linvel': Array(21.293, dtype=float32), 'eval/episode_reward_quadctrl': Array(-15.777, dtype=float32), 'eval/episode_x_position': Array(5.339, dtype=float32), 'eval/episode_x_velocity': Array(17.035, dtype=float32), 'eval/episode_y_position': Array(-1.249, dtype=float32), 'eval/episode_y_velocity': Array(-4.624, dtype=float32), 'eval/episode_distance_from_origin_std': Array(5.426, dtype=float32), 'eval/episode_forward_reward_std': Array(11.571, dtype=float32), 'eval/episode_reward_std': Array(33.542, dtype=float32), 'eval/episode_reward_alive_std': Array(27.387, dtype=float32), 'eval/episode_reward_linvel_std': Array(11.571, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(2.455, dtype=float32), 'eval/episode_x_position_std': Array(3.301, dtype=float32), 'eval/episode_x_velocity_std': Array(9.257, dtype=float32), 'eval/episode_y_position_std': Array(1.418, dtype=float32), 'eval/episode_y_velocity_std': Array(5.388, dtype=float32), 'eval/avg_episode_length': Array(38.852, dtype=float32), 'eval/epoch_eval_time': 10.354471921920776, 'eval/sps': 12361.808594895077}
time to jit: 0:00:51.246553
time to train: 0:01:24.462335

--------

Iteration4-Robot2

--------

{'eval/walltime': 48.47671937942505, 'training/sps': 36480.23974155236, 'training/walltime': 71.85917687416077, 'training/entropy_loss': Array(-0.001, dtype=float32), 'training/policy_loss': Array(-0.016, dtype=float32), 'training/total_loss': Array(0.74, dtype=float32), 'training/v_loss': Array(0.757, dtype=float32), 'eval/episode_distance_from_origin': Array(1477.55, dtype=float32), 'eval/episode_forward_reward': Array(573.111, dtype=float32), 'eval/episode_reward': Array(1394.637, dtype=float32), 'eval/episode_reward_alive': Array(923.516, dtype=float32), 'eval/episode_reward_linvel': Array(573.111, dtype=float32), 'eval/episode_reward_quadctrl': Array(-101.989, dtype=float32), 'eval/episode_x_position': Array(1428.322, dtype=float32), 'eval/episode_x_velocity': Array(458.489, dtype=float32), 'eval/episode_y_position': Array(148.736, dtype=float32), 'eval/episode_y_velocity': Array(41.076, dtype=float32), 'eval/episode_distance_from_origin_std': Array(2291.515, dtype=float32), 'eval/episode_forward_reward_std': Array(480.131, dtype=float32), 'eval/episode_reward_std': Array(1025.481, dtype=float32), 'eval/episode_reward_alive_std': Array(617.899, dtype=float32), 'eval/episode_reward_linvel_std': Array(480.131, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(71.963, dtype=float32), 'eval/episode_x_position_std': Array(2270.416, dtype=float32), 'eval/episode_x_velocity_std': Array(384.105, dtype=float32), 'eval/episode_y_position_std': Array(302.556, dtype=float32), 'eval/episode_y_velocity_std': Array(60.964, dtype=float32), 'eval/avg_episode_length': Array(184.703, dtype=float32), 'eval/epoch_eval_time': 10.3420090675354, 'eval/sps': 12376.70545095583}
time to jit: 0:00:51.911554
time to train: 0:01:22.278534

--------

Iteration4-Robot3

--------

{'eval/walltime': 48.102214097976685, 'training/sps': 36501.36589940025, 'training/walltime': 71.81758642196655, 'training/entropy_loss': Array(-0.002, dtype=float32), 'training/policy_loss': Array(-0.013, dtype=float32), 'training/total_loss': Array(0.229, dtype=float32), 'training/v_loss': Array(0.244, dtype=float32), 'eval/episode_distance_from_origin': Array(910.752, dtype=float32), 'eval/episode_forward_reward': Array(348.024, dtype=float32), 'eval/episode_reward': Array(1230.294, dtype=float32), 'eval/episode_reward_alive': Array(990.586, dtype=float32), 'eval/episode_reward_linvel': Array(348.024, dtype=float32), 'eval/episode_reward_quadctrl': Array(-108.316, dtype=float32), 'eval/episode_x_position': Array(807.504, dtype=float32), 'eval/episode_x_velocity': Array(278.42, dtype=float32), 'eval/episode_y_position': Array(291.581, dtype=float32), 'eval/episode_y_velocity': Array(103.424, dtype=float32), 'eval/episode_distance_from_origin_std': Array(1172.942, dtype=float32), 'eval/episode_forward_reward_std': Array(196.528, dtype=float32), 'eval/episode_reward_std': Array(630.108, dtype=float32), 'eval/episode_reward_alive_std': Array(490.85, dtype=float32), 'eval/episode_reward_linvel_std': Array(196.528, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(53.907, dtype=float32), 'eval/episode_x_position_std': Array(1076.361, dtype=float32), 'eval/episode_x_velocity_std': Array(157.223, dtype=float32), 'eval/episode_y_position_std': Array(469.456, dtype=float32), 'eval/episode_y_velocity_std': Array(81.102, dtype=float32), 'eval/avg_episode_length': Array(198.117, dtype=float32), 'eval/epoch_eval_time': 10.3485586643219, 'eval/sps': 12368.872241242432}
time to jit: 0:00:52.964476
time to train: 0:01:22.247582
5

--------

Iteration5-Robot0

--------

{'eval/walltime': 45.271897077560425, 'training/sps': 35735.533979841624, 'training/walltime': 73.35667634010315, 'training/entropy_loss': Array(0.002, dtype=float32), 'training/policy_loss': Array(-0.008, dtype=float32), 'training/total_loss': Array(0.544, dtype=float32), 'training/v_loss': Array(0.55, dtype=float32), 'eval/episode_distance_from_origin': Array(2939.673, dtype=float32), 'eval/episode_forward_reward': Array(703.863, dtype=float32), 'eval/episode_reward': Array(1882.416, dtype=float32), 'eval/episode_reward_alive': Array(1314.336, dtype=float32), 'eval/episode_reward_linvel': Array(703.863, dtype=float32), 'eval/episode_reward_quadctrl': Array(-135.783, dtype=float32), 'eval/episode_x_position': Array(2760.511, dtype=float32), 'eval/episode_x_velocity': Array(563.091, dtype=float32), 'eval/episode_y_position': Array(785.397, dtype=float32), 'eval/episode_y_velocity': Array(174.689, dtype=float32), 'eval/episode_distance_from_origin_std': Array(4479.999, dtype=float32), 'eval/episode_forward_reward_std': Array(613.864, dtype=float32), 'eval/episode_reward_std': Array(1471.312, dtype=float32), 'eval/episode_reward_alive_std': Array(960.42, dtype=float32), 'eval/episode_reward_linvel_std': Array(613.864, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(102.04, dtype=float32), 'eval/episode_x_position_std': Array(4286.276, dtype=float32), 'eval/episode_x_velocity_std': Array(491.092, dtype=float32), 'eval/episode_y_position_std': Array(1320.04, dtype=float32), 'eval/episode_y_velocity_std': Array(158.292, dtype=float32), 'eval/avg_episode_length': Array(262.867, dtype=float32), 'eval/epoch_eval_time': 10.354720830917358, 'eval/sps': 12361.511439093048}
{'eval/walltime': 55.64921951293945, 'training/sps': 49290.42942071806, 'training/walltime': 126.54022598266602, 'training/entropy_loss': Array(0.004, dtype=float32), 'training/policy_loss': Array(-0.006, dtype=float32), 'training/total_loss': Array(0.216, dtype=float32), 'training/v_loss': Array(0.218, dtype=float32), 'eval/episode_distance_from_origin': Array(23990.031, dtype=float32), 'eval/episode_forward_reward': Array(2883.313, dtype=float32), 'eval/episode_reward': Array(5954.008, dtype=float32), 'eval/episode_reward_alive': Array(3464.883, dtype=float32), 'eval/episode_reward_linvel': Array(2883.313, dtype=float32), 'eval/episode_reward_quadctrl': Array(-394.186, dtype=float32), 'eval/episode_x_position': Array(22685.465, dtype=float32), 'eval/episode_x_velocity': Array(2306.657, dtype=float32), 'eval/episode_y_position': Array(7554.91, dtype=float32), 'eval/episode_y_velocity': Array(800.005, dtype=float32), 'eval/episode_distance_from_origin_std': Array(16937.69, dtype=float32), 'eval/episode_forward_reward_std': Array(1385.75, dtype=float32), 'eval/episode_reward_std': Array(2761.095, dtype=float32), 'eval/episode_reward_alive_std': Array(1554.487, dtype=float32), 'eval/episode_reward_linvel_std': Array(1385.75, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(177.951, dtype=float32), 'eval/episode_x_position_std': Array(16058.259, dtype=float32), 'eval/episode_x_velocity_std': Array(1108.604, dtype=float32), 'eval/episode_y_position_std': Array(5465.766, dtype=float32), 'eval/episode_y_velocity_std': Array(379.319, dtype=float32), 'eval/avg_episode_length': Array(692.977, dtype=float32), 'eval/epoch_eval_time': 10.377322435379028, 'eval/sps': 12334.588309949226}
{'eval/walltime': 66.03124260902405, 'training/sps': 49142.61316995054, 'training/walltime': 179.88374662399292, 'training/entropy_loss': Array(0.006, dtype=float32), 'training/policy_loss': Array(-0.001, dtype=float32), 'training/total_loss': Array(0.15, dtype=float32), 'training/v_loss': Array(0.145, dtype=float32), 'eval/episode_distance_from_origin': Array(38857.344, dtype=float32), 'eval/episode_forward_reward': Array(4259.081, dtype=float32), 'eval/episode_reward': Array(7932.099, dtype=float32), 'eval/episode_reward_alive': Array(4196.914, dtype=float32), 'eval/episode_reward_linvel': Array(4259.081, dtype=float32), 'eval/episode_reward_quadctrl': Array(-523.893, dtype=float32), 'eval/episode_x_position': Array(36935.133, dtype=float32), 'eval/episode_x_velocity': Array(3407.275, dtype=float32), 'eval/episode_y_position': Array(11795.919, dtype=float32), 'eval/episode_y_velocity': Array(1158.663, dtype=float32), 'eval/episode_distance_from_origin_std': Array(17791.73, dtype=float32), 'eval/episode_forward_reward_std': Array(1344.732, dtype=float32), 'eval/episode_reward_std': Array(2426.148, dtype=float32), 'eval/episode_reward_alive_std': Array(1238.517, dtype=float32), 'eval/episode_reward_linvel_std': Array(1344.732, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(156.619, dtype=float32), 'eval/episode_x_position_std': Array(16897.72, dtype=float32), 'eval/episode_x_velocity_std': Array(1075.791, dtype=float32), 'eval/episode_y_position_std': Array(5693.088, dtype=float32), 'eval/episode_y_velocity_std': Array(389.188, dtype=float32), 'eval/avg_episode_length': Array(839.383, dtype=float32), 'eval/epoch_eval_time': 10.382023096084595, 'eval/sps': 12329.003587775975}
{'eval/walltime': 76.40735054016113, 'training/sps': 49017.09931393683, 'training/walltime': 233.36385941505432, 'training/entropy_loss': Array(0.008, dtype=float32), 'training/policy_loss': Array(0.001, dtype=float32), 'training/total_loss': Array(0.099, dtype=float32), 'training/v_loss': Array(0.09, dtype=float32), 'eval/episode_distance_from_origin': Array(40056.617, dtype=float32), 'eval/episode_forward_reward': Array(4566.832, dtype=float32), 'eval/episode_reward': Array(8038.628, dtype=float32), 'eval/episode_reward_alive': Array(4004.922, dtype=float32), 'eval/episode_reward_linvel': Array(4566.832, dtype=float32), 'eval/episode_reward_quadctrl': Array(-533.122, dtype=float32), 'eval/episode_x_position': Array(38792.016, dtype=float32), 'eval/episode_x_velocity': Array(3653.476, dtype=float32), 'eval/episode_y_position': Array(9730.271, dtype=float32), 'eval/episode_y_velocity': Array(943.677, dtype=float32), 'eval/episode_distance_from_origin_std': Array(21241.715, dtype=float32), 'eval/episode_forward_reward_std': Array(1675.103, dtype=float32), 'eval/episode_reward_std': Array(2862.484, dtype=float32), 'eval/episode_reward_alive_std': Array(1373.89, dtype=float32), 'eval/episode_reward_linvel_std': Array(1675.103, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(186.279, dtype=float32), 'eval/episode_x_position_std': Array(20568.684, dtype=float32), 'eval/episode_x_velocity_std': Array(1340.088, dtype=float32), 'eval/episode_y_position_std': Array(5409.296, dtype=float32), 'eval/episode_y_velocity_std': Array(374.231, dtype=float32), 'eval/avg_episode_length': Array(800.984, dtype=float32), 'eval/epoch_eval_time': 10.376107931137085, 'eval/sps': 12336.032050697153}
time to jit: 0:00:49.463478
time to train: 0:04:35.136164

--------

Iteration5-Robot1

--------

{'eval/walltime': 47.905763387680054, 'training/sps': 35588.59290736522, 'training/walltime': 73.6595573425293, 'training/entropy_loss': Array(0.003, dtype=float32), 'training/policy_loss': Array(-0.01, dtype=float32), 'training/total_loss': Array(1.518, dtype=float32), 'training/v_loss': Array(1.525, dtype=float32), 'eval/episode_distance_from_origin': Array(22168.137, dtype=float32), 'eval/episode_forward_reward': Array(2872.724, dtype=float32), 'eval/episode_reward': Array(5474.191, dtype=float32), 'eval/episode_reward_alive': Array(2940.43, dtype=float32), 'eval/episode_reward_linvel': Array(2872.724, dtype=float32), 'eval/episode_reward_quadctrl': Array(-338.961, dtype=float32), 'eval/episode_x_position': Array(22069.064, dtype=float32), 'eval/episode_x_velocity': Array(2298.186, dtype=float32), 'eval/episode_y_position': Array(-1591.016, dtype=float32), 'eval/episode_y_velocity': Array(-162.532, dtype=float32), 'eval/episode_distance_from_origin_std': Array(19967.879, dtype=float32), 'eval/episode_forward_reward_std': Array(1819.285, dtype=float32), 'eval/episode_reward_std': Array(3352.75, dtype=float32), 'eval/episode_reward_alive_std': Array(1736.423, dtype=float32), 'eval/episode_reward_linvel_std': Array(1819.285, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(202.745, dtype=float32), 'eval/episode_x_position_std': Array(19912.062, dtype=float32), 'eval/episode_x_velocity_std': Array(1455.433, dtype=float32), 'eval/episode_y_position_std': Array(1515.281, dtype=float32), 'eval/episode_y_velocity_std': Array(109.363, dtype=float32), 'eval/avg_episode_length': Array(588.086, dtype=float32), 'eval/epoch_eval_time': 10.37596845626831, 'eval/sps': 12336.197872948705}
{'eval/walltime': 58.26600527763367, 'training/sps': 49256.43230353103, 'training/walltime': 126.87981462478638, 'training/entropy_loss': Array(0.005, dtype=float32), 'training/policy_loss': Array(-0.002, dtype=float32), 'training/total_loss': Array(0.201, dtype=float32), 'training/v_loss': Array(0.198, dtype=float32), 'eval/episode_distance_from_origin': Array(45884.516, dtype=float32), 'eval/episode_forward_reward': Array(4902.073, dtype=float32), 'eval/episode_reward': Array(8959.178, dtype=float32), 'eval/episode_reward_alive': Array(4583.008, dtype=float32), 'eval/episode_reward_linvel': Array(4902.073, dtype=float32), 'eval/episode_reward_quadctrl': Array(-525.9, dtype=float32), 'eval/episode_x_position': Array(45742.164, dtype=float32), 'eval/episode_x_velocity': Array(3921.671, dtype=float32), 'eval/episode_y_position': Array(-2986.732, dtype=float32), 'eval/episode_y_velocity': Array(-267.679, dtype=float32), 'eval/episode_distance_from_origin_std': Array(14899.414, dtype=float32), 'eval/episode_forward_reward_std': Array(1262.704, dtype=float32), 'eval/episode_reward_std': Array(2263.252, dtype=float32), 'eval/episode_reward_alive_std': Array(1131.832, dtype=float32), 'eval/episode_reward_linvel_std': Array(1262.704, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(131.008, dtype=float32), 'eval/episode_x_position_std': Array(14865.486, dtype=float32), 'eval/episode_x_velocity_std': Array(1010.168, dtype=float32), 'eval/episode_y_position_std': Array(1221.163, dtype=float32), 'eval/episode_y_velocity_std': Array(86.008, dtype=float32), 'eval/avg_episode_length': Array(916.602, dtype=float32), 'eval/epoch_eval_time': 10.360241889953613, 'eval/sps': 12354.923886875878}
{'eval/walltime': 68.62304735183716, 'training/sps': 49054.188269268445, 'training/walltime': 180.3194921016693, 'training/entropy_loss': Array(0.005, dtype=float32), 'training/policy_loss': Array(0.011, dtype=float32), 'training/total_loss': Array(0.166, dtype=float32), 'training/v_loss': Array(0.149, dtype=float32), 'eval/episode_distance_from_origin': Array(24.139, dtype=float32), 'eval/episode_forward_reward': Array(20.599, dtype=float32), 'eval/episode_reward': Array(142.942, dtype=float32), 'eval/episode_reward_alive': Array(135.547, dtype=float32), 'eval/episode_reward_linvel': Array(20.599, dtype=float32), 'eval/episode_reward_quadctrl': Array(-13.204, dtype=float32), 'eval/episode_x_position': Array(3.946, dtype=float32), 'eval/episode_x_velocity': Array(16.479, dtype=float32), 'eval/episode_y_position': Array(-0.25, dtype=float32), 'eval/episode_y_velocity': Array(-0.395, dtype=float32), 'eval/episode_distance_from_origin_std': Array(6.581, dtype=float32), 'eval/episode_forward_reward_std': Array(14.863, dtype=float32), 'eval/episode_reward_std': Array(41.135, dtype=float32), 'eval/episode_reward_alive_std': Array(29.714, dtype=float32), 'eval/episode_reward_linvel_std': Array(14.863, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(3.292, dtype=float32), 'eval/episode_x_position_std': Array(4.712, dtype=float32), 'eval/episode_x_velocity_std': Array(11.89, dtype=float32), 'eval/episode_y_position_std': Array(0.728, dtype=float32), 'eval/episode_y_velocity_std': Array(2.837, dtype=float32), 'eval/avg_episode_length': Array(27.109, dtype=float32), 'eval/epoch_eval_time': 10.357042074203491, 'eval/sps': 12358.740949678322}
time to jit: 0:00:50.377406
time to train: 0:03:31.620768

--------

Iteration5-Robot2

--------

{'eval/walltime': 48.850239753723145, 'training/sps': 36414.67033341131, 'training/walltime': 71.9885687828064, 'training/entropy_loss': Array(0.002, dtype=float32), 'training/policy_loss': Array(-0.011, dtype=float32), 'training/total_loss': Array(0.43, dtype=float32), 'training/v_loss': Array(0.438, dtype=float32), 'eval/episode_distance_from_origin': Array(35489.62, dtype=float32), 'eval/episode_forward_reward': Array(3897.976, dtype=float32), 'eval/episode_reward': Array(7525.434, dtype=float32), 'eval/episode_reward_alive': Array(4087.5, dtype=float32), 'eval/episode_reward_linvel': Array(3897.976, dtype=float32), 'eval/episode_reward_quadctrl': Array(-460.038, dtype=float32), 'eval/episode_x_position': Array(34384.594, dtype=float32), 'eval/episode_x_velocity': Array(3118.39, dtype=float32), 'eval/episode_y_position': Array(8566.457, dtype=float32), 'eval/episode_y_velocity': Array(790.396, dtype=float32), 'eval/episode_distance_from_origin_std': Array(17901.652, dtype=float32), 'eval/episode_forward_reward_std': Array(1452.667, dtype=float32), 'eval/episode_reward_std': Array(2731.597, dtype=float32), 'eval/episode_reward_alive_std': Array(1442.564, dtype=float32), 'eval/episode_reward_linvel_std': Array(1452.667, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(163.324, dtype=float32), 'eval/episode_x_position_std': Array(17359.05, dtype=float32), 'eval/episode_x_velocity_std': Array(1162.138, dtype=float32), 'eval/episode_y_position_std': Array(4439.01, dtype=float32), 'eval/episode_y_velocity_std': Array(301.794, dtype=float32), 'eval/avg_episode_length': Array(817.5, dtype=float32), 'eval/epoch_eval_time': 10.351684808731079, 'eval/sps': 12365.136918778575}
{'eval/walltime': 59.22243881225586, 'training/sps': 49150.97594907028, 'training/walltime': 125.32301330566406, 'training/entropy_loss': Array(0.005, dtype=float32), 'training/policy_loss': Array(0.003, dtype=float32), 'training/total_loss': Array(0.154, dtype=float32), 'training/v_loss': Array(0.146, dtype=float32), 'eval/episode_distance_from_origin': Array(19728.492, dtype=float32), 'eval/episode_forward_reward': Array(2900.571, dtype=float32), 'eval/episode_reward': Array(5330.775, dtype=float32), 'eval/episode_reward_alive': Array(2754.766, dtype=float32), 'eval/episode_reward_linvel': Array(2900.571, dtype=float32), 'eval/episode_reward_quadctrl': Array(-324.56, dtype=float32), 'eval/episode_x_position': Array(19492.916, dtype=float32), 'eval/episode_x_velocity': Array(2320.462, dtype=float32), 'eval/episode_y_position': Array(2723.666, dtype=float32), 'eval/episode_y_velocity': Array(331.922, dtype=float32), 'eval/episode_distance_from_origin_std': Array(18731.836, dtype=float32), 'eval/episode_forward_reward_std': Array(1653.618, dtype=float32), 'eval/episode_reward_std': Array(2921.034, dtype=float32), 'eval/episode_reward_alive_std': Array(1439.439, dtype=float32), 'eval/episode_reward_linvel_std': Array(1653.618, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(171.751, dtype=float32), 'eval/episode_x_position_std': Array(18541.15, dtype=float32), 'eval/episode_x_velocity_std': Array(1322.899, dtype=float32), 'eval/episode_y_position_std': Array(2686.342, dtype=float32), 'eval/episode_y_velocity_std': Array(197.812, dtype=float32), 'eval/avg_episode_length': Array(550.953, dtype=float32), 'eval/epoch_eval_time': 10.372199058532715, 'eval/sps': 12340.681014475951}
{'eval/walltime': 69.58764410018921, 'training/sps': 48981.27412351242, 'training/walltime': 178.8422417640686, 'training/entropy_loss': Array(0.006, dtype=float32), 'training/policy_loss': Array(-0.001, dtype=float32), 'training/total_loss': Array(0.182, dtype=float32), 'training/v_loss': Array(0.177, dtype=float32), 'eval/episode_distance_from_origin': Array(44418.08, dtype=float32), 'eval/episode_forward_reward': Array(4906.643, dtype=float32), 'eval/episode_reward': Array(8627.91, dtype=float32), 'eval/episode_reward_alive': Array(4226.25, dtype=float32), 'eval/episode_reward_linvel': Array(4906.643, dtype=float32), 'eval/episode_reward_quadctrl': Array(-504.978, dtype=float32), 'eval/episode_x_position': Array(44135.14, dtype=float32), 'eval/episode_x_velocity': Array(3925.326, dtype=float32), 'eval/episode_y_position': Array(4503.713, dtype=float32), 'eval/episode_y_velocity': Array(416.382, dtype=float32), 'eval/episode_distance_from_origin_std': Array(20318.465, dtype=float32), 'eval/episode_forward_reward_std': Array(1737.06, dtype=float32), 'eval/episode_reward_std': Array(2984.859, dtype=float32), 'eval/episode_reward_alive_std': Array(1419.191, dtype=float32), 'eval/episode_reward_linvel_std': Array(1737.06, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(171.208, dtype=float32), 'eval/episode_x_position_std': Array(20203.084, dtype=float32), 'eval/episode_x_velocity_std': Array(1389.653, dtype=float32), 'eval/episode_y_position_std': Array(2350.378, dtype=float32), 'eval/episode_y_velocity_std': Array(164.935, dtype=float32), 'eval/avg_episode_length': Array(845.25, dtype=float32), 'eval/epoch_eval_time': 10.36520528793335, 'eval/sps': 12349.00770841569}
{'eval/walltime': 79.96082997322083, 'training/sps': 48883.773305803814, 'training/walltime': 232.46821665763855, 'training/entropy_loss': Array(0.008, dtype=float32), 'training/policy_loss': Array(0.002, dtype=float32), 'training/total_loss': Array(0.119, dtype=float32), 'training/v_loss': Array(0.109, dtype=float32), 'eval/episode_distance_from_origin': Array(49359.074, dtype=float32), 'eval/episode_forward_reward': Array(5439.468, dtype=float32), 'eval/episode_reward': Array(9327.643, dtype=float32), 'eval/episode_reward_alive': Array(4431.562, dtype=float32), 'eval/episode_reward_linvel': Array(5439.468, dtype=float32), 'eval/episode_reward_quadctrl': Array(-543.384, dtype=float32), 'eval/episode_x_position': Array(49101.746, dtype=float32), 'eval/episode_x_velocity': Array(4351.587, dtype=float32), 'eval/episode_y_position': Array(4400.117, dtype=float32), 'eval/episode_y_velocity': Array(399.883, dtype=float32), 'eval/episode_distance_from_origin_std': Array(18582.18, dtype=float32), 'eval/episode_forward_reward_std': Array(1500.174, dtype=float32), 'eval/episode_reward_std': Array(2512.565, dtype=float32), 'eval/episode_reward_alive_std': Array(1156.287, dtype=float32), 'eval/episode_reward_linvel_std': Array(1500.174, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(143.715, dtype=float32), 'eval/episode_x_position_std': Array(18502.373, dtype=float32), 'eval/episode_x_velocity_std': Array(1200.144, dtype=float32), 'eval/episode_y_position_std': Array(2143.69, dtype=float32), 'eval/episode_y_velocity_std': Array(145.264, dtype=float32), 'eval/avg_episode_length': Array(886.312, dtype=float32), 'eval/epoch_eval_time': 10.373185873031616, 'eval/sps': 12339.507029636532}
time to jit: 0:00:51.643353
time to train: 0:04:34.194052

--------

Iteration5-Robot3

--------

{'eval/walltime': 48.742340087890625, 'training/sps': 35316.208228726966, 'training/walltime': 74.22767424583435, 'training/entropy_loss': Array(0.002, dtype=float32), 'training/policy_loss': Array(-0.008, dtype=float32), 'training/total_loss': Array(0.334, dtype=float32), 'training/v_loss': Array(0.34, dtype=float32), 'eval/episode_distance_from_origin': Array(3134.193, dtype=float32), 'eval/episode_forward_reward': Array(530.249, dtype=float32), 'eval/episode_reward': Array(1983.963, dtype=float32), 'eval/episode_reward_alive': Array(1611.016, dtype=float32), 'eval/episode_reward_linvel': Array(530.249, dtype=float32), 'eval/episode_reward_quadctrl': Array(-157.301, dtype=float32), 'eval/episode_x_position': Array(2366.575, dtype=float32), 'eval/episode_x_velocity': Array(424.2, dtype=float32), 'eval/episode_y_position': Array(1959.781, dtype=float32), 'eval/episode_y_velocity': Array(372.569, dtype=float32), 'eval/episode_distance_from_origin_std': Array(4389.359, dtype=float32), 'eval/episode_forward_reward_std': Array(373.278, dtype=float32), 'eval/episode_reward_std': Array(1297.656, dtype=float32), 'eval/episode_reward_alive_std': Array(1027.234, dtype=float32), 'eval/episode_reward_linvel_std': Array(373.278, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(101.248, dtype=float32), 'eval/episode_x_position_std': Array(3312.624, dtype=float32), 'eval/episode_x_velocity_std': Array(298.623, dtype=float32), 'eval/episode_y_position_std': Array(2878.994, dtype=float32), 'eval/episode_y_velocity_std': Array(286.994, dtype=float32), 'eval/avg_episode_length': Array(322.203, dtype=float32), 'eval/epoch_eval_time': 10.348755836486816, 'eval/sps': 12368.63658032281}
time to jit: 0:00:51.527175
time to train: 0:01:24.655771
6

--------

Iteration6-Robot0

--------

{'eval/walltime': 46.70526933670044, 'training/sps': 35206.128800473016, 'training/walltime': 74.45976281166077, 'training/entropy_loss': Array(0.004, dtype=float32), 'training/policy_loss': Array(-0.008, dtype=float32), 'training/total_loss': Array(0.544, dtype=float32), 'training/v_loss': Array(0.548, dtype=float32), 'eval/episode_distance_from_origin': Array(225.724, dtype=float32), 'eval/episode_forward_reward': Array(145.794, dtype=float32), 'eval/episode_reward': Array(575.793, dtype=float32), 'eval/episode_reward_alive': Array(475.898, dtype=float32), 'eval/episode_reward_linvel': Array(145.794, dtype=float32), 'eval/episode_reward_quadctrl': Array(-45.9, dtype=float32), 'eval/episode_x_position': Array(170.166, dtype=float32), 'eval/episode_x_velocity': Array(116.636, dtype=float32), 'eval/episode_y_position': Array(70.262, dtype=float32), 'eval/episode_y_velocity': Array(54.882, dtype=float32), 'eval/episode_distance_from_origin_std': Array(293.991, dtype=float32), 'eval/episode_forward_reward_std': Array(136.749, dtype=float32), 'eval/episode_reward_std': Array(377.541, dtype=float32), 'eval/episode_reward_alive_std': Array(269.727, dtype=float32), 'eval/episode_reward_linvel_std': Array(136.749, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(27.959, dtype=float32), 'eval/episode_x_position_std': Array(265.114, dtype=float32), 'eval/episode_x_velocity_std': Array(109.4, dtype=float32), 'eval/episode_y_position_std': Array(119.743, dtype=float32), 'eval/episode_y_velocity_std': Array(62.581, dtype=float32), 'eval/avg_episode_length': Array(95.18, dtype=float32), 'eval/epoch_eval_time': 10.351764678955078, 'eval/sps': 12365.041514150851}
{'eval/walltime': 57.06137037277222, 'training/sps': 49319.59694181901, 'training/walltime': 127.6118597984314, 'training/entropy_loss': Array(0.005, dtype=float32), 'training/policy_loss': Array(-0.005, dtype=float32), 'training/total_loss': Array(0.535, dtype=float32), 'training/v_loss': Array(0.534, dtype=float32), 'eval/episode_distance_from_origin': Array(8170.239, dtype=float32), 'eval/episode_forward_reward': Array(1130.906, dtype=float32), 'eval/episode_reward': Array(3045.987, dtype=float32), 'eval/episode_reward_alive': Array(2125.078, dtype=float32), 'eval/episode_reward_linvel': Array(1130.906, dtype=float32), 'eval/episode_reward_quadctrl': Array(-209.996, dtype=float32), 'eval/episode_x_position': Array(7275.248, dtype=float32), 'eval/episode_x_velocity': Array(904.727, dtype=float32), 'eval/episode_y_position': Array(3501.75, dtype=float32), 'eval/episode_y_velocity': Array(459.517, dtype=float32), 'eval/episode_distance_from_origin_std': Array(10502.767, dtype=float32), 'eval/episode_forward_reward_std': Array(941.8, dtype=float32), 'eval/episode_reward_std': Array(2328.855, dtype=float32), 'eval/episode_reward_alive_std': Array(1540.515, dtype=float32), 'eval/episode_reward_linvel_std': Array(941.8, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(152.567, dtype=float32), 'eval/episode_x_position_std': Array(9514.3, dtype=float32), 'eval/episode_x_velocity_std': Array(753.442, dtype=float32), 'eval/episode_y_position_std': Array(4477.765, dtype=float32), 'eval/episode_y_velocity_std': Array(388.006, dtype=float32), 'eval/avg_episode_length': Array(425.016, dtype=float32), 'eval/epoch_eval_time': 10.356101036071777, 'eval/sps': 12359.86396368264}
{'eval/walltime': 67.42615485191345, 'training/sps': 49088.55582612089, 'training/walltime': 181.01412343978882, 'training/entropy_loss': Array(0.006, dtype=float32), 'training/policy_loss': Array(-0.005, dtype=float32), 'training/total_loss': Array(0.164, dtype=float32), 'training/v_loss': Array(0.163, dtype=float32), 'eval/episode_distance_from_origin': Array(32426.09, dtype=float32), 'eval/episode_forward_reward': Array(3212.249, dtype=float32), 'eval/episode_reward': Array(7335.137, dtype=float32), 'eval/episode_reward_alive': Array(4562.539, dtype=float32), 'eval/episode_reward_linvel': Array(3212.249, dtype=float32), 'eval/episode_reward_quadctrl': Array(-439.649, dtype=float32), 'eval/episode_x_position': Array(29835.395, dtype=float32), 'eval/episode_x_velocity': Array(2569.807, dtype=float32), 'eval/episode_y_position': Array(12409.268, dtype=float32), 'eval/episode_y_velocity': Array(1078.94, dtype=float32), 'eval/episode_distance_from_origin_std': Array(10567.423, dtype=float32), 'eval/episode_forward_reward_std': Array(896.406, dtype=float32), 'eval/episode_reward_std': Array(1972.219, dtype=float32), 'eval/episode_reward_alive_std': Array(1192.247, dtype=float32), 'eval/episode_reward_linvel_std': Array(896.406, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(115.16, dtype=float32), 'eval/episode_x_position_std': Array(9732.519, dtype=float32), 'eval/episode_x_velocity_std': Array(717.128, dtype=float32), 'eval/episode_y_position_std': Array(4461.824, dtype=float32), 'eval/episode_y_velocity_std': Array(326.127, dtype=float32), 'eval/avg_episode_length': Array(912.508, dtype=float32), 'eval/epoch_eval_time': 10.364784479141235, 'eval/sps': 12349.509076391845}
{'eval/walltime': 77.79168343544006, 'training/sps': 49065.74182846268, 'training/walltime': 234.44121742248535, 'training/entropy_loss': Array(0.007, dtype=float32), 'training/policy_loss': Array(-0.005, dtype=float32), 'training/total_loss': Array(0.036, dtype=float32), 'training/v_loss': Array(0.034, dtype=float32), 'eval/episode_distance_from_origin': Array(41989.61, dtype=float32), 'eval/episode_forward_reward': Array(4255.212, dtype=float32), 'eval/episode_reward': Array(8562.764, dtype=float32), 'eval/episode_reward_alive': Array(4778.75, dtype=float32), 'eval/episode_reward_linvel': Array(4255.212, dtype=float32), 'eval/episode_reward_quadctrl': Array(-471.195, dtype=float32), 'eval/episode_x_position': Array(39988.27, dtype=float32), 'eval/episode_x_velocity': Array(3404.182, dtype=float32), 'eval/episode_y_position': Array(12553.977, dtype=float32), 'eval/episode_y_velocity': Array(1080.135, dtype=float32), 'eval/episode_distance_from_origin_std': Array(9370.02, dtype=float32), 'eval/episode_forward_reward_std': Array(892.163, dtype=float32), 'eval/episode_reward_std': Array(1749.804, dtype=float32), 'eval/episode_reward_alive_std': Array(952.948, dtype=float32), 'eval/episode_reward_linvel_std': Array(892.163, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(94.607, dtype=float32), 'eval/episode_x_position_std': Array(8922.797, dtype=float32), 'eval/episode_x_velocity_std': Array(713.733, dtype=float32), 'eval/episode_y_position_std': Array(3109.619, dtype=float32), 'eval/episode_y_velocity_std': Array(249.745, dtype=float32), 'eval/avg_episode_length': Array(955.75, dtype=float32), 'eval/epoch_eval_time': 10.365528583526611, 'eval/sps': 12348.622549111838}
time to jit: 0:00:51.707626
time to train: 0:04:36.159842

--------

Iteration6-Robot1

--------

{'eval/walltime': 48.90118622779846, 'training/sps': 36421.34625062536, 'training/walltime': 71.97537350654602, 'training/entropy_loss': Array(0.006, dtype=float32), 'training/policy_loss': Array(-0.006, dtype=float32), 'training/total_loss': Array(0.158, dtype=float32), 'training/v_loss': Array(0.158, dtype=float32), 'eval/episode_distance_from_origin': Array(53415.086, dtype=float32), 'eval/episode_forward_reward': Array(5581.506, dtype=float32), 'eval/episode_reward': Array(9961.522, dtype=float32), 'eval/episode_reward_alive': Array(4912.07, dtype=float32), 'eval/episode_reward_linvel': Array(5581.506, dtype=float32), 'eval/episode_reward_quadctrl': Array(-532.049, dtype=float32), 'eval/episode_x_position': Array(52404.246, dtype=float32), 'eval/episode_x_velocity': Array(4465.219, dtype=float32), 'eval/episode_y_position': Array(9949.442, dtype=float32), 'eval/episode_y_velocity': Array(791.243, dtype=float32), 'eval/episode_distance_from_origin_std': Array(7476.68, dtype=float32), 'eval/episode_forward_reward_std': Array(538.897, dtype=float32), 'eval/episode_reward_std': Array(931.117, dtype=float32), 'eval/episode_reward_alive_std': Array(440.457, dtype=float32), 'eval/episode_reward_linvel_std': Array(538.897, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(47.893, dtype=float32), 'eval/episode_x_position_std': Array(7376.896, dtype=float32), 'eval/episode_x_velocity_std': Array(431.119, dtype=float32), 'eval/episode_y_position_std': Array(1940.694, dtype=float32), 'eval/episode_y_velocity_std': Array(113.16, dtype=float32), 'eval/avg_episode_length': Array(982.414, dtype=float32), 'eval/epoch_eval_time': 10.36994194984436, 'eval/sps': 12343.367071782028}
{'eval/walltime': 59.279900550842285, 'training/sps': 49182.93329807766, 'training/walltime': 125.27516317367554, 'training/entropy_loss': Array(0.009, dtype=float32), 'training/policy_loss': Array(0.011, dtype=float32), 'training/total_loss': Array(0.067, dtype=float32), 'training/v_loss': Array(0.047, dtype=float32), 'eval/episode_distance_from_origin': Array(61.035, dtype=float32), 'eval/episode_forward_reward': Array(83.966, dtype=float32), 'eval/episode_reward': Array(314.254, dtype=float32), 'eval/episode_reward_alive': Array(254.18, dtype=float32), 'eval/episode_reward_linvel': Array(83.966, dtype=float32), 'eval/episode_reward_quadctrl': Array(-23.892, dtype=float32), 'eval/episode_x_position': Array(33.398, dtype=float32), 'eval/episode_x_velocity': Array(67.173, dtype=float32), 'eval/episode_y_position': Array(2.704, dtype=float32), 'eval/episode_y_velocity': Array(5.715, dtype=float32), 'eval/episode_distance_from_origin_std': Array(31.398, dtype=float32), 'eval/episode_forward_reward_std': Array(46.552, dtype=float32), 'eval/episode_reward_std': Array(109.91, dtype=float32), 'eval/episode_reward_alive_std': Array(71.885, dtype=float32), 'eval/episode_reward_linvel_std': Array(46.552, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(8.235, dtype=float32), 'eval/episode_x_position_std': Array(28.678, dtype=float32), 'eval/episode_x_velocity_std': Array(37.242, dtype=float32), 'eval/episode_y_position_std': Array(6.371, dtype=float32), 'eval/episode_y_velocity_std': Array(12.138, dtype=float32), 'eval/avg_episode_length': Array(50.836, dtype=float32), 'eval/epoch_eval_time': 10.378714323043823, 'eval/sps': 12332.934120346876}
time to jit: 0:00:51.698057
time to train: 0:02:26.182208

--------

Iteration6-Robot2

--------

{'eval/walltime': 46.51337003707886, 'training/sps': 35365.79169683422, 'training/walltime': 74.12360572814941, 'training/entropy_loss': Array(0.003, dtype=float32), 'training/policy_loss': Array(-0.011, dtype=float32), 'training/total_loss': Array(0.558, dtype=float32), 'training/v_loss': Array(0.567, dtype=float32), 'eval/episode_distance_from_origin': Array(807.495, dtype=float32), 'eval/episode_forward_reward': Array(292.935, dtype=float32), 'eval/episode_reward': Array(876.513, dtype=float32), 'eval/episode_reward_alive': Array(645.977, dtype=float32), 'eval/episode_reward_linvel': Array(292.935, dtype=float32), 'eval/episode_reward_quadctrl': Array(-62.399, dtype=float32), 'eval/episode_x_position': Array(713.847, dtype=float32), 'eval/episode_x_velocity': Array(234.348, dtype=float32), 'eval/episode_y_position': Array(276.018, dtype=float32), 'eval/episode_y_velocity': Array(100.422, dtype=float32), 'eval/episode_distance_from_origin_std': Array(1705.059, dtype=float32), 'eval/episode_forward_reward_std': Array(389.335, dtype=float32), 'eval/episode_reward_std': Array(912.813, dtype=float32), 'eval/episode_reward_alive_std': Array(585.337, dtype=float32), 'eval/episode_reward_linvel_std': Array(389.335, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(60.532, dtype=float32), 'eval/episode_x_position_std': Array(1595.654, dtype=float32), 'eval/episode_x_velocity_std': Array(311.468, dtype=float32), 'eval/episode_y_position_std': Array(590.652, dtype=float32), 'eval/episode_y_velocity_std': Array(126.757, dtype=float32), 'eval/avg_episode_length': Array(129.195, dtype=float32), 'eval/epoch_eval_time': 10.366679191589355, 'eval/sps': 12347.251963179138}
{'eval/walltime': 56.87920165061951, 'training/sps': 49248.824196241934, 'training/walltime': 127.35208463668823, 'training/entropy_loss': Array(0.005, dtype=float32), 'training/policy_loss': Array(-0.005, dtype=float32), 'training/total_loss': Array(0.525, dtype=float32), 'training/v_loss': Array(0.525, dtype=float32), 'eval/episode_distance_from_origin': Array(26058.19, dtype=float32), 'eval/episode_forward_reward': Array(2900.353, dtype=float32), 'eval/episode_reward': Array(6231.613, dtype=float32), 'eval/episode_reward_alive': Array(3698.867, dtype=float32), 'eval/episode_reward_linvel': Array(2900.353, dtype=float32), 'eval/episode_reward_quadctrl': Array(-367.605, dtype=float32), 'eval/episode_x_position': Array(25106.865, dtype=float32), 'eval/episode_x_velocity': Array(2320.29, dtype=float32), 'eval/episode_y_position': Array(6612.815, dtype=float32), 'eval/episode_y_velocity': Array(593.522, dtype=float32), 'eval/episode_distance_from_origin_std': Array(16566.47, dtype=float32), 'eval/episode_forward_reward_std': Array(1499.215, dtype=float32), 'eval/episode_reward_std': Array(3087.976, dtype=float32), 'eval/episode_reward_alive_std': Array(1765.874, dtype=float32), 'eval/episode_reward_linvel_std': Array(1499.215, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(176.542, dtype=float32), 'eval/episode_x_position_std': Array(16011.828, dtype=float32), 'eval/episode_x_velocity_std': Array(1199.376, dtype=float32), 'eval/episode_y_position_std': Array(4419.817, dtype=float32), 'eval/episode_y_velocity_std': Array(320.472, dtype=float32), 'eval/avg_episode_length': Array(739.773, dtype=float32), 'eval/epoch_eval_time': 10.36583161354065, 'eval/sps': 12348.261555088018}
{'eval/walltime': 67.25192523002625, 'training/sps': 49109.71692297231, 'training/walltime': 180.73133754730225, 'training/entropy_loss': Array(0.007, dtype=float32), 'training/policy_loss': Array(-0.004, dtype=float32), 'training/total_loss': Array(0.084, dtype=float32), 'training/v_loss': Array(0.081, dtype=float32), 'eval/episode_distance_from_origin': Array(40576.34, dtype=float32), 'eval/episode_forward_reward': Array(4309.076, dtype=float32), 'eval/episode_reward': Array(8382.486, dtype=float32), 'eval/episode_reward_alive': Array(4533.633, dtype=float32), 'eval/episode_reward_linvel': Array(4309.076, dtype=float32), 'eval/episode_reward_quadctrl': Array(-460.22, dtype=float32), 'eval/episode_x_position': Array(39837.766, dtype=float32), 'eval/episode_x_velocity': Array(3447.272, dtype=float32), 'eval/episode_y_position': Array(7327.996, dtype=float32), 'eval/episode_y_velocity': Array(609.242, dtype=float32), 'eval/episode_distance_from_origin_std': Array(13596.135, dtype=float32), 'eval/episode_forward_reward_std': Array(1315.931, dtype=float32), 'eval/episode_reward_std': Array(2487.958, dtype=float32), 'eval/episode_reward_alive_std': Array(1306.335, dtype=float32), 'eval/episode_reward_linvel_std': Array(1315.931, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(133.582, dtype=float32), 'eval/episode_x_position_std': Array(13363.462, dtype=float32), 'eval/episode_x_velocity_std': Array(1052.749, dtype=float32), 'eval/episode_y_position_std': Array(2739.782, dtype=float32), 'eval/episode_y_velocity_std': Array(207.69, dtype=float32), 'eval/avg_episode_length': Array(906.727, dtype=float32), 'eval/epoch_eval_time': 10.372723579406738, 'eval/sps': 12340.056979260686}
{'eval/walltime': 77.63523435592651, 'training/sps': 49013.32900493082, 'training/walltime': 234.21556425094604, 'training/entropy_loss': Array(0.009, dtype=float32), 'training/policy_loss': Array(0.002, dtype=float32), 'training/total_loss': Array(0.069, dtype=float32), 'training/v_loss': Array(0.059, dtype=float32), 'eval/episode_distance_from_origin': Array(43924.945, dtype=float32), 'eval/episode_forward_reward': Array(4902.216, dtype=float32), 'eval/episode_reward': Array(8784.252, dtype=float32), 'eval/episode_reward_alive': Array(4377.539, dtype=float32), 'eval/episode_reward_linvel': Array(4902.216, dtype=float32), 'eval/episode_reward_quadctrl': Array(-495.499, dtype=float32), 'eval/episode_x_position': Array(43366.742, dtype=float32), 'eval/episode_x_velocity': Array(3921.784, dtype=float32), 'eval/episode_y_position': Array(6587.692, dtype=float32), 'eval/episode_y_velocity': Array(590.439, dtype=float32), 'eval/episode_distance_from_origin_std': Array(17725.69, dtype=float32), 'eval/episode_forward_reward_std': Array(1462.422, dtype=float32), 'eval/episode_reward_std': Array(2534.404, dtype=float32), 'eval/episode_reward_alive_std': Array(1211.957, dtype=float32), 'eval/episode_reward_linvel_std': Array(1462.422, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(139.659, dtype=float32), 'eval/episode_x_position_std': Array(17532.11, dtype=float32), 'eval/episode_x_velocity_std': Array(1169.942, dtype=float32), 'eval/episode_y_position_std': Array(2795.836, dtype=float32), 'eval/episode_y_velocity_std': Array(181.377, dtype=float32), 'eval/avg_episode_length': Array(875.508, dtype=float32), 'eval/epoch_eval_time': 10.383309125900269, 'eval/sps': 12327.47657302382}
time to jit: 0:00:52.047776
time to train: 0:04:35.957330

--------

Iteration6-Robot3

--------

{'eval/walltime': 49.930649757385254, 'training/sps': 36559.24490752228, 'training/walltime': 71.70388793945312, 'training/entropy_loss': Array(0.007, dtype=float32), 'training/policy_loss': Array(-0.003, dtype=float32), 'training/total_loss': Array(0.346, dtype=float32), 'training/v_loss': Array(0.343, dtype=float32), 'eval/episode_distance_from_origin': Array(49946.4, dtype=float32), 'eval/episode_forward_reward': Array(5394.81, dtype=float32), 'eval/episode_reward': Array(9455.395, dtype=float32), 'eval/episode_reward_alive': Array(4572.969, dtype=float32), 'eval/episode_reward_linvel': Array(5394.81, dtype=float32), 'eval/episode_reward_quadctrl': Array(-512.38, dtype=float32), 'eval/episode_x_position': Array(49330.207, dtype=float32), 'eval/episode_x_velocity': Array(4315.861, dtype=float32), 'eval/episode_y_position': Array(7418.022, dtype=float32), 'eval/episode_y_velocity': Array(618.698, dtype=float32), 'eval/episode_distance_from_origin_std': Array(16516.791, dtype=float32), 'eval/episode_forward_reward_std': Array(1309.374, dtype=float32), 'eval/episode_reward_std': Array(2235.74, dtype=float32), 'eval/episode_reward_alive_std': Array(1044.567, dtype=float32), 'eval/episode_reward_linvel_std': Array(1309.374, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(117.93, dtype=float32), 'eval/episode_x_position_std': Array(16338.631, dtype=float32), 'eval/episode_x_velocity_std': Array(1047.503, dtype=float32), 'eval/episode_y_position_std': Array(2669.515, dtype=float32), 'eval/episode_y_velocity_std': Array(160.324, dtype=float32), 'eval/avg_episode_length': Array(914.594, dtype=float32), 'eval/epoch_eval_time': 10.376018762588501, 'eval/sps': 12336.138063041426}
{'eval/walltime': 60.303049087524414, 'training/sps': 49104.857295089845, 'training/walltime': 125.08842349052429, 'training/entropy_loss': Array(0.008, dtype=float32), 'training/policy_loss': Array(0.01, dtype=float32), 'training/total_loss': Array(0.414, dtype=float32), 'training/v_loss': Array(0.396, dtype=float32), 'eval/episode_distance_from_origin': Array(122.209, dtype=float32), 'eval/episode_forward_reward': Array(150.939, dtype=float32), 'eval/episode_reward': Array(481.135, dtype=float32), 'eval/episode_reward_alive': Array(364.062, dtype=float32), 'eval/episode_reward_linvel': Array(150.939, dtype=float32), 'eval/episode_reward_quadctrl': Array(-33.866, dtype=float32), 'eval/episode_x_position': Array(88.748, dtype=float32), 'eval/episode_x_velocity': Array(120.751, dtype=float32), 'eval/episode_y_position': Array(19.7, dtype=float32), 'eval/episode_y_velocity': Array(25.92, dtype=float32), 'eval/episode_distance_from_origin_std': Array(73.849, dtype=float32), 'eval/episode_forward_reward_std': Array(69.294, dtype=float32), 'eval/episode_reward_std': Array(155.52, dtype=float32), 'eval/episode_reward_alive_std': Array(96.967, dtype=float32), 'eval/episode_reward_linvel_std': Array(69.294, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(9.951, dtype=float32), 'eval/episode_x_position_std': Array(70.193, dtype=float32), 'eval/episode_x_velocity_std': Array(55.435, dtype=float32), 'eval/episode_y_position_std': Array(21.198, dtype=float32), 'eval/episode_y_velocity_std': Array(22.215, dtype=float32), 'eval/avg_episode_length': Array(72.812, dtype=float32), 'eval/epoch_eval_time': 10.37239933013916, 'eval/sps': 12340.442739036225}
time to jit: 0:00:53.009403
time to train: 0:02:26.015133
7

--------

Iteration7-Robot0

--------

{'eval/walltime': 49.30427360534668, 'training/sps': 36714.07292485097, 'training/walltime': 71.40150332450867, 'training/entropy_loss': Array(0.005, dtype=float32), 'training/policy_loss': Array(-0.006, dtype=float32), 'training/total_loss': Array(0.496, dtype=float32), 'training/v_loss': Array(0.497, dtype=float32), 'eval/episode_distance_from_origin': Array(26285.41, dtype=float32), 'eval/episode_forward_reward': Array(2753.297, dtype=float32), 'eval/episode_reward': Array(6322.933, dtype=float32), 'eval/episode_reward_alive': Array(3949.258, dtype=float32), 'eval/episode_reward_linvel': Array(2753.297, dtype=float32), 'eval/episode_reward_quadctrl': Array(-379.62, dtype=float32), 'eval/episode_x_position': Array(23809.57, dtype=float32), 'eval/episode_x_velocity': Array(2202.644, dtype=float32), 'eval/episode_y_position': Array(10873.79, dtype=float32), 'eval/episode_y_velocity': Array(1009.713, dtype=float32), 'eval/episode_distance_from_origin_std': Array(14114.006, dtype=float32), 'eval/episode_forward_reward_std': Array(1101.745, dtype=float32), 'eval/episode_reward_std': Array(2436.552, dtype=float32), 'eval/episode_reward_alive_std': Array(1476.564, dtype=float32), 'eval/episode_reward_linvel_std': Array(1101.745, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(140.751, dtype=float32), 'eval/episode_x_position_std': Array(12835.606, dtype=float32), 'eval/episode_x_velocity_std': Array(881.399, dtype=float32), 'eval/episode_y_position_std': Array(6075.679, dtype=float32), 'eval/episode_y_velocity_std': Array(427.768, dtype=float32), 'eval/avg_episode_length': Array(789.852, dtype=float32), 'eval/epoch_eval_time': 10.345032930374146, 'eval/sps': 12373.087728331733}
{'eval/walltime': 59.67926907539368, 'training/sps': 49253.55129808389, 'training/walltime': 124.62487363815308, 'training/entropy_loss': Array(0.006, dtype=float32), 'training/policy_loss': Array(-0.007, dtype=float32), 'training/total_loss': Array(0.042, dtype=float32), 'training/v_loss': Array(0.044, dtype=float32), 'eval/episode_distance_from_origin': Array(42130.3, dtype=float32), 'eval/episode_forward_reward': Array(4256.495, dtype=float32), 'eval/episode_reward': Array(8699.825, dtype=float32), 'eval/episode_reward_alive': Array(4904.219, dtype=float32), 'eval/episode_reward_linvel': Array(4256.495, dtype=float32), 'eval/episode_reward_quadctrl': Array(-460.885, dtype=float32), 'eval/episode_x_position': Array(40120.625, dtype=float32), 'eval/episode_x_velocity': Array(3405.208, dtype=float32), 'eval/episode_y_position': Array(12557.569, dtype=float32), 'eval/episode_y_velocity': Array(1032.714, dtype=float32), 'eval/episode_distance_from_origin_std': Array(6450.598, dtype=float32), 'eval/episode_forward_reward_std': Array(488.701, dtype=float32), 'eval/episode_reward_std': Array(966.369, dtype=float32), 'eval/episode_reward_alive_std': Array(528.783, dtype=float32), 'eval/episode_reward_linvel_std': Array(488.701, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(49.978, dtype=float32), 'eval/episode_x_position_std': Array(6125.529, dtype=float32), 'eval/episode_x_velocity_std': Array(390.962, dtype=float32), 'eval/episode_y_position_std': Array(2606.548, dtype=float32), 'eval/episode_y_velocity_std': Array(169.401, dtype=float32), 'eval/avg_episode_length': Array(980.844, dtype=float32), 'eval/epoch_eval_time': 10.374995470046997, 'eval/sps': 12337.354784350588}
{'eval/walltime': 70.04322052001953, 'training/sps': 49144.82111702248, 'training/walltime': 177.96599769592285, 'training/entropy_loss': Array(0.007, dtype=float32), 'training/policy_loss': Array(-0.005, dtype=float32), 'training/total_loss': Array(0.019, dtype=float32), 'training/v_loss': Array(0.017, dtype=float32), 'eval/episode_distance_from_origin': Array(50139.586, dtype=float32), 'eval/episode_forward_reward': Array(5199.554, dtype=float32), 'eval/episode_reward': Array(9572.951, dtype=float32), 'eval/episode_reward_alive': Array(4881.055, dtype=float32), 'eval/episode_reward_linvel': Array(5199.554, dtype=float32), 'eval/episode_reward_quadctrl': Array(-507.653, dtype=float32), 'eval/episode_x_position': Array(48909.402, dtype=float32), 'eval/episode_x_velocity': Array(4159.657, dtype=float32), 'eval/episode_y_position': Array(10663.532, dtype=float32), 'eval/episode_y_velocity': Array(872.273, dtype=float32), 'eval/episode_distance_from_origin_std': Array(8101.092, dtype=float32), 'eval/episode_forward_reward_std': Array(741.857, dtype=float32), 'eval/episode_reward_std': Array(1335.38, dtype=float32), 'eval/episode_reward_alive_std': Array(663.682, dtype=float32), 'eval/episode_reward_linvel_std': Array(741.857, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(69.452, dtype=float32), 'eval/episode_x_position_std': Array(7914.022, dtype=float32), 'eval/episode_x_velocity_std': Array(593.487, dtype=float32), 'eval/episode_y_position_std': Array(2383.392, dtype=float32), 'eval/episode_y_velocity_std': Array(159.291, dtype=float32), 'eval/avg_episode_length': Array(976.211, dtype=float32), 'eval/epoch_eval_time': 10.363951444625854, 'eval/sps': 12350.50170621683}
{'eval/walltime': 80.40523052215576, 'training/sps': 49046.238630922206, 'training/walltime': 231.41433691978455, 'training/entropy_loss': Array(0.01, dtype=float32), 'training/policy_loss': Array(0.001, dtype=float32), 'training/total_loss': Array(0.032, dtype=float32), 'training/v_loss': Array(0.021, dtype=float32), 'eval/episode_distance_from_origin': Array(55486.004, dtype=float32), 'eval/episode_forward_reward': Array(5794.84, dtype=float32), 'eval/episode_reward': Array(10120.053, dtype=float32), 'eval/episode_reward_alive': Array(4886.641, dtype=float32), 'eval/episode_reward_linvel': Array(5794.84, dtype=float32), 'eval/episode_reward_quadctrl': Array(-561.424, dtype=float32), 'eval/episode_x_position': Array(54104.203, dtype=float32), 'eval/episode_x_velocity': Array(4635.887, dtype=float32), 'eval/episode_y_position': Array(11963.134, dtype=float32), 'eval/episode_y_velocity': Array(1008.303, dtype=float32), 'eval/episode_distance_from_origin_std': Array(8881.637, dtype=float32), 'eval/episode_forward_reward_std': Array(643.376, dtype=float32), 'eval/episode_reward_std': Array(1089.721, dtype=float32), 'eval/episode_reward_alive_std': Array(504.6, dtype=float32), 'eval/episode_reward_linvel_std': Array(643.376, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(57.802, dtype=float32), 'eval/episode_x_position_std': Array(8707.557, dtype=float32), 'eval/episode_x_velocity_std': Array(514.703, dtype=float32), 'eval/episode_y_position_std': Array(2437.134, dtype=float32), 'eval/episode_y_velocity_std': Array(157.723, dtype=float32), 'eval/avg_episode_length': Array(977.328, dtype=float32), 'eval/epoch_eval_time': 10.36201000213623, 'eval/sps': 12352.815715639295}
time to jit: 0:00:52.207849
time to train: 0:04:33.125165

--------

Iteration7-Robot1

--------

{'eval/walltime': 49.07759881019592, 'training/sps': 36717.65445789237, 'training/walltime': 71.39453864097595, 'training/entropy_loss': Array(0.008, dtype=float32), 'training/policy_loss': Array(0.001, dtype=float32), 'training/total_loss': Array(0.202, dtype=float32), 'training/v_loss': Array(0.193, dtype=float32), 'eval/episode_distance_from_origin': Array(49738.094, dtype=float32), 'eval/episode_forward_reward': Array(5386.51, dtype=float32), 'eval/episode_reward': Array(9260.465, dtype=float32), 'eval/episode_reward_alive': Array(4402.109, dtype=float32), 'eval/episode_reward_linvel': Array(5386.51, dtype=float32), 'eval/episode_reward_quadctrl': Array(-528.15, dtype=float32), 'eval/episode_x_position': Array(48976.098, dtype=float32), 'eval/episode_x_velocity': Array(4309.221, dtype=float32), 'eval/episode_y_position': Array(8281.886, dtype=float32), 'eval/episode_y_velocity': Array(749.743, dtype=float32), 'eval/episode_distance_from_origin_std': Array(19109.379, dtype=float32), 'eval/episode_forward_reward_std': Array(1626.159, dtype=float32), 'eval/episode_reward_std': Array(2735.891, dtype=float32), 'eval/episode_reward_alive_std': Array(1263.273, dtype=float32), 'eval/episode_reward_linvel_std': Array(1626.159, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(153.333, dtype=float32), 'eval/episode_x_position_std': Array(18817.13, dtype=float32), 'eval/episode_x_velocity_std': Array(1300.931, dtype=float32), 'eval/episode_y_position_std': Array(3619.809, dtype=float32), 'eval/episode_y_velocity_std': Array(254.91, dtype=float32), 'eval/avg_episode_length': Array(880.422, dtype=float32), 'eval/epoch_eval_time': 10.366744041442871, 'eval/sps': 12347.174724127231}
{'eval/walltime': 59.43463492393494, 'training/sps': 49175.89530249086, 'training/walltime': 124.70195651054382, 'training/entropy_loss': Array(0.004, dtype=float32), 'training/policy_loss': Array(-0.007, dtype=float32), 'training/total_loss': Array(0.392, dtype=float32), 'training/v_loss': Array(0.395, dtype=float32), 'eval/episode_distance_from_origin': Array(437.194, dtype=float32), 'eval/episode_forward_reward': Array(396.115, dtype=float32), 'eval/episode_reward': Array(940.893, dtype=float32), 'eval/episode_reward_alive': Array(607.93, dtype=float32), 'eval/episode_reward_linvel': Array(396.115, dtype=float32), 'eval/episode_reward_quadctrl': Array(-63.152, dtype=float32), 'eval/episode_x_position': Array(400.924, dtype=float32), 'eval/episode_x_velocity': Array(316.892, dtype=float32), 'eval/episode_y_position': Array(31.269, dtype=float32), 'eval/episode_y_velocity': Array(30.921, dtype=float32), 'eval/episode_distance_from_origin_std': Array(222.993, dtype=float32), 'eval/episode_forward_reward_std': Array(125.83, dtype=float32), 'eval/episode_reward_std': Array(243.602, dtype=float32), 'eval/episode_reward_alive_std': Array(133.549, dtype=float32), 'eval/episode_reward_linvel_std': Array(125.83, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(15.249, dtype=float32), 'eval/episode_x_position_std': Array(221.086, dtype=float32), 'eval/episode_x_velocity_std': Array(100.664, dtype=float32), 'eval/episode_y_position_std': Array(40.315, dtype=float32), 'eval/episode_y_velocity_std': Array(28.636, dtype=float32), 'eval/avg_episode_length': Array(121.586, dtype=float32), 'eval/epoch_eval_time': 10.357036113739014, 'eval/sps': 12358.748062122038}
time to jit: 0:00:51.975180
time to train: 0:02:25.570173

--------

Iteration7-Robot2

--------

{'eval/walltime': 46.262497663497925, 'training/sps': 35355.54610672992, 'training/walltime': 74.14508581161499, 'training/entropy_loss': Array(0.007, dtype=float32), 'training/policy_loss': Array(-0.005, dtype=float32), 'training/total_loss': Array(0.92, dtype=float32), 'training/v_loss': Array(0.917, dtype=float32), 'eval/episode_distance_from_origin': Array(46596.617, dtype=float32), 'eval/episode_forward_reward': Array(5085.104, dtype=float32), 'eval/episode_reward': Array(8915.084, dtype=float32), 'eval/episode_reward_alive': Array(4335.039, dtype=float32), 'eval/episode_reward_linvel': Array(5085.104, dtype=float32), 'eval/episode_reward_quadctrl': Array(-505.055, dtype=float32), 'eval/episode_x_position': Array(46082.39, dtype=float32), 'eval/episode_x_velocity': Array(4068.095, dtype=float32), 'eval/episode_y_position': Array(6341.409, dtype=float32), 'eval/episode_y_velocity': Array(529.023, dtype=float32), 'eval/episode_distance_from_origin_std': Array(19362.014, dtype=float32), 'eval/episode_forward_reward_std': Array(1663.582, dtype=float32), 'eval/episode_reward_std': Array(2846.941, dtype=float32), 'eval/episode_reward_alive_std': Array(1342.09, dtype=float32), 'eval/episode_reward_linvel_std': Array(1663.582, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(158.324, dtype=float32), 'eval/episode_x_position_std': Array(19185.29, dtype=float32), 'eval/episode_x_velocity_std': Array(1330.871, dtype=float32), 'eval/episode_y_position_std': Array(3027.83, dtype=float32), 'eval/episode_y_velocity_std': Array(192.016, dtype=float32), 'eval/avg_episode_length': Array(867.008, dtype=float32), 'eval/epoch_eval_time': 10.35333800315857, 'eval/sps': 12363.16248546605}
{'eval/walltime': 56.60068678855896, 'training/sps': 49183.22964419354, 'training/walltime': 127.44455432891846, 'training/entropy_loss': Array(0.01, dtype=float32), 'training/policy_loss': Array(0.004, dtype=float32), 'training/total_loss': Array(0.124, dtype=float32), 'training/v_loss': Array(0.109, dtype=float32), 'eval/episode_distance_from_origin': Array(56713.008, dtype=float32), 'eval/episode_forward_reward': Array(6067.265, dtype=float32), 'eval/episode_reward': Array(10236.75, dtype=float32), 'eval/episode_reward_alive': Array(4747.383, dtype=float32), 'eval/episode_reward_linvel': Array(6067.265, dtype=float32), 'eval/episode_reward_quadctrl': Array(-577.892, dtype=float32), 'eval/episode_x_position': Array(56340.78, dtype=float32), 'eval/episode_x_velocity': Array(4853.826, dtype=float32), 'eval/episode_y_position': Array(5831.273, dtype=float32), 'eval/episode_y_velocity': Array(525.303, dtype=float32), 'eval/episode_distance_from_origin_std': Array(13309.782, dtype=float32), 'eval/episode_forward_reward_std': Array(1103.934, dtype=float32), 'eval/episode_reward_std': Array(1824.621, dtype=float32), 'eval/episode_reward_alive_std': Array(822.95, dtype=float32), 'eval/episode_reward_linvel_std': Array(1103.934, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(101.987, dtype=float32), 'eval/episode_x_position_std': Array(13221.359, dtype=float32), 'eval/episode_x_velocity_std': Array(883.151, dtype=float32), 'eval/episode_y_position_std': Array(2226.524, dtype=float32), 'eval/episode_y_velocity_std': Array(158.407, dtype=float32), 'eval/avg_episode_length': Array(949.477, dtype=float32), 'eval/epoch_eval_time': 10.338189125061035, 'eval/sps': 12381.278621582995}
Traceback (most recent call last):
  File "/home/name/Desktop/Codes/ScalingOptimization (1)/TheAlgorithm.py", line 16, in <module>
    import mediapy as media
ModuleNotFoundError: No module named 'mediapy'
Traceback (most recent call last):
  File "/home/name/Desktop/Codes/ScalingOptimization (1)/TheAlgorithm.py", line 27, in <module>
    import jax
ModuleNotFoundError: No module named 'jax'
7

--------

Iteration7-Robot0

--------

{'eval/walltime': 59.0545916557312, 'training/sps': 31447.263970129814, 'training/walltime': 83.35987520217896, 'training/entropy_loss': Array(nan, dtype=float32), 'training/policy_loss': Array(nan, dtype=float32), 'training/total_loss': Array(nan, dtype=float32), 'training/v_loss': Array(nan, dtype=float32), 'eval/episode_distance_from_origin': Array(nan, dtype=float32), 'eval/episode_forward_reward': Array(nan, dtype=float32), 'eval/episode_reward': Array(nan, dtype=float32), 'eval/episode_reward_alive': Array(5000., dtype=float32), 'eval/episode_reward_linvel': Array(nan, dtype=float32), 'eval/episode_reward_quadctrl': Array(nan, dtype=float32), 'eval/episode_x_position': Array(nan, dtype=float32), 'eval/episode_x_velocity': Array(nan, dtype=float32), 'eval/episode_y_position': Array(nan, dtype=float32), 'eval/episode_y_velocity': Array(nan, dtype=float32), 'eval/episode_distance_from_origin_std': Array(nan, dtype=float32), 'eval/episode_forward_reward_std': Array(nan, dtype=float32), 'eval/episode_reward_std': Array(nan, dtype=float32), 'eval/episode_reward_alive_std': Array(0., dtype=float32), 'eval/episode_reward_linvel_std': Array(nan, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(nan, dtype=float32), 'eval/episode_x_position_std': Array(nan, dtype=float32), 'eval/episode_x_velocity_std': Array(nan, dtype=float32), 'eval/episode_y_position_std': Array(nan, dtype=float32), 'eval/episode_y_velocity_std': Array(nan, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/epoch_eval_time': 10.34010100364685, 'eval/sps': 12378.989330457765}
{'eval/walltime': 69.52414655685425, 'training/sps': 50293.93959387106, 'training/walltime': 135.48225855827332, 'training/entropy_loss': Array(nan, dtype=float32), 'training/policy_loss': Array(nan, dtype=float32), 'training/total_loss': Array(nan, dtype=float32), 'training/v_loss': Array(nan, dtype=float32), 'eval/episode_distance_from_origin': Array(nan, dtype=float32), 'eval/episode_forward_reward': Array(nan, dtype=float32), 'eval/episode_reward': Array(nan, dtype=float32), 'eval/episode_reward_alive': Array(5000., dtype=float32), 'eval/episode_reward_linvel': Array(nan, dtype=float32), 'eval/episode_reward_quadctrl': Array(nan, dtype=float32), 'eval/episode_x_position': Array(nan, dtype=float32), 'eval/episode_x_velocity': Array(nan, dtype=float32), 'eval/episode_y_position': Array(nan, dtype=float32), 'eval/episode_y_velocity': Array(nan, dtype=float32), 'eval/episode_distance_from_origin_std': Array(nan, dtype=float32), 'eval/episode_forward_reward_std': Array(nan, dtype=float32), 'eval/episode_reward_std': Array(nan, dtype=float32), 'eval/episode_reward_alive_std': Array(0., dtype=float32), 'eval/episode_reward_linvel_std': Array(nan, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(nan, dtype=float32), 'eval/episode_x_position_std': Array(nan, dtype=float32), 'eval/episode_x_velocity_std': Array(nan, dtype=float32), 'eval/episode_y_position_std': Array(nan, dtype=float32), 'eval/episode_y_velocity_std': Array(nan, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/epoch_eval_time': 10.469554901123047, 'eval/sps': 12225.925668174272}
{'eval/walltime': 79.88805961608887, 'training/sps': 49989.02902568748, 'training/walltime': 187.92256498336792, 'training/entropy_loss': Array(nan, dtype=float32), 'training/policy_loss': Array(nan, dtype=float32), 'training/total_loss': Array(nan, dtype=float32), 'training/v_loss': Array(nan, dtype=float32), 'eval/episode_distance_from_origin': Array(nan, dtype=float32), 'eval/episode_forward_reward': Array(nan, dtype=float32), 'eval/episode_reward': Array(nan, dtype=float32), 'eval/episode_reward_alive': Array(5000., dtype=float32), 'eval/episode_reward_linvel': Array(nan, dtype=float32), 'eval/episode_reward_quadctrl': Array(nan, dtype=float32), 'eval/episode_x_position': Array(nan, dtype=float32), 'eval/episode_x_velocity': Array(nan, dtype=float32), 'eval/episode_y_position': Array(nan, dtype=float32), 'eval/episode_y_velocity': Array(nan, dtype=float32), 'eval/episode_distance_from_origin_std': Array(nan, dtype=float32), 'eval/episode_forward_reward_std': Array(nan, dtype=float32), 'eval/episode_reward_std': Array(nan, dtype=float32), 'eval/episode_reward_alive_std': Array(0., dtype=float32), 'eval/episode_reward_linvel_std': Array(nan, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(nan, dtype=float32), 'eval/episode_x_position_std': Array(nan, dtype=float32), 'eval/episode_x_velocity_std': Array(nan, dtype=float32), 'eval/episode_y_position_std': Array(nan, dtype=float32), 'eval/episode_y_velocity_std': Array(nan, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/epoch_eval_time': 10.36391305923462, 'eval/sps': 12350.547449445014}
{'eval/walltime': 90.25540161132812, 'training/sps': 49768.53790766073, 'training/walltime': 240.59519934654236, 'training/entropy_loss': Array(nan, dtype=float32), 'training/policy_loss': Array(nan, dtype=float32), 'training/total_loss': Array(nan, dtype=float32), 'training/v_loss': Array(nan, dtype=float32), 'eval/episode_distance_from_origin': Array(nan, dtype=float32), 'eval/episode_forward_reward': Array(nan, dtype=float32), 'eval/episode_reward': Array(nan, dtype=float32), 'eval/episode_reward_alive': Array(5000., dtype=float32), 'eval/episode_reward_linvel': Array(nan, dtype=float32), 'eval/episode_reward_quadctrl': Array(nan, dtype=float32), 'eval/episode_x_position': Array(nan, dtype=float32), 'eval/episode_x_velocity': Array(nan, dtype=float32), 'eval/episode_y_position': Array(nan, dtype=float32), 'eval/episode_y_velocity': Array(nan, dtype=float32), 'eval/episode_distance_from_origin_std': Array(nan, dtype=float32), 'eval/episode_forward_reward_std': Array(nan, dtype=float32), 'eval/episode_reward_std': Array(nan, dtype=float32), 'eval/episode_reward_alive_std': Array(0., dtype=float32), 'eval/episode_reward_linvel_std': Array(nan, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(nan, dtype=float32), 'eval/episode_x_position_std': Array(nan, dtype=float32), 'eval/episode_x_velocity_std': Array(nan, dtype=float32), 'eval/episode_y_position_std': Array(nan, dtype=float32), 'eval/episode_y_velocity_std': Array(nan, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/epoch_eval_time': 10.367341995239258, 'eval/sps': 12346.462580165517}

--------

Iteration7-Robot1

--------

{'eval/walltime': 59.680816650390625, 'training/sps': 34029.09340583124, 'training/walltime': 77.03525829315186, 'training/entropy_loss': Array(nan, dtype=float32), 'training/policy_loss': Array(nan, dtype=float32), 'training/total_loss': Array(nan, dtype=float32), 'training/v_loss': Array(nan, dtype=float32), 'eval/episode_distance_from_origin': Array(nan, dtype=float32), 'eval/episode_forward_reward': Array(nan, dtype=float32), 'eval/episode_reward': Array(nan, dtype=float32), 'eval/episode_reward_alive': Array(5000., dtype=float32), 'eval/episode_reward_linvel': Array(nan, dtype=float32), 'eval/episode_reward_quadctrl': Array(nan, dtype=float32), 'eval/episode_x_position': Array(nan, dtype=float32), 'eval/episode_x_velocity': Array(nan, dtype=float32), 'eval/episode_y_position': Array(nan, dtype=float32), 'eval/episode_y_velocity': Array(nan, dtype=float32), 'eval/episode_distance_from_origin_std': Array(nan, dtype=float32), 'eval/episode_forward_reward_std': Array(nan, dtype=float32), 'eval/episode_reward_std': Array(nan, dtype=float32), 'eval/episode_reward_alive_std': Array(0., dtype=float32), 'eval/episode_reward_linvel_std': Array(nan, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(nan, dtype=float32), 'eval/episode_x_position_std': Array(nan, dtype=float32), 'eval/episode_x_velocity_std': Array(nan, dtype=float32), 'eval/episode_y_position_std': Array(nan, dtype=float32), 'eval/episode_y_velocity_std': Array(nan, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/epoch_eval_time': 10.31834888458252, 'eval/sps': 12405.08548719991}
{'eval/walltime': 70.04053092002869, 'training/sps': 49927.95546126545, 'training/walltime': 129.53971147537231, 'training/entropy_loss': Array(nan, dtype=float32), 'training/policy_loss': Array(nan, dtype=float32), 'training/total_loss': Array(nan, dtype=float32), 'training/v_loss': Array(nan, dtype=float32), 'eval/episode_distance_from_origin': Array(nan, dtype=float32), 'eval/episode_forward_reward': Array(nan, dtype=float32), 'eval/episode_reward': Array(nan, dtype=float32), 'eval/episode_reward_alive': Array(5000., dtype=float32), 'eval/episode_reward_linvel': Array(nan, dtype=float32), 'eval/episode_reward_quadctrl': Array(nan, dtype=float32), 'eval/episode_x_position': Array(nan, dtype=float32), 'eval/episode_x_velocity': Array(nan, dtype=float32), 'eval/episode_y_position': Array(nan, dtype=float32), 'eval/episode_y_velocity': Array(nan, dtype=float32), 'eval/episode_distance_from_origin_std': Array(nan, dtype=float32), 'eval/episode_forward_reward_std': Array(nan, dtype=float32), 'eval/episode_reward_std': Array(nan, dtype=float32), 'eval/episode_reward_alive_std': Array(0., dtype=float32), 'eval/episode_reward_linvel_std': Array(nan, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(nan, dtype=float32), 'eval/episode_x_position_std': Array(nan, dtype=float32), 'eval/episode_x_velocity_std': Array(nan, dtype=float32), 'eval/episode_y_position_std': Array(nan, dtype=float32), 'eval/episode_y_velocity_std': Array(nan, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/epoch_eval_time': 10.359714269638062, 'eval/sps': 12355.553123230295}
{'eval/walltime': 80.35159754753113, 'training/sps': 49834.46894859769, 'training/walltime': 182.14265990257263, 'training/entropy_loss': Array(nan, dtype=float32), 'training/policy_loss': Array(nan, dtype=float32), 'training/total_loss': Array(nan, dtype=float32), 'training/v_loss': Array(nan, dtype=float32), 'eval/episode_distance_from_origin': Array(nan, dtype=float32), 'eval/episode_forward_reward': Array(nan, dtype=float32), 'eval/episode_reward': Array(nan, dtype=float32), 'eval/episode_reward_alive': Array(5000., dtype=float32), 'eval/episode_reward_linvel': Array(nan, dtype=float32), 'eval/episode_reward_quadctrl': Array(nan, dtype=float32), 'eval/episode_x_position': Array(nan, dtype=float32), 'eval/episode_x_velocity': Array(nan, dtype=float32), 'eval/episode_y_position': Array(nan, dtype=float32), 'eval/episode_y_velocity': Array(nan, dtype=float32), 'eval/episode_distance_from_origin_std': Array(nan, dtype=float32), 'eval/episode_forward_reward_std': Array(nan, dtype=float32), 'eval/episode_reward_std': Array(nan, dtype=float32), 'eval/episode_reward_alive_std': Array(0., dtype=float32), 'eval/episode_reward_linvel_std': Array(nan, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(nan, dtype=float32), 'eval/episode_x_position_std': Array(nan, dtype=float32), 'eval/episode_x_velocity_std': Array(nan, dtype=float32), 'eval/episode_y_position_std': Array(nan, dtype=float32), 'eval/episode_y_velocity_std': Array(nan, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/epoch_eval_time': 10.311066627502441, 'eval/sps': 12413.846658558961}
{'eval/walltime': 90.72969460487366, 'training/sps': 49691.129297857035, 'training/walltime': 234.89734745025635, 'training/entropy_loss': Array(nan, dtype=float32), 'training/policy_loss': Array(nan, dtype=float32), 'training/total_loss': Array(nan, dtype=float32), 'training/v_loss': Array(nan, dtype=float32), 'eval/episode_distance_from_origin': Array(nan, dtype=float32), 'eval/episode_forward_reward': Array(nan, dtype=float32), 'eval/episode_reward': Array(nan, dtype=float32), 'eval/episode_reward_alive': Array(5000., dtype=float32), 'eval/episode_reward_linvel': Array(nan, dtype=float32), 'eval/episode_reward_quadctrl': Array(nan, dtype=float32), 'eval/episode_x_position': Array(nan, dtype=float32), 'eval/episode_x_velocity': Array(nan, dtype=float32), 'eval/episode_y_position': Array(nan, dtype=float32), 'eval/episode_y_velocity': Array(nan, dtype=float32), 'eval/episode_distance_from_origin_std': Array(nan, dtype=float32), 'eval/episode_forward_reward_std': Array(nan, dtype=float32), 'eval/episode_reward_std': Array(nan, dtype=float32), 'eval/episode_reward_alive_std': Array(0., dtype=float32), 'eval/episode_reward_linvel_std': Array(nan, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(nan, dtype=float32), 'eval/episode_x_position_std': Array(nan, dtype=float32), 'eval/episode_x_velocity_std': Array(nan, dtype=float32), 'eval/episode_y_position_std': Array(nan, dtype=float32), 'eval/episode_y_velocity_std': Array(nan, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/epoch_eval_time': 10.37809705734253, 'eval/sps': 12333.667655327976}

--------

Iteration7-Robot2

--------

{'eval/walltime': 54.72046422958374, 'training/sps': 33671.456237781385, 'training/walltime': 77.85347867012024, 'training/entropy_loss': Array(-0.013, dtype=float32), 'training/policy_loss': Array(0.002, dtype=float32), 'training/total_loss': Array(0.154, dtype=float32), 'training/v_loss': Array(0.165, dtype=float32), 'eval/episode_distance_from_origin': Array(31.845, dtype=float32), 'eval/episode_forward_reward': Array(3.826, dtype=float32), 'eval/episode_reward': Array(171.337, dtype=float32), 'eval/episode_reward_alive': Array(188.516, dtype=float32), 'eval/episode_reward_linvel': Array(3.826, dtype=float32), 'eval/episode_reward_quadctrl': Array(-21.004, dtype=float32), 'eval/episode_x_position': Array(1.541, dtype=float32), 'eval/episode_x_velocity': Array(3.06, dtype=float32), 'eval/episode_y_position': Array(0.209, dtype=float32), 'eval/episode_y_velocity': Array(-0.007, dtype=float32), 'eval/episode_distance_from_origin_std': Array(6.894, dtype=float32), 'eval/episode_forward_reward_std': Array(6.979, dtype=float32), 'eval/episode_reward_std': Array(37.619, dtype=float32), 'eval/episode_reward_alive_std': Array(42.936, dtype=float32), 'eval/episode_reward_linvel_std': Array(6.979, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(4.963, dtype=float32), 'eval/episode_x_position_std': Array(1.626, dtype=float32), 'eval/episode_x_velocity_std': Array(5.583, dtype=float32), 'eval/episode_y_position_std': Array(2.106, dtype=float32), 'eval/episode_y_velocity_std': Array(6.999, dtype=float32), 'eval/avg_episode_length': Array(37.703, dtype=float32), 'eval/epoch_eval_time': 10.333394527435303, 'eval/sps': 12387.023418118632}
{'eval/walltime': 65.03026247024536, 'training/sps': 49058.99475046644, 'training/walltime': 131.2879204750061, 'training/entropy_loss': Array(-0.011, dtype=float32), 'training/policy_loss': Array(-0.016, dtype=float32), 'training/total_loss': Array(0.063, dtype=float32), 'training/v_loss': Array(0.09, dtype=float32), 'eval/episode_distance_from_origin': Array(43.591, dtype=float32), 'eval/episode_forward_reward': Array(7.391, dtype=float32), 'eval/episode_reward': Array(240.006, dtype=float32), 'eval/episode_reward_alive': Array(259.805, dtype=float32), 'eval/episode_reward_linvel': Array(7.391, dtype=float32), 'eval/episode_reward_quadctrl': Array(-27.19, dtype=float32), 'eval/episode_x_position': Array(3.574, dtype=float32), 'eval/episode_x_velocity': Array(5.913, dtype=float32), 'eval/episode_y_position': Array(1.221, dtype=float32), 'eval/episode_y_velocity': Array(2.724, dtype=float32), 'eval/episode_distance_from_origin_std': Array(10.083, dtype=float32), 'eval/episode_forward_reward_std': Array(10.596, dtype=float32), 'eval/episode_reward_std': Array(56.525, dtype=float32), 'eval/episode_reward_alive_std': Array(61.782, dtype=float32), 'eval/episode_reward_linvel_std': Array(10.596, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(6.602, dtype=float32), 'eval/episode_x_position_std': Array(3.334, dtype=float32), 'eval/episode_x_velocity_std': Array(8.477, dtype=float32), 'eval/episode_y_position_std': Array(3.881, dtype=float32), 'eval/episode_y_velocity_std': Array(7.598, dtype=float32), 'eval/avg_episode_length': Array(51.961, dtype=float32), 'eval/epoch_eval_time': 10.309798240661621, 'eval/sps': 12415.373900836466}
{'eval/walltime': 75.43023657798767, 'training/sps': 48741.60536801161, 'training/walltime': 185.07030987739563, 'training/entropy_loss': Array(-0.01, dtype=float32), 'training/policy_loss': Array(-0.012, dtype=float32), 'training/total_loss': Array(0.052, dtype=float32), 'training/v_loss': Array(0.074, dtype=float32), 'eval/episode_distance_from_origin': Array(53.907, dtype=float32), 'eval/episode_forward_reward': Array(13.81, dtype=float32), 'eval/episode_reward': Array(302.132, dtype=float32), 'eval/episode_reward_alive': Array(319.727, dtype=float32), 'eval/episode_reward_linvel': Array(13.81, dtype=float32), 'eval/episode_reward_quadctrl': Array(-31.405, dtype=float32), 'eval/episode_x_position': Array(7.555, dtype=float32), 'eval/episode_x_velocity': Array(11.048, dtype=float32), 'eval/episode_y_position': Array(1.408, dtype=float32), 'eval/episode_y_velocity': Array(0.066, dtype=float32), 'eval/episode_distance_from_origin_std': Array(11.902, dtype=float32), 'eval/episode_forward_reward_std': Array(12.984, dtype=float32), 'eval/episode_reward_std': Array(66.748, dtype=float32), 'eval/episode_reward_alive_std': Array(72.039, dtype=float32), 'eval/episode_reward_linvel_std': Array(12.984, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(7.123, dtype=float32), 'eval/episode_x_position_std': Array(4.787, dtype=float32), 'eval/episode_x_velocity_std': Array(10.387, dtype=float32), 'eval/episode_y_position_std': Array(4.431, dtype=float32), 'eval/episode_y_velocity_std': Array(8.405, dtype=float32), 'eval/avg_episode_length': Array(63.945, dtype=float32), 'eval/epoch_eval_time': 10.39997410774231, 'eval/sps': 12307.722949493673}
{'eval/walltime': 85.81226897239685, 'training/sps': 48670.26550927122, 'training/walltime': 238.9315323829651, 'training/entropy_loss': Array(-0.009, dtype=float32), 'training/policy_loss': Array(-0.012, dtype=float32), 'training/total_loss': Array(0.046, dtype=float32), 'training/v_loss': Array(0.067, dtype=float32), 'eval/episode_distance_from_origin': Array(73.32, dtype=float32), 'eval/episode_forward_reward': Array(25.083, dtype=float32), 'eval/episode_reward': Array(401.263, dtype=float32), 'eval/episode_reward_alive': Array(415.469, dtype=float32), 'eval/episode_reward_linvel': Array(25.083, dtype=float32), 'eval/episode_reward_quadctrl': Array(-39.289, dtype=float32), 'eval/episode_x_position': Array(19.374, dtype=float32), 'eval/episode_x_velocity': Array(20.066, dtype=float32), 'eval/episode_y_position': Array(3.187, dtype=float32), 'eval/episode_y_velocity': Array(0.473, dtype=float32), 'eval/episode_distance_from_origin_std': Array(24.181, dtype=float32), 'eval/episode_forward_reward_std': Array(18.405, dtype=float32), 'eval/episode_reward_std': Array(111.329, dtype=float32), 'eval/episode_reward_alive_std': Array(114.545, dtype=float32), 'eval/episode_reward_linvel_std': Array(18.405, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(10.747, dtype=float32), 'eval/episode_x_position_std': Array(16.925, dtype=float32), 'eval/episode_x_velocity_std': Array(14.724, dtype=float32), 'eval/episode_y_position_std': Array(7.945, dtype=float32), 'eval/episode_y_velocity_std': Array(11.233, dtype=float32), 'eval/avg_episode_length': Array(83.094, dtype=float32), 'eval/epoch_eval_time': 10.38203239440918, 'eval/sps': 12328.992545710913}
time to jit: 0:01:00.231132
time to train: 0:04:40.608240

--------

Iteration7-Robot3

--------

{'eval/walltime': 54.73782777786255, 'training/sps': 33529.24188822471, 'training/walltime': 78.18369436264038, 'training/entropy_loss': Array(-0.013, dtype=float32), 'training/policy_loss': Array(0., dtype=float32), 'training/total_loss': Array(0.176, dtype=float32), 'training/v_loss': Array(0.189, dtype=float32), 'eval/episode_distance_from_origin': Array(31.286, dtype=float32), 'eval/episode_forward_reward': Array(3.274, dtype=float32), 'eval/episode_reward': Array(164.972, dtype=float32), 'eval/episode_reward_alive': Array(181.914, dtype=float32), 'eval/episode_reward_linvel': Array(3.274, dtype=float32), 'eval/episode_reward_quadctrl': Array(-20.216, dtype=float32), 'eval/episode_x_position': Array(1.29, dtype=float32), 'eval/episode_x_velocity': Array(2.619, dtype=float32), 'eval/episode_y_position': Array(0.178, dtype=float32), 'eval/episode_y_velocity': Array(-0.379, dtype=float32), 'eval/episode_distance_from_origin_std': Array(6.99, dtype=float32), 'eval/episode_forward_reward_std': Array(7.576, dtype=float32), 'eval/episode_reward_std': Array(37.305, dtype=float32), 'eval/episode_reward_alive_std': Array(42.775, dtype=float32), 'eval/episode_reward_linvel_std': Array(7.576, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(5.083, dtype=float32), 'eval/episode_x_position_std': Array(1.706, dtype=float32), 'eval/episode_x_velocity_std': Array(6.061, dtype=float32), 'eval/episode_y_position_std': Array(2.05, dtype=float32), 'eval/episode_y_velocity_std': Array(6.414, dtype=float32), 'eval/avg_episode_length': Array(36.383, dtype=float32), 'eval/epoch_eval_time': 10.37616753578186, 'eval/sps': 12335.961187846704}
{'eval/walltime': 65.13090538978577, 'training/sps': 48905.774895001254, 'training/walltime': 131.7855441570282, 'training/entropy_loss': Array(-0.011, dtype=float32), 'training/policy_loss': Array(-0.016, dtype=float32), 'training/total_loss': Array(0.064, dtype=float32), 'training/v_loss': Array(0.091, dtype=float32), 'eval/episode_distance_from_origin': Array(44.75, dtype=float32), 'eval/episode_forward_reward': Array(8.38, dtype=float32), 'eval/episode_reward': Array(242.305, dtype=float32), 'eval/episode_reward_alive': Array(261.25, dtype=float32), 'eval/episode_reward_linvel': Array(8.38, dtype=float32), 'eval/episode_reward_quadctrl': Array(-27.325, dtype=float32), 'eval/episode_x_position': Array(4.128, dtype=float32), 'eval/episode_x_velocity': Array(6.704, dtype=float32), 'eval/episode_y_position': Array(1.512, dtype=float32), 'eval/episode_y_velocity': Array(2.856, dtype=float32), 'eval/episode_distance_from_origin_std': Array(10.01, dtype=float32), 'eval/episode_forward_reward_std': Array(11.916, dtype=float32), 'eval/episode_reward_std': Array(56.069, dtype=float32), 'eval/episode_reward_alive_std': Array(59.772, dtype=float32), 'eval/episode_reward_linvel_std': Array(11.916, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(6.35, dtype=float32), 'eval/episode_x_position_std': Array(3.836, dtype=float32), 'eval/episode_x_velocity_std': Array(9.533, dtype=float32), 'eval/episode_y_position_std': Array(3.793, dtype=float32), 'eval/episode_y_velocity_std': Array(7.561, dtype=float32), 'eval/avg_episode_length': Array(52.25, dtype=float32), 'eval/epoch_eval_time': 10.393077611923218, 'eval/sps': 12315.889939391482}
{'eval/walltime': 75.5258424282074, 'training/sps': 48686.57314834278, 'training/walltime': 185.62872576713562, 'training/entropy_loss': Array(-0.01, dtype=float32), 'training/policy_loss': Array(-0.012, dtype=float32), 'training/total_loss': Array(0.052, dtype=float32), 'training/v_loss': Array(0.073, dtype=float32), 'eval/episode_distance_from_origin': Array(52.074, dtype=float32), 'eval/episode_forward_reward': Array(14.191, dtype=float32), 'eval/episode_reward': Array(287.851, dtype=float32), 'eval/episode_reward_alive': Array(303.359, dtype=float32), 'eval/episode_reward_linvel': Array(14.191, dtype=float32), 'eval/episode_reward_quadctrl': Array(-29.699, dtype=float32), 'eval/episode_x_position': Array(7.228, dtype=float32), 'eval/episode_x_velocity': Array(11.353, dtype=float32), 'eval/episode_y_position': Array(1.864, dtype=float32), 'eval/episode_y_velocity': Array(1.765, dtype=float32), 'eval/episode_distance_from_origin_std': Array(9.515, dtype=float32), 'eval/episode_forward_reward_std': Array(12.667, dtype=float32), 'eval/episode_reward_std': Array(50.952, dtype=float32), 'eval/episode_reward_alive_std': Array(55.822, dtype=float32), 'eval/episode_reward_linvel_std': Array(12.667, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(5.922, dtype=float32), 'eval/episode_x_position_std': Array(3.962, dtype=float32), 'eval/episode_x_velocity_std': Array(10.133, dtype=float32), 'eval/episode_y_position_std': Array(3.932, dtype=float32), 'eval/episode_y_velocity_std': Array(7.64, dtype=float32), 'eval/avg_episode_length': Array(60.672, dtype=float32), 'eval/epoch_eval_time': 10.39493703842163, 'eval/sps': 12313.686896504334}
{'eval/walltime': 85.93459868431091, 'training/sps': 48611.07525058114, 'training/walltime': 239.55553126335144, 'training/entropy_loss': Array(-0.009, dtype=float32), 'training/policy_loss': Array(-0.012, dtype=float32), 'training/total_loss': Array(0.041, dtype=float32), 'training/v_loss': Array(0.062, dtype=float32), 'eval/episode_distance_from_origin': Array(73.417, dtype=float32), 'eval/episode_forward_reward': Array(16.099, dtype=float32), 'eval/episode_reward': Array(396.143, dtype=float32), 'eval/episode_reward_alive': Array(419.141, dtype=float32), 'eval/episode_reward_linvel': Array(16.099, dtype=float32), 'eval/episode_reward_quadctrl': Array(-39.097, dtype=float32), 'eval/episode_x_position': Array(13.904, dtype=float32), 'eval/episode_x_velocity': Array(12.879, dtype=float32), 'eval/episode_y_position': Array(5.409, dtype=float32), 'eval/episode_y_velocity': Array(4.505, dtype=float32), 'eval/episode_distance_from_origin_std': Array(25.766, dtype=float32), 'eval/episode_forward_reward_std': Array(18.62, dtype=float32), 'eval/episode_reward_std': Array(130.049, dtype=float32), 'eval/episode_reward_alive_std': Array(138.358, dtype=float32), 'eval/episode_reward_linvel_std': Array(18.62, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(12.859, dtype=float32), 'eval/episode_x_position_std': Array(13.553, dtype=float32), 'eval/episode_x_velocity_std': Array(14.896, dtype=float32), 'eval/episode_y_position_std': Array(7.54, dtype=float32), 'eval/episode_y_velocity_std': Array(10.322, dtype=float32), 'eval/avg_episode_length': Array(83.828, dtype=float32), 'eval/epoch_eval_time': 10.408756256103516, 'eval/sps': 12297.338591721082}
time to jit: 0:01:01.180765
time to train: 0:04:41.452573
8

--------

Iteration8-Robot0

--------

{'eval/walltime': 52.92470145225525, 'training/sps': 33716.56682884363, 'training/walltime': 77.74931573867798, 'training/entropy_loss': Array(nan, dtype=float32), 'training/policy_loss': Array(nan, dtype=float32), 'training/total_loss': Array(nan, dtype=float32), 'training/v_loss': Array(nan, dtype=float32), 'eval/episode_distance_from_origin': Array(nan, dtype=float32), 'eval/episode_forward_reward': Array(nan, dtype=float32), 'eval/episode_reward': Array(nan, dtype=float32), 'eval/episode_reward_alive': Array(5000., dtype=float32), 'eval/episode_reward_linvel': Array(nan, dtype=float32), 'eval/episode_reward_quadctrl': Array(nan, dtype=float32), 'eval/episode_x_position': Array(nan, dtype=float32), 'eval/episode_x_velocity': Array(nan, dtype=float32), 'eval/episode_y_position': Array(nan, dtype=float32), 'eval/episode_y_velocity': Array(nan, dtype=float32), 'eval/episode_distance_from_origin_std': Array(nan, dtype=float32), 'eval/episode_forward_reward_std': Array(nan, dtype=float32), 'eval/episode_reward_std': Array(nan, dtype=float32), 'eval/episode_reward_alive_std': Array(0., dtype=float32), 'eval/episode_reward_linvel_std': Array(nan, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(nan, dtype=float32), 'eval/episode_x_position_std': Array(nan, dtype=float32), 'eval/episode_x_velocity_std': Array(nan, dtype=float32), 'eval/episode_y_position_std': Array(nan, dtype=float32), 'eval/episode_y_velocity_std': Array(nan, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/epoch_eval_time': 10.39751935005188, 'eval/sps': 12310.628688501678}
{'eval/walltime': 63.34998416900635, 'training/sps': 50031.49779496981, 'training/walltime': 130.14510869979858, 'training/entropy_loss': Array(nan, dtype=float32), 'training/policy_loss': Array(nan, dtype=float32), 'training/total_loss': Array(nan, dtype=float32), 'training/v_loss': Array(nan, dtype=float32), 'eval/episode_distance_from_origin': Array(nan, dtype=float32), 'eval/episode_forward_reward': Array(nan, dtype=float32), 'eval/episode_reward': Array(nan, dtype=float32), 'eval/episode_reward_alive': Array(5000., dtype=float32), 'eval/episode_reward_linvel': Array(nan, dtype=float32), 'eval/episode_reward_quadctrl': Array(nan, dtype=float32), 'eval/episode_x_position': Array(nan, dtype=float32), 'eval/episode_x_velocity': Array(nan, dtype=float32), 'eval/episode_y_position': Array(nan, dtype=float32), 'eval/episode_y_velocity': Array(nan, dtype=float32), 'eval/episode_distance_from_origin_std': Array(nan, dtype=float32), 'eval/episode_forward_reward_std': Array(nan, dtype=float32), 'eval/episode_reward_std': Array(nan, dtype=float32), 'eval/episode_reward_alive_std': Array(0., dtype=float32), 'eval/episode_reward_linvel_std': Array(nan, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(nan, dtype=float32), 'eval/episode_x_position_std': Array(nan, dtype=float32), 'eval/episode_x_velocity_std': Array(nan, dtype=float32), 'eval/episode_y_position_std': Array(nan, dtype=float32), 'eval/episode_y_velocity_std': Array(nan, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/epoch_eval_time': 10.425282716751099, 'eval/sps': 12277.84449378362}
7

--------

Iteration7-Robot0

--------

{'eval/walltime': 52.794753313064575, 'training/sps': 33589.46599344333, 'training/walltime': 78.0435152053833, 'training/entropy_loss': Array(-0.013, dtype=float32), 'training/policy_loss': Array(0.007, dtype=float32), 'training/total_loss': Array(47416.07, dtype=float32), 'training/v_loss': Array(47416.07, dtype=float32), 'eval/episode_distance_from_origin': Array(26.365, dtype=float32), 'eval/episode_forward_reward': Array(4.677, dtype=float32), 'eval/episode_reward': Array(137.949, dtype=float32), 'eval/episode_reward_alive': Array(150.156, dtype=float32), 'eval/episode_reward_linvel': Array(4.677, dtype=float32), 'eval/episode_reward_quadctrl': Array(-16.884, dtype=float32), 'eval/episode_x_position': Array(1.458, dtype=float32), 'eval/episode_x_velocity': Array(3.742, dtype=float32), 'eval/episode_y_position': Array(-0.02, dtype=float32), 'eval/episode_y_velocity': Array(-0.575, dtype=float32), 'eval/episode_distance_from_origin_std': Array(5.92, dtype=float32), 'eval/episode_forward_reward_std': Array(5.238, dtype=float32), 'eval/episode_reward_std': Array(33.782, dtype=float32), 'eval/episode_reward_alive_std': Array(36.518, dtype=float32), 'eval/episode_reward_linvel_std': Array(5.238, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(4.294, dtype=float32), 'eval/episode_x_position_std': Array(1.242, dtype=float32), 'eval/episode_x_velocity_std': Array(4.191, dtype=float32), 'eval/episode_y_position_std': Array(1.303, dtype=float32), 'eval/episode_y_velocity_std': Array(5.162, dtype=float32), 'eval/avg_episode_length': Array(30.031, dtype=float32), 'eval/epoch_eval_time': 10.419021606445312, 'eval/sps': 12285.22262789223}
{'eval/walltime': 63.21776008605957, 'training/sps': 49047.73186352939, 'training/walltime': 131.49022722244263, 'training/entropy_loss': Array(-0.012, dtype=float32), 'training/policy_loss': Array(-0.019, dtype=float32), 'training/total_loss': Array(0.119, dtype=float32), 'training/v_loss': Array(0.15, dtype=float32), 'eval/episode_distance_from_origin': Array(39.9, dtype=float32), 'eval/episode_forward_reward': Array(8.772, dtype=float32), 'eval/episode_reward': Array(213.648, dtype=float32), 'eval/episode_reward_alive': Array(229.258, dtype=float32), 'eval/episode_reward_linvel': Array(8.772, dtype=float32), 'eval/episode_reward_quadctrl': Array(-24.382, dtype=float32), 'eval/episode_x_position': Array(3.525, dtype=float32), 'eval/episode_x_velocity': Array(7.018, dtype=float32), 'eval/episode_y_position': Array(0.562, dtype=float32), 'eval/episode_y_velocity': Array(0.636, dtype=float32), 'eval/episode_distance_from_origin_std': Array(9.595, dtype=float32), 'eval/episode_forward_reward_std': Array(9.759, dtype=float32), 'eval/episode_reward_std': Array(50.928, dtype=float32), 'eval/episode_reward_alive_std': Array(56.583, dtype=float32), 'eval/episode_reward_linvel_std': Array(9.759, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(6.237, dtype=float32), 'eval/episode_x_position_std': Array(2.885, dtype=float32), 'eval/episode_x_velocity_std': Array(7.807, dtype=float32), 'eval/episode_y_position_std': Array(3.294, dtype=float32), 'eval/episode_y_velocity_std': Array(7.703, dtype=float32), 'eval/avg_episode_length': Array(45.852, dtype=float32), 'eval/epoch_eval_time': 10.423006772994995, 'eval/sps': 12280.52545563298}
{'eval/walltime': 73.66135168075562, 'training/sps': 48764.4698785166, 'training/walltime': 185.24739933013916, 'training/entropy_loss': Array(-0.01, dtype=float32), 'training/policy_loss': Array(-0.013, dtype=float32), 'training/total_loss': Array(0.078, dtype=float32), 'training/v_loss': Array(0.101, dtype=float32), 'eval/episode_distance_from_origin': Array(46.416, dtype=float32), 'eval/episode_forward_reward': Array(10.674, dtype=float32), 'eval/episode_reward': Array(251.432, dtype=float32), 'eval/episode_reward_alive': Array(267.148, dtype=float32), 'eval/episode_reward_linvel': Array(10.674, dtype=float32), 'eval/episode_reward_quadctrl': Array(-26.39, dtype=float32), 'eval/episode_x_position': Array(4.833, dtype=float32), 'eval/episode_x_velocity': Array(8.539, dtype=float32), 'eval/episode_y_position': Array(-0.167, dtype=float32), 'eval/episode_y_velocity': Array(-0.046, dtype=float32), 'eval/episode_distance_from_origin_std': Array(10.463, dtype=float32), 'eval/episode_forward_reward_std': Array(10.828, dtype=float32), 'eval/episode_reward_std': Array(55.887, dtype=float32), 'eval/episode_reward_alive_std': Array(61.354, dtype=float32), 'eval/episode_reward_linvel_std': Array(10.828, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(6.246, dtype=float32), 'eval/episode_x_position_std': Array(3.276, dtype=float32), 'eval/episode_x_velocity_std': Array(8.663, dtype=float32), 'eval/episode_y_position_std': Array(3.804, dtype=float32), 'eval/episode_y_velocity_std': Array(7.735, dtype=float32), 'eval/avg_episode_length': Array(53.43, dtype=float32), 'eval/epoch_eval_time': 10.443591594696045, 'eval/sps': 12256.319948877259}
{'eval/walltime': 84.09424352645874, 'training/sps': 48665.705920157474, 'training/walltime': 239.11366820335388, 'training/entropy_loss': Array(-0.009, dtype=float32), 'training/policy_loss': Array(-0.011, dtype=float32), 'training/total_loss': Array(0.06, dtype=float32), 'training/v_loss': Array(0.081, dtype=float32), 'eval/episode_distance_from_origin': Array(55.861, dtype=float32), 'eval/episode_forward_reward': Array(12.986, dtype=float32), 'eval/episode_reward': Array(304.284, dtype=float32), 'eval/episode_reward_alive': Array(320.977, dtype=float32), 'eval/episode_reward_linvel': Array(12.986, dtype=float32), 'eval/episode_reward_quadctrl': Array(-29.679, dtype=float32), 'eval/episode_x_position': Array(7.299, dtype=float32), 'eval/episode_x_velocity': Array(10.389, dtype=float32), 'eval/episode_y_position': Array(-0.074, dtype=float32), 'eval/episode_y_velocity': Array(-0.313, dtype=float32), 'eval/episode_distance_from_origin_std': Array(13.617, dtype=float32), 'eval/episode_forward_reward_std': Array(14.217, dtype=float32), 'eval/episode_reward_std': Array(73.911, dtype=float32), 'eval/episode_reward_alive_std': Array(78.352, dtype=float32), 'eval/episode_reward_linvel_std': Array(14.217, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(7.329, dtype=float32), 'eval/episode_x_position_std': Array(5.508, dtype=float32), 'eval/episode_x_velocity_std': Array(11.374, dtype=float32), 'eval/episode_y_position_std': Array(3.833, dtype=float32), 'eval/episode_y_velocity_std': Array(7.471, dtype=float32), 'eval/avg_episode_length': Array(64.195, dtype=float32), 'eval/epoch_eval_time': 10.432891845703125, 'eval/sps': 12268.889766428269}
time to jit: 0:00:59.438682
time to train: 0:04:41.464427

--------

Iteration7-Robot1

--------

{'eval/walltime': 54.74758982658386, 'training/sps': 33700.85639992586, 'training/walltime': 77.78556036949158, 'training/entropy_loss': Array(-0.013, dtype=float32), 'training/policy_loss': Array(-0.002, dtype=float32), 'training/total_loss': Array(0.232, dtype=float32), 'training/v_loss': Array(0.247, dtype=float32), 'eval/episode_distance_from_origin': Array(31.28, dtype=float32), 'eval/episode_forward_reward': Array(4.161, dtype=float32), 'eval/episode_reward': Array(162.219, dtype=float32), 'eval/episode_reward_alive': Array(177.891, dtype=float32), 'eval/episode_reward_linvel': Array(4.161, dtype=float32), 'eval/episode_reward_quadctrl': Array(-19.833, dtype=float32), 'eval/episode_x_position': Array(1.598, dtype=float32), 'eval/episode_x_velocity': Array(3.329, dtype=float32), 'eval/episode_y_position': Array(0.034, dtype=float32), 'eval/episode_y_velocity': Array(-0.463, dtype=float32), 'eval/episode_distance_from_origin_std': Array(6.959, dtype=float32), 'eval/episode_forward_reward_std': Array(6.715, dtype=float32), 'eval/episode_reward_std': Array(37.619, dtype=float32), 'eval/episode_reward_alive_std': Array(41.588, dtype=float32), 'eval/episode_reward_linvel_std': Array(6.715, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(4.799, dtype=float32), 'eval/episode_x_position_std': Array(1.69, dtype=float32), 'eval/episode_x_velocity_std': Array(5.372, dtype=float32), 'eval/episode_y_position_std': Array(1.782, dtype=float32), 'eval/episode_y_velocity_std': Array(5.803, dtype=float32), 'eval/avg_episode_length': Array(35.578, dtype=float32), 'eval/epoch_eval_time': 10.400960922241211, 'eval/sps': 12306.555226670192}
{'eval/walltime': 65.1657440662384, 'training/sps': 48952.11690249168, 'training/walltime': 131.3366663455963, 'training/entropy_loss': Array(-0.011, dtype=float32), 'training/policy_loss': Array(-0.017, dtype=float32), 'training/total_loss': Array(0.067, dtype=float32), 'training/v_loss': Array(0.095, dtype=float32), 'eval/episode_distance_from_origin': Array(45.377, dtype=float32), 'eval/episode_forward_reward': Array(6.037, dtype=float32), 'eval/episode_reward': Array(238.434, dtype=float32), 'eval/episode_reward_alive': Array(259.688, dtype=float32), 'eval/episode_reward_linvel': Array(6.037, dtype=float32), 'eval/episode_reward_quadctrl': Array(-27.291, dtype=float32), 'eval/episode_x_position': Array(3.404, dtype=float32), 'eval/episode_x_velocity': Array(4.829, dtype=float32), 'eval/episode_y_position': Array(0.84, dtype=float32), 'eval/episode_y_velocity': Array(1.655, dtype=float32), 'eval/episode_distance_from_origin_std': Array(11.799, dtype=float32), 'eval/episode_forward_reward_std': Array(11.214, dtype=float32), 'eval/episode_reward_std': Array(63.472, dtype=float32), 'eval/episode_reward_alive_std': Array(67.886, dtype=float32), 'eval/episode_reward_linvel_std': Array(11.214, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(7.444, dtype=float32), 'eval/episode_x_position_std': Array(4.014, dtype=float32), 'eval/episode_x_velocity_std': Array(8.971, dtype=float32), 'eval/episode_y_position_std': Array(4.692, dtype=float32), 'eval/episode_y_velocity_std': Array(7.619, dtype=float32), 'eval/avg_episode_length': Array(51.938, dtype=float32), 'eval/epoch_eval_time': 10.418154239654541, 'eval/sps': 12286.24543806374}
{'eval/walltime': 75.58800482749939, 'training/sps': 48710.04679415504, 'training/walltime': 185.15390062332153, 'training/entropy_loss': Array(-0.01, dtype=float32), 'training/policy_loss': Array(-0.012, dtype=float32), 'training/total_loss': Array(0.055, dtype=float32), 'training/v_loss': Array(0.077, dtype=float32), 'eval/episode_distance_from_origin': Array(52.307, dtype=float32), 'eval/episode_forward_reward': Array(11.693, dtype=float32), 'eval/episode_reward': Array(280.585, dtype=float32), 'eval/episode_reward_alive': Array(298.047, dtype=float32), 'eval/episode_reward_linvel': Array(11.693, dtype=float32), 'eval/episode_reward_quadctrl': Array(-29.155, dtype=float32), 'eval/episode_x_position': Array(6.022, dtype=float32), 'eval/episode_x_velocity': Array(9.354, dtype=float32), 'eval/episode_y_position': Array(1.76, dtype=float32), 'eval/episode_y_velocity': Array(1.452, dtype=float32), 'eval/episode_distance_from_origin_std': Array(9.986, dtype=float32), 'eval/episode_forward_reward_std': Array(14.235, dtype=float32), 'eval/episode_reward_std': Array(52.791, dtype=float32), 'eval/episode_reward_alive_std': Array(56.313, dtype=float32), 'eval/episode_reward_linvel_std': Array(14.235, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(5.885, dtype=float32), 'eval/episode_x_position_std': Array(4.462, dtype=float32), 'eval/episode_x_velocity_std': Array(11.388, dtype=float32), 'eval/episode_y_position_std': Array(4.61, dtype=float32), 'eval/episode_y_velocity_std': Array(8.373, dtype=float32), 'eval/avg_episode_length': Array(59.609, dtype=float32), 'eval/epoch_eval_time': 10.422260761260986, 'eval/sps': 12281.404479512688}
{'eval/walltime': 85.99886155128479, 'training/sps': 48649.62686599821, 'training/walltime': 239.03797268867493, 'training/entropy_loss': Array(-0.009, dtype=float32), 'training/policy_loss': Array(-0.012, dtype=float32), 'training/total_loss': Array(0.045, dtype=float32), 'training/v_loss': Array(0.066, dtype=float32), 'eval/episode_distance_from_origin': Array(69.997, dtype=float32), 'eval/episode_forward_reward': Array(16.685, dtype=float32), 'eval/episode_reward': Array(374.018, dtype=float32), 'eval/episode_reward_alive': Array(394.102, dtype=float32), 'eval/episode_reward_linvel': Array(16.685, dtype=float32), 'eval/episode_reward_quadctrl': Array(-36.768, dtype=float32), 'eval/episode_x_position': Array(12.386, dtype=float32), 'eval/episode_x_velocity': Array(13.348, dtype=float32), 'eval/episode_y_position': Array(3.018, dtype=float32), 'eval/episode_y_velocity': Array(1.971, dtype=float32), 'eval/episode_distance_from_origin_std': Array(18.151, dtype=float32), 'eval/episode_forward_reward_std': Array(19.595, dtype=float32), 'eval/episode_reward_std': Array(96.188, dtype=float32), 'eval/episode_reward_alive_std': Array(101.16, dtype=float32), 'eval/episode_reward_linvel_std': Array(19.595, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(9.484, dtype=float32), 'eval/episode_x_position_std': Array(9.434, dtype=float32), 'eval/episode_x_velocity_std': Array(15.676, dtype=float32), 'eval/episode_y_position_std': Array(6.02, dtype=float32), 'eval/episode_y_velocity_std': Array(9.472, dtype=float32), 'eval/avg_episode_length': Array(78.82, dtype=float32), 'eval/epoch_eval_time': 10.4108567237854, 'eval/sps': 12294.857512308463}
time to jit: 0:01:00.439310
time to train: 0:04:41.004750

--------

Iteration7-Robot2

--------

{'eval/walltime': 54.71064043045044, 'training/sps': 33340.924222175156, 'training/walltime': 78.62529492378235, 'training/entropy_loss': Array(-0.013, dtype=float32), 'training/policy_loss': Array(0.002, dtype=float32), 'training/total_loss': Array(0.157, dtype=float32), 'training/v_loss': Array(0.168, dtype=float32), 'eval/episode_distance_from_origin': Array(30.565, dtype=float32), 'eval/episode_forward_reward': Array(3.391, dtype=float32), 'eval/episode_reward': Array(162.364, dtype=float32), 'eval/episode_reward_alive': Array(179.023, dtype=float32), 'eval/episode_reward_linvel': Array(3.391, dtype=float32), 'eval/episode_reward_quadctrl': Array(-20.051, dtype=float32), 'eval/episode_x_position': Array(1.417, dtype=float32), 'eval/episode_x_velocity': Array(2.713, dtype=float32), 'eval/episode_y_position': Array(0.286, dtype=float32), 'eval/episode_y_velocity': Array(0.284, dtype=float32), 'eval/episode_distance_from_origin_std': Array(7.057, dtype=float32), 'eval/episode_forward_reward_std': Array(6.546, dtype=float32), 'eval/episode_reward_std': Array(39.125, dtype=float32), 'eval/episode_reward_alive_std': Array(44.269, dtype=float32), 'eval/episode_reward_linvel_std': Array(6.546, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(5.097, dtype=float32), 'eval/episode_x_position_std': Array(1.567, dtype=float32), 'eval/episode_x_velocity_std': Array(5.236, dtype=float32), 'eval/episode_y_position_std': Array(1.84, dtype=float32), 'eval/episode_y_velocity_std': Array(6.215, dtype=float32), 'eval/avg_episode_length': Array(35.805, dtype=float32), 'eval/epoch_eval_time': 10.414864540100098, 'eval/sps': 12290.126242848837}
{'eval/walltime': 65.15467524528503, 'training/sps': 48831.81077813478, 'training/walltime': 132.30833387374878, 'training/entropy_loss': Array(-0.011, dtype=float32), 'training/policy_loss': Array(-0.016, dtype=float32), 'training/total_loss': Array(0.07, dtype=float32), 'training/v_loss': Array(0.098, dtype=float32), 'eval/episode_distance_from_origin': Array(42.027, dtype=float32), 'eval/episode_forward_reward': Array(7.038, dtype=float32), 'eval/episode_reward': Array(228.896, dtype=float32), 'eval/episode_reward_alive': Array(248.125, dtype=float32), 'eval/episode_reward_linvel': Array(7.038, dtype=float32), 'eval/episode_reward_quadctrl': Array(-26.267, dtype=float32), 'eval/episode_x_position': Array(3.377, dtype=float32), 'eval/episode_x_velocity': Array(5.631, dtype=float32), 'eval/episode_y_position': Array(0.816, dtype=float32), 'eval/episode_y_velocity': Array(1.575, dtype=float32), 'eval/episode_distance_from_origin_std': Array(8.45, dtype=float32), 'eval/episode_forward_reward_std': Array(10.822, dtype=float32), 'eval/episode_reward_std': Array(49.271, dtype=float32), 'eval/episode_reward_alive_std': Array(51.417, dtype=float32), 'eval/episode_reward_linvel_std': Array(10.822, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(5.534, dtype=float32), 'eval/episode_x_position_std': Array(3.22, dtype=float32), 'eval/episode_x_velocity_std': Array(8.657, dtype=float32), 'eval/episode_y_position_std': Array(2.875, dtype=float32), 'eval/episode_y_velocity_std': Array(6.78, dtype=float32), 'eval/avg_episode_length': Array(49.625, dtype=float32), 'eval/epoch_eval_time': 10.444034814834595, 'eval/sps': 12255.799819643475}
{'eval/walltime': 75.5698504447937, 'training/sps': 48694.4089846262, 'training/walltime': 186.14285111427307, 'training/entropy_loss': Array(-0.01, dtype=float32), 'training/policy_loss': Array(-0.012, dtype=float32), 'training/total_loss': Array(0.058, dtype=float32), 'training/v_loss': Array(0.08, dtype=float32), 'eval/episode_distance_from_origin': Array(51.073, dtype=float32), 'eval/episode_forward_reward': Array(10.203, dtype=float32), 'eval/episode_reward': Array(282.025, dtype=float32), 'eval/episode_reward_alive': Array(301.992, dtype=float32), 'eval/episode_reward_linvel': Array(10.203, dtype=float32), 'eval/episode_reward_quadctrl': Array(-30.17, dtype=float32), 'eval/episode_x_position': Array(5.102, dtype=float32), 'eval/episode_x_velocity': Array(8.163, dtype=float32), 'eval/episode_y_position': Array(1.577, dtype=float32), 'eval/episode_y_velocity': Array(2.258, dtype=float32), 'eval/episode_distance_from_origin_std': Array(11.366, dtype=float32), 'eval/episode_forward_reward_std': Array(13.203, dtype=float32), 'eval/episode_reward_std': Array(61.699, dtype=float32), 'eval/episode_reward_alive_std': Array(68.916, dtype=float32), 'eval/episode_reward_linvel_std': Array(13.203, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(7.344, dtype=float32), 'eval/episode_x_position_std': Array(4.143, dtype=float32), 'eval/episode_x_velocity_std': Array(10.562, dtype=float32), 'eval/episode_y_position_std': Array(4.803, dtype=float32), 'eval/episode_y_velocity_std': Array(7.653, dtype=float32), 'eval/avg_episode_length': Array(60.398, dtype=float32), 'eval/epoch_eval_time': 10.415175199508667, 'eval/sps': 12289.759658199351}
{'eval/walltime': 85.95413160324097, 'training/sps': 48651.28591559681, 'training/walltime': 240.02508568763733, 'training/entropy_loss': Array(-0.009, dtype=float32), 'training/policy_loss': Array(-0.011, dtype=float32), 'training/total_loss': Array(0.046, dtype=float32), 'training/v_loss': Array(0.067, dtype=float32), 'eval/episode_distance_from_origin': Array(67.395, dtype=float32), 'eval/episode_forward_reward': Array(16.016, dtype=float32), 'eval/episode_reward': Array(370.148, dtype=float32), 'eval/episode_reward_alive': Array(391.562, dtype=float32), 'eval/episode_reward_linvel': Array(16.016, dtype=float32), 'eval/episode_reward_quadctrl': Array(-37.43, dtype=float32), 'eval/episode_x_position': Array(11.07, dtype=float32), 'eval/episode_x_velocity': Array(12.813, dtype=float32), 'eval/episode_y_position': Array(6.353, dtype=float32), 'eval/episode_y_velocity': Array(5.908, dtype=float32), 'eval/episode_distance_from_origin_std': Array(17.983, dtype=float32), 'eval/episode_forward_reward_std': Array(16.537, dtype=float32), 'eval/episode_reward_std': Array(91.694, dtype=float32), 'eval/episode_reward_alive_std': Array(96.47, dtype=float32), 'eval/episode_reward_linvel_std': Array(16.537, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(9.392, dtype=float32), 'eval/episode_x_position_std': Array(8.318, dtype=float32), 'eval/episode_x_velocity_std': Array(13.23, dtype=float32), 'eval/episode_y_position_std': Array(9.237, dtype=float32), 'eval/episode_y_velocity_std': Array(9.856, dtype=float32), 'eval/avg_episode_length': Array(78.312, dtype=float32), 'eval/epoch_eval_time': 10.384281158447266, 'eval/sps': 12326.322645441498}
time to jit: 0:01:00.229022
time to train: 0:04:41.926524

--------

Iteration7-Robot3

--------

{'eval/walltime': 51.881720781326294, 'training/sps': 34519.38079560382, 'training/walltime': 75.9411072731018, 'training/entropy_loss': Array(-0.013, dtype=float32), 'training/policy_loss': Array(0., dtype=float32), 'training/total_loss': Array(0.144, dtype=float32), 'training/v_loss': Array(0.157, dtype=float32), 'eval/episode_distance_from_origin': Array(30.801, dtype=float32), 'eval/episode_forward_reward': Array(4.22, dtype=float32), 'eval/episode_reward': Array(161.544, dtype=float32), 'eval/episode_reward_alive': Array(176.953, dtype=float32), 'eval/episode_reward_linvel': Array(4.22, dtype=float32), 'eval/episode_reward_quadctrl': Array(-19.629, dtype=float32), 'eval/episode_x_position': Array(1.659, dtype=float32), 'eval/episode_x_velocity': Array(3.376, dtype=float32), 'eval/episode_y_position': Array(0.104, dtype=float32), 'eval/episode_y_velocity': Array(-0.209, dtype=float32), 'eval/episode_distance_from_origin_std': Array(6.215, dtype=float32), 'eval/episode_forward_reward_std': Array(6.135, dtype=float32), 'eval/episode_reward_std': Array(33.471, dtype=float32), 'eval/episode_reward_alive_std': Array(37.761, dtype=float32), 'eval/episode_reward_linvel_std': Array(6.135, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(4.41, dtype=float32), 'eval/episode_x_position_std': Array(1.328, dtype=float32), 'eval/episode_x_velocity_std': Array(4.908, dtype=float32), 'eval/episode_y_position_std': Array(1.634, dtype=float32), 'eval/episode_y_velocity_std': Array(5.651, dtype=float32), 'eval/avg_episode_length': Array(35.391, dtype=float32), 'eval/epoch_eval_time': 10.359466552734375, 'eval/sps': 12355.848570814149}
{'eval/walltime': 62.24657225608826, 'training/sps': 49043.37012912097, 'training/walltime': 129.39257264137268, 'training/entropy_loss': Array(-0.011, dtype=float32), 'training/policy_loss': Array(-0.016, dtype=float32), 'training/total_loss': Array(0.061, dtype=float32), 'training/v_loss': Array(0.089, dtype=float32), 'eval/episode_distance_from_origin': Array(44.219, dtype=float32), 'eval/episode_forward_reward': Array(7.375, dtype=float32), 'eval/episode_reward': Array(235.766, dtype=float32), 'eval/episode_reward_alive': Array(254.805, dtype=float32), 'eval/episode_reward_linvel': Array(7.375, dtype=float32), 'eval/episode_reward_quadctrl': Array(-26.413, dtype=float32), 'eval/episode_x_position': Array(3.855, dtype=float32), 'eval/episode_x_velocity': Array(5.9, dtype=float32), 'eval/episode_y_position': Array(0.951, dtype=float32), 'eval/episode_y_velocity': Array(2.092, dtype=float32), 'eval/episode_distance_from_origin_std': Array(11.61, dtype=float32), 'eval/episode_forward_reward_std': Array(11.852, dtype=float32), 'eval/episode_reward_std': Array(61.909, dtype=float32), 'eval/episode_reward_alive_std': Array(67.718, dtype=float32), 'eval/episode_reward_linvel_std': Array(11.852, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(7.164, dtype=float32), 'eval/episode_x_position_std': Array(3.728, dtype=float32), 'eval/episode_x_velocity_std': Array(9.482, dtype=float32), 'eval/episode_y_position_std': Array(3.852, dtype=float32), 'eval/episode_y_velocity_std': Array(7.586, dtype=float32), 'eval/avg_episode_length': Array(50.961, dtype=float32), 'eval/epoch_eval_time': 10.364851474761963, 'eval/sps': 12349.42925247654}
{'eval/walltime': 72.64287090301514, 'training/sps': 48997.41248163151, 'training/walltime': 182.89417338371277, 'training/entropy_loss': Array(-0.01, dtype=float32), 'training/policy_loss': Array(-0.012, dtype=float32), 'training/total_loss': Array(0.054, dtype=float32), 'training/v_loss': Array(0.076, dtype=float32), 'eval/episode_distance_from_origin': Array(50.914, dtype=float32), 'eval/episode_forward_reward': Array(12.152, dtype=float32), 'eval/episode_reward': Array(276.577, dtype=float32), 'eval/episode_reward_alive': Array(292.734, dtype=float32), 'eval/episode_reward_linvel': Array(12.152, dtype=float32), 'eval/episode_reward_quadctrl': Array(-28.31, dtype=float32), 'eval/episode_x_position': Array(6.228, dtype=float32), 'eval/episode_x_velocity': Array(9.722, dtype=float32), 'eval/episode_y_position': Array(1.647, dtype=float32), 'eval/episode_y_velocity': Array(2.026, dtype=float32), 'eval/episode_distance_from_origin_std': Array(11.581, dtype=float32), 'eval/episode_forward_reward_std': Array(13.544, dtype=float32), 'eval/episode_reward_std': Array(59.382, dtype=float32), 'eval/episode_reward_alive_std': Array(66.985, dtype=float32), 'eval/episode_reward_linvel_std': Array(13.544, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(6.802, dtype=float32), 'eval/episode_x_position_std': Array(4.174, dtype=float32), 'eval/episode_x_velocity_std': Array(10.835, dtype=float32), 'eval/episode_y_position_std': Array(4.784, dtype=float32), 'eval/episode_y_velocity_std': Array(7.931, dtype=float32), 'eval/avg_episode_length': Array(58.547, dtype=float32), 'eval/epoch_eval_time': 10.39629864692688, 'eval/sps': 12312.074166687822}
{'eval/walltime': 83.02844452857971, 'training/sps': 48834.53767679537, 'training/walltime': 236.57421469688416, 'training/entropy_loss': Array(-0.009, dtype=float32), 'training/policy_loss': Array(-0.013, dtype=float32), 'training/total_loss': Array(0.045, dtype=float32), 'training/v_loss': Array(0.067, dtype=float32), 'eval/episode_distance_from_origin': Array(75.609, dtype=float32), 'eval/episode_forward_reward': Array(24.55, dtype=float32), 'eval/episode_reward': Array(405.707, dtype=float32), 'eval/episode_reward_alive': Array(419.922, dtype=float32), 'eval/episode_reward_linvel': Array(24.55, dtype=float32), 'eval/episode_reward_quadctrl': Array(-38.765, dtype=float32), 'eval/episode_x_position': Array(17.255, dtype=float32), 'eval/episode_x_velocity': Array(19.64, dtype=float32), 'eval/episode_y_position': Array(6.33, dtype=float32), 'eval/episode_y_velocity': Array(5.135, dtype=float32), 'eval/episode_distance_from_origin_std': Array(21.115, dtype=float32), 'eval/episode_forward_reward_std': Array(20.009, dtype=float32), 'eval/episode_reward_std': Array(107.117, dtype=float32), 'eval/episode_reward_alive_std': Array(112.18, dtype=float32), 'eval/episode_reward_linvel_std': Array(20.009, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(10.295, dtype=float32), 'eval/episode_x_position_std': Array(10.533, dtype=float32), 'eval/episode_x_velocity_std': Array(16.007, dtype=float32), 'eval/episode_y_position_std': Array(9.078, dtype=float32), 'eval/episode_y_velocity_std': Array(11.428, dtype=float32), 'eval/avg_episode_length': Array(83.984, dtype=float32), 'eval/epoch_eval_time': 10.385573625564575, 'eval/sps': 12324.788655382696}
time to jit: 0:00:56.694862
time to train: 0:04:38.315016
8

--------

Iteration8-Robot0

--------

{'eval/walltime': 49.963966369628906, 'training/sps': 35158.63799096017, 'training/walltime': 74.56033992767334, 'training/entropy_loss': Array(nan, dtype=float32), 'training/policy_loss': Array(nan, dtype=float32), 'training/total_loss': Array(nan, dtype=float32), 'training/v_loss': Array(nan, dtype=float32), 'eval/episode_distance_from_origin': Array(nan, dtype=float32), 'eval/episode_forward_reward': Array(nan, dtype=float32), 'eval/episode_reward': Array(nan, dtype=float32), 'eval/episode_reward_alive': Array(5000., dtype=float32), 'eval/episode_reward_linvel': Array(nan, dtype=float32), 'eval/episode_reward_quadctrl': Array(nan, dtype=float32), 'eval/episode_x_position': Array(nan, dtype=float32), 'eval/episode_x_velocity': Array(nan, dtype=float32), 'eval/episode_y_position': Array(nan, dtype=float32), 'eval/episode_y_velocity': Array(nan, dtype=float32), 'eval/episode_distance_from_origin_std': Array(nan, dtype=float32), 'eval/episode_forward_reward_std': Array(nan, dtype=float32), 'eval/episode_reward_std': Array(nan, dtype=float32), 'eval/episode_reward_alive_std': Array(0., dtype=float32), 'eval/episode_reward_linvel_std': Array(nan, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(nan, dtype=float32), 'eval/episode_x_position_std': Array(nan, dtype=float32), 'eval/episode_x_velocity_std': Array(nan, dtype=float32), 'eval/episode_y_position_std': Array(nan, dtype=float32), 'eval/episode_y_velocity_std': Array(nan, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/epoch_eval_time': 10.3386070728302, 'eval/sps': 12380.77809692403}
{'eval/walltime': 60.29868698120117, 'training/sps': 50197.58339546331, 'training/walltime': 126.7827742099762, 'training/entropy_loss': Array(nan, dtype=float32), 'training/policy_loss': Array(nan, dtype=float32), 'training/total_loss': Array(nan, dtype=float32), 'training/v_loss': Array(nan, dtype=float32), 'eval/episode_distance_from_origin': Array(nan, dtype=float32), 'eval/episode_forward_reward': Array(nan, dtype=float32), 'eval/episode_reward': Array(nan, dtype=float32), 'eval/episode_reward_alive': Array(5000., dtype=float32), 'eval/episode_reward_linvel': Array(nan, dtype=float32), 'eval/episode_reward_quadctrl': Array(nan, dtype=float32), 'eval/episode_x_position': Array(nan, dtype=float32), 'eval/episode_x_velocity': Array(nan, dtype=float32), 'eval/episode_y_position': Array(nan, dtype=float32), 'eval/episode_y_velocity': Array(nan, dtype=float32), 'eval/episode_distance_from_origin_std': Array(nan, dtype=float32), 'eval/episode_forward_reward_std': Array(nan, dtype=float32), 'eval/episode_reward_std': Array(nan, dtype=float32), 'eval/episode_reward_alive_std': Array(0., dtype=float32), 'eval/episode_reward_linvel_std': Array(nan, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(nan, dtype=float32), 'eval/episode_x_position_std': Array(nan, dtype=float32), 'eval/episode_x_velocity_std': Array(nan, dtype=float32), 'eval/episode_y_position_std': Array(nan, dtype=float32), 'eval/episode_y_velocity_std': Array(nan, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/epoch_eval_time': 10.334720611572266, 'eval/sps': 12385.433995832695}
{'eval/walltime': 70.63156771659851, 'training/sps': 50065.151753360515, 'training/walltime': 179.14334654808044, 'training/entropy_loss': Array(nan, dtype=float32), 'training/policy_loss': Array(nan, dtype=float32), 'training/total_loss': Array(nan, dtype=float32), 'training/v_loss': Array(nan, dtype=float32), 'eval/episode_distance_from_origin': Array(nan, dtype=float32), 'eval/episode_forward_reward': Array(nan, dtype=float32), 'eval/episode_reward': Array(nan, dtype=float32), 'eval/episode_reward_alive': Array(5000., dtype=float32), 'eval/episode_reward_linvel': Array(nan, dtype=float32), 'eval/episode_reward_quadctrl': Array(nan, dtype=float32), 'eval/episode_x_position': Array(nan, dtype=float32), 'eval/episode_x_velocity': Array(nan, dtype=float32), 'eval/episode_y_position': Array(nan, dtype=float32), 'eval/episode_y_velocity': Array(nan, dtype=float32), 'eval/episode_distance_from_origin_std': Array(nan, dtype=float32), 'eval/episode_forward_reward_std': Array(nan, dtype=float32), 'eval/episode_reward_std': Array(nan, dtype=float32), 'eval/episode_reward_alive_std': Array(0., dtype=float32), 'eval/episode_reward_linvel_std': Array(nan, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(nan, dtype=float32), 'eval/episode_x_position_std': Array(nan, dtype=float32), 'eval/episode_x_velocity_std': Array(nan, dtype=float32), 'eval/episode_y_position_std': Array(nan, dtype=float32), 'eval/episode_y_velocity_std': Array(nan, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/epoch_eval_time': 10.332880735397339, 'eval/sps': 12387.639350322755}
{'eval/walltime': 80.97688460350037, 'training/sps': 50013.777081741755, 'training/walltime': 231.55770421028137, 'training/entropy_loss': Array(nan, dtype=float32), 'training/policy_loss': Array(nan, dtype=float32), 'training/total_loss': Array(nan, dtype=float32), 'training/v_loss': Array(nan, dtype=float32), 'eval/episode_distance_from_origin': Array(nan, dtype=float32), 'eval/episode_forward_reward': Array(nan, dtype=float32), 'eval/episode_reward': Array(nan, dtype=float32), 'eval/episode_reward_alive': Array(5000., dtype=float32), 'eval/episode_reward_linvel': Array(nan, dtype=float32), 'eval/episode_reward_quadctrl': Array(nan, dtype=float32), 'eval/episode_x_position': Array(nan, dtype=float32), 'eval/episode_x_velocity': Array(nan, dtype=float32), 'eval/episode_y_position': Array(nan, dtype=float32), 'eval/episode_y_velocity': Array(nan, dtype=float32), 'eval/episode_distance_from_origin_std': Array(nan, dtype=float32), 'eval/episode_forward_reward_std': Array(nan, dtype=float32), 'eval/episode_reward_std': Array(nan, dtype=float32), 'eval/episode_reward_alive_std': Array(0., dtype=float32), 'eval/episode_reward_linvel_std': Array(nan, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(nan, dtype=float32), 'eval/episode_x_position_std': Array(nan, dtype=float32), 'eval/episode_x_velocity_std': Array(nan, dtype=float32), 'eval/episode_y_position_std': Array(nan, dtype=float32), 'eval/episode_y_velocity_std': Array(nan, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/epoch_eval_time': 10.345316886901855, 'eval/sps': 12372.748113889102}

--------

Iteration8-Robot1

--------

{'eval/walltime': 50.74863147735596, 'training/sps': 35349.26583746343, 'training/walltime': 74.15825867652893, 'training/entropy_loss': Array(nan, dtype=float32), 'training/policy_loss': Array(nan, dtype=float32), 'training/total_loss': Array(nan, dtype=float32), 'training/v_loss': Array(nan, dtype=float32), 'eval/episode_distance_from_origin': Array(nan, dtype=float32), 'eval/episode_forward_reward': Array(nan, dtype=float32), 'eval/episode_reward': Array(nan, dtype=float32), 'eval/episode_reward_alive': Array(5000., dtype=float32), 'eval/episode_reward_linvel': Array(nan, dtype=float32), 'eval/episode_reward_quadctrl': Array(nan, dtype=float32), 'eval/episode_x_position': Array(nan, dtype=float32), 'eval/episode_x_velocity': Array(nan, dtype=float32), 'eval/episode_y_position': Array(nan, dtype=float32), 'eval/episode_y_velocity': Array(nan, dtype=float32), 'eval/episode_distance_from_origin_std': Array(nan, dtype=float32), 'eval/episode_forward_reward_std': Array(nan, dtype=float32), 'eval/episode_reward_std': Array(nan, dtype=float32), 'eval/episode_reward_alive_std': Array(0., dtype=float32), 'eval/episode_reward_linvel_std': Array(nan, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(nan, dtype=float32), 'eval/episode_x_position_std': Array(nan, dtype=float32), 'eval/episode_x_velocity_std': Array(nan, dtype=float32), 'eval/episode_y_position_std': Array(nan, dtype=float32), 'eval/episode_y_velocity_std': Array(nan, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/epoch_eval_time': 10.338087558746338, 'eval/sps': 12381.400261183519}
{'eval/walltime': 61.093210220336914, 'training/sps': 50182.240746030555, 'training/walltime': 126.39665937423706, 'training/entropy_loss': Array(nan, dtype=float32), 'training/policy_loss': Array(nan, dtype=float32), 'training/total_loss': Array(nan, dtype=float32), 'training/v_loss': Array(nan, dtype=float32), 'eval/episode_distance_from_origin': Array(nan, dtype=float32), 'eval/episode_forward_reward': Array(nan, dtype=float32), 'eval/episode_reward': Array(nan, dtype=float32), 'eval/episode_reward_alive': Array(5000., dtype=float32), 'eval/episode_reward_linvel': Array(nan, dtype=float32), 'eval/episode_reward_quadctrl': Array(nan, dtype=float32), 'eval/episode_x_position': Array(nan, dtype=float32), 'eval/episode_x_velocity': Array(nan, dtype=float32), 'eval/episode_y_position': Array(nan, dtype=float32), 'eval/episode_y_velocity': Array(nan, dtype=float32), 'eval/episode_distance_from_origin_std': Array(nan, dtype=float32), 'eval/episode_forward_reward_std': Array(nan, dtype=float32), 'eval/episode_reward_std': Array(nan, dtype=float32), 'eval/episode_reward_alive_std': Array(0., dtype=float32), 'eval/episode_reward_linvel_std': Array(nan, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(nan, dtype=float32), 'eval/episode_x_position_std': Array(nan, dtype=float32), 'eval/episode_x_velocity_std': Array(nan, dtype=float32), 'eval/episode_y_position_std': Array(nan, dtype=float32), 'eval/episode_y_velocity_std': Array(nan, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/epoch_eval_time': 10.344578742980957, 'eval/sps': 12373.630979110778}
{'eval/walltime': 71.47181677818298, 'training/sps': 50057.54543530116, 'training/walltime': 178.7651879787445, 'training/entropy_loss': Array(nan, dtype=float32), 'training/policy_loss': Array(nan, dtype=float32), 'training/total_loss': Array(nan, dtype=float32), 'training/v_loss': Array(nan, dtype=float32), 'eval/episode_distance_from_origin': Array(nan, dtype=float32), 'eval/episode_forward_reward': Array(nan, dtype=float32), 'eval/episode_reward': Array(nan, dtype=float32), 'eval/episode_reward_alive': Array(5000., dtype=float32), 'eval/episode_reward_linvel': Array(nan, dtype=float32), 'eval/episode_reward_quadctrl': Array(nan, dtype=float32), 'eval/episode_x_position': Array(nan, dtype=float32), 'eval/episode_x_velocity': Array(nan, dtype=float32), 'eval/episode_y_position': Array(nan, dtype=float32), 'eval/episode_y_velocity': Array(nan, dtype=float32), 'eval/episode_distance_from_origin_std': Array(nan, dtype=float32), 'eval/episode_forward_reward_std': Array(nan, dtype=float32), 'eval/episode_reward_std': Array(nan, dtype=float32), 'eval/episode_reward_alive_std': Array(0., dtype=float32), 'eval/episode_reward_linvel_std': Array(nan, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(nan, dtype=float32), 'eval/episode_x_position_std': Array(nan, dtype=float32), 'eval/episode_x_velocity_std': Array(nan, dtype=float32), 'eval/episode_y_position_std': Array(nan, dtype=float32), 'eval/episode_y_velocity_std': Array(nan, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/epoch_eval_time': 10.37860655784607, 'eval/sps': 12333.062178104627}
{'eval/walltime': 81.85820722579956, 'training/sps': 49917.38353049212, 'training/walltime': 231.28076100349426, 'training/entropy_loss': Array(nan, dtype=float32), 'training/policy_loss': Array(nan, dtype=float32), 'training/total_loss': Array(nan, dtype=float32), 'training/v_loss': Array(nan, dtype=float32), 'eval/episode_distance_from_origin': Array(nan, dtype=float32), 'eval/episode_forward_reward': Array(nan, dtype=float32), 'eval/episode_reward': Array(nan, dtype=float32), 'eval/episode_reward_alive': Array(5000., dtype=float32), 'eval/episode_reward_linvel': Array(nan, dtype=float32), 'eval/episode_reward_quadctrl': Array(nan, dtype=float32), 'eval/episode_x_position': Array(nan, dtype=float32), 'eval/episode_x_velocity': Array(nan, dtype=float32), 'eval/episode_y_position': Array(nan, dtype=float32), 'eval/episode_y_velocity': Array(nan, dtype=float32), 'eval/episode_distance_from_origin_std': Array(nan, dtype=float32), 'eval/episode_forward_reward_std': Array(nan, dtype=float32), 'eval/episode_reward_std': Array(nan, dtype=float32), 'eval/episode_reward_alive_std': Array(0., dtype=float32), 'eval/episode_reward_linvel_std': Array(nan, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(nan, dtype=float32), 'eval/episode_x_position_std': Array(nan, dtype=float32), 'eval/episode_x_velocity_std': Array(nan, dtype=float32), 'eval/episode_y_position_std': Array(nan, dtype=float32), 'eval/episode_y_velocity_std': Array(nan, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/epoch_eval_time': 10.386390447616577, 'eval/sps': 12323.819390919669}

--------

Iteration8-Robot2

--------

{'eval/walltime': 51.268434286117554, 'training/sps': 35195.587148598664, 'training/walltime': 74.4820647239685, 'training/entropy_loss': Array(nan, dtype=float32), 'training/policy_loss': Array(nan, dtype=float32), 'training/total_loss': Array(nan, dtype=float32), 'training/v_loss': Array(nan, dtype=float32), 'eval/episode_distance_from_origin': Array(nan, dtype=float32), 'eval/episode_forward_reward': Array(nan, dtype=float32), 'eval/episode_reward': Array(nan, dtype=float32), 'eval/episode_reward_alive': Array(5000., dtype=float32), 'eval/episode_reward_linvel': Array(nan, dtype=float32), 'eval/episode_reward_quadctrl': Array(nan, dtype=float32), 'eval/episode_x_position': Array(nan, dtype=float32), 'eval/episode_x_velocity': Array(nan, dtype=float32), 'eval/episode_y_position': Array(nan, dtype=float32), 'eval/episode_y_velocity': Array(nan, dtype=float32), 'eval/episode_distance_from_origin_std': Array(nan, dtype=float32), 'eval/episode_forward_reward_std': Array(nan, dtype=float32), 'eval/episode_reward_std': Array(nan, dtype=float32), 'eval/episode_reward_alive_std': Array(0., dtype=float32), 'eval/episode_reward_linvel_std': Array(nan, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(nan, dtype=float32), 'eval/episode_x_position_std': Array(nan, dtype=float32), 'eval/episode_x_velocity_std': Array(nan, dtype=float32), 'eval/episode_y_position_std': Array(nan, dtype=float32), 'eval/episode_y_velocity_std': Array(nan, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/epoch_eval_time': 10.319183826446533, 'eval/sps': 12404.081771656693}
{'eval/walltime': 61.5959415435791, 'training/sps': 50196.30486496255, 'training/walltime': 126.70582914352417, 'training/entropy_loss': Array(nan, dtype=float32), 'training/policy_loss': Array(nan, dtype=float32), 'training/total_loss': Array(nan, dtype=float32), 'training/v_loss': Array(nan, dtype=float32), 'eval/episode_distance_from_origin': Array(nan, dtype=float32), 'eval/episode_forward_reward': Array(nan, dtype=float32), 'eval/episode_reward': Array(nan, dtype=float32), 'eval/episode_reward_alive': Array(5000., dtype=float32), 'eval/episode_reward_linvel': Array(nan, dtype=float32), 'eval/episode_reward_quadctrl': Array(nan, dtype=float32), 'eval/episode_x_position': Array(nan, dtype=float32), 'eval/episode_x_velocity': Array(nan, dtype=float32), 'eval/episode_y_position': Array(nan, dtype=float32), 'eval/episode_y_velocity': Array(nan, dtype=float32), 'eval/episode_distance_from_origin_std': Array(nan, dtype=float32), 'eval/episode_forward_reward_std': Array(nan, dtype=float32), 'eval/episode_reward_std': Array(nan, dtype=float32), 'eval/episode_reward_alive_std': Array(0., dtype=float32), 'eval/episode_reward_linvel_std': Array(nan, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(nan, dtype=float32), 'eval/episode_x_position_std': Array(nan, dtype=float32), 'eval/episode_x_velocity_std': Array(nan, dtype=float32), 'eval/episode_y_position_std': Array(nan, dtype=float32), 'eval/episode_y_velocity_std': Array(nan, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/epoch_eval_time': 10.327507257461548, 'eval/sps': 12394.084730128941}
{'eval/walltime': 71.94821286201477, 'training/sps': 50067.9629675124, 'training/walltime': 179.06346154212952, 'training/entropy_loss': Array(nan, dtype=float32), 'training/policy_loss': Array(nan, dtype=float32), 'training/total_loss': Array(nan, dtype=float32), 'training/v_loss': Array(nan, dtype=float32), 'eval/episode_distance_from_origin': Array(nan, dtype=float32), 'eval/episode_forward_reward': Array(nan, dtype=float32), 'eval/episode_reward': Array(nan, dtype=float32), 'eval/episode_reward_alive': Array(5000., dtype=float32), 'eval/episode_reward_linvel': Array(nan, dtype=float32), 'eval/episode_reward_quadctrl': Array(nan, dtype=float32), 'eval/episode_x_position': Array(nan, dtype=float32), 'eval/episode_x_velocity': Array(nan, dtype=float32), 'eval/episode_y_position': Array(nan, dtype=float32), 'eval/episode_y_velocity': Array(nan, dtype=float32), 'eval/episode_distance_from_origin_std': Array(nan, dtype=float32), 'eval/episode_forward_reward_std': Array(nan, dtype=float32), 'eval/episode_reward_std': Array(nan, dtype=float32), 'eval/episode_reward_alive_std': Array(0., dtype=float32), 'eval/episode_reward_linvel_std': Array(nan, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(nan, dtype=float32), 'eval/episode_x_position_std': Array(nan, dtype=float32), 'eval/episode_x_velocity_std': Array(nan, dtype=float32), 'eval/episode_y_position_std': Array(nan, dtype=float32), 'eval/episode_y_velocity_std': Array(nan, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/epoch_eval_time': 10.352271318435669, 'eval/sps': 12364.436369828652}
{'eval/walltime': 82.31926083564758, 'training/sps': 49963.50420581696, 'training/walltime': 231.53055810928345, 'training/entropy_loss': Array(nan, dtype=float32), 'training/policy_loss': Array(nan, dtype=float32), 'training/total_loss': Array(nan, dtype=float32), 'training/v_loss': Array(nan, dtype=float32), 'eval/episode_distance_from_origin': Array(nan, dtype=float32), 'eval/episode_forward_reward': Array(nan, dtype=float32), 'eval/episode_reward': Array(nan, dtype=float32), 'eval/episode_reward_alive': Array(5000., dtype=float32), 'eval/episode_reward_linvel': Array(nan, dtype=float32), 'eval/episode_reward_quadctrl': Array(nan, dtype=float32), 'eval/episode_x_position': Array(nan, dtype=float32), 'eval/episode_x_velocity': Array(nan, dtype=float32), 'eval/episode_y_position': Array(nan, dtype=float32), 'eval/episode_y_velocity': Array(nan, dtype=float32), 'eval/episode_distance_from_origin_std': Array(nan, dtype=float32), 'eval/episode_forward_reward_std': Array(nan, dtype=float32), 'eval/episode_reward_std': Array(nan, dtype=float32), 'eval/episode_reward_alive_std': Array(0., dtype=float32), 'eval/episode_reward_linvel_std': Array(nan, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(nan, dtype=float32), 'eval/episode_x_position_std': Array(nan, dtype=float32), 'eval/episode_x_velocity_std': Array(nan, dtype=float32), 'eval/episode_y_position_std': Array(nan, dtype=float32), 'eval/episode_y_velocity_std': Array(nan, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/epoch_eval_time': 10.371047973632812, 'eval/sps': 12342.050709381074}

--------

Iteration8-Robot3

--------

{'eval/walltime': 51.73308181762695, 'training/sps': 35157.08400399811, 'training/walltime': 74.56363558769226, 'training/entropy_loss': Array(nan, dtype=float32), 'training/policy_loss': Array(nan, dtype=float32), 'training/total_loss': Array(nan, dtype=float32), 'training/v_loss': Array(nan, dtype=float32), 'eval/episode_distance_from_origin': Array(nan, dtype=float32), 'eval/episode_forward_reward': Array(nan, dtype=float32), 'eval/episode_reward': Array(nan, dtype=float32), 'eval/episode_reward_alive': Array(5000., dtype=float32), 'eval/episode_reward_linvel': Array(nan, dtype=float32), 'eval/episode_reward_quadctrl': Array(nan, dtype=float32), 'eval/episode_x_position': Array(nan, dtype=float32), 'eval/episode_x_velocity': Array(nan, dtype=float32), 'eval/episode_y_position': Array(nan, dtype=float32), 'eval/episode_y_velocity': Array(nan, dtype=float32), 'eval/episode_distance_from_origin_std': Array(nan, dtype=float32), 'eval/episode_forward_reward_std': Array(nan, dtype=float32), 'eval/episode_reward_std': Array(nan, dtype=float32), 'eval/episode_reward_alive_std': Array(0., dtype=float32), 'eval/episode_reward_linvel_std': Array(nan, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(nan, dtype=float32), 'eval/episode_x_position_std': Array(nan, dtype=float32), 'eval/episode_x_velocity_std': Array(nan, dtype=float32), 'eval/episode_y_position_std': Array(nan, dtype=float32), 'eval/episode_y_velocity_std': Array(nan, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/epoch_eval_time': 10.329065799713135, 'eval/sps': 12392.214599267525}
{'eval/walltime': 62.074326276779175, 'training/sps': 50179.36378378431, 'training/walltime': 126.80503129959106, 'training/entropy_loss': Array(nan, dtype=float32), 'training/policy_loss': Array(nan, dtype=float32), 'training/total_loss': Array(nan, dtype=float32), 'training/v_loss': Array(nan, dtype=float32), 'eval/episode_distance_from_origin': Array(nan, dtype=float32), 'eval/episode_forward_reward': Array(nan, dtype=float32), 'eval/episode_reward': Array(nan, dtype=float32), 'eval/episode_reward_alive': Array(5000., dtype=float32), 'eval/episode_reward_linvel': Array(nan, dtype=float32), 'eval/episode_reward_quadctrl': Array(nan, dtype=float32), 'eval/episode_x_position': Array(nan, dtype=float32), 'eval/episode_x_velocity': Array(nan, dtype=float32), 'eval/episode_y_position': Array(nan, dtype=float32), 'eval/episode_y_velocity': Array(nan, dtype=float32), 'eval/episode_distance_from_origin_std': Array(nan, dtype=float32), 'eval/episode_forward_reward_std': Array(nan, dtype=float32), 'eval/episode_reward_std': Array(nan, dtype=float32), 'eval/episode_reward_alive_std': Array(0., dtype=float32), 'eval/episode_reward_linvel_std': Array(nan, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(nan, dtype=float32), 'eval/episode_x_position_std': Array(nan, dtype=float32), 'eval/episode_x_velocity_std': Array(nan, dtype=float32), 'eval/episode_y_position_std': Array(nan, dtype=float32), 'eval/episode_y_velocity_std': Array(nan, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/epoch_eval_time': 10.341244459152222, 'eval/sps': 12377.620556751976}
{'eval/walltime': 72.43868470191956, 'training/sps': 50055.7485292074, 'training/walltime': 179.17543983459473, 'training/entropy_loss': Array(nan, dtype=float32), 'training/policy_loss': Array(nan, dtype=float32), 'training/total_loss': Array(nan, dtype=float32), 'training/v_loss': Array(nan, dtype=float32), 'eval/episode_distance_from_origin': Array(nan, dtype=float32), 'eval/episode_forward_reward': Array(nan, dtype=float32), 'eval/episode_reward': Array(nan, dtype=float32), 'eval/episode_reward_alive': Array(5000., dtype=float32), 'eval/episode_reward_linvel': Array(nan, dtype=float32), 'eval/episode_reward_quadctrl': Array(nan, dtype=float32), 'eval/episode_x_position': Array(nan, dtype=float32), 'eval/episode_x_velocity': Array(nan, dtype=float32), 'eval/episode_y_position': Array(nan, dtype=float32), 'eval/episode_y_velocity': Array(nan, dtype=float32), 'eval/episode_distance_from_origin_std': Array(nan, dtype=float32), 'eval/episode_forward_reward_std': Array(nan, dtype=float32), 'eval/episode_reward_std': Array(nan, dtype=float32), 'eval/episode_reward_alive_std': Array(0., dtype=float32), 'eval/episode_reward_linvel_std': Array(nan, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(nan, dtype=float32), 'eval/episode_x_position_std': Array(nan, dtype=float32), 'eval/episode_x_velocity_std': Array(nan, dtype=float32), 'eval/episode_y_position_std': Array(nan, dtype=float32), 'eval/episode_y_velocity_std': Array(nan, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/epoch_eval_time': 10.36435842514038, 'eval/sps': 12350.016735190851}
{'eval/walltime': 82.81597256660461, 'training/sps': 49940.7431880695, 'training/walltime': 231.66644883155823, 'training/entropy_loss': Array(nan, dtype=float32), 'training/policy_loss': Array(nan, dtype=float32), 'training/total_loss': Array(nan, dtype=float32), 'training/v_loss': Array(nan, dtype=float32), 'eval/episode_distance_from_origin': Array(nan, dtype=float32), 'eval/episode_forward_reward': Array(nan, dtype=float32), 'eval/episode_reward': Array(nan, dtype=float32), 'eval/episode_reward_alive': Array(5000., dtype=float32), 'eval/episode_reward_linvel': Array(nan, dtype=float32), 'eval/episode_reward_quadctrl': Array(nan, dtype=float32), 'eval/episode_x_position': Array(nan, dtype=float32), 'eval/episode_x_velocity': Array(nan, dtype=float32), 'eval/episode_y_position': Array(nan, dtype=float32), 'eval/episode_y_velocity': Array(nan, dtype=float32), 'eval/episode_distance_from_origin_std': Array(nan, dtype=float32), 'eval/episode_forward_reward_std': Array(nan, dtype=float32), 'eval/episode_reward_std': Array(nan, dtype=float32), 'eval/episode_reward_alive_std': Array(0., dtype=float32), 'eval/episode_reward_linvel_std': Array(nan, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(nan, dtype=float32), 'eval/episode_x_position_std': Array(nan, dtype=float32), 'eval/episode_x_velocity_std': Array(nan, dtype=float32), 'eval/episode_y_position_std': Array(nan, dtype=float32), 'eval/episode_y_velocity_std': Array(nan, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/epoch_eval_time': 10.377287864685059, 'eval/sps': 12334.629401155644}
9

--------

Iteration9-Robot0

--------

{'eval/walltime': 52.22513699531555, 'training/sps': 35227.788732057095, 'training/walltime': 74.41398096084595, 'training/entropy_loss': Array(nan, dtype=float32), 'training/policy_loss': Array(nan, dtype=float32), 'training/total_loss': Array(nan, dtype=float32), 'training/v_loss': Array(nan, dtype=float32), 'eval/episode_distance_from_origin': Array(nan, dtype=float32), 'eval/episode_forward_reward': Array(nan, dtype=float32), 'eval/episode_reward': Array(nan, dtype=float32), 'eval/episode_reward_alive': Array(5000., dtype=float32), 'eval/episode_reward_linvel': Array(nan, dtype=float32), 'eval/episode_reward_quadctrl': Array(nan, dtype=float32), 'eval/episode_x_position': Array(nan, dtype=float32), 'eval/episode_x_velocity': Array(nan, dtype=float32), 'eval/episode_y_position': Array(nan, dtype=float32), 'eval/episode_y_velocity': Array(nan, dtype=float32), 'eval/episode_distance_from_origin_std': Array(nan, dtype=float32), 'eval/episode_forward_reward_std': Array(nan, dtype=float32), 'eval/episode_reward_std': Array(nan, dtype=float32), 'eval/episode_reward_alive_std': Array(0., dtype=float32), 'eval/episode_reward_linvel_std': Array(nan, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(nan, dtype=float32), 'eval/episode_x_position_std': Array(nan, dtype=float32), 'eval/episode_x_velocity_std': Array(nan, dtype=float32), 'eval/episode_y_position_std': Array(nan, dtype=float32), 'eval/episode_y_velocity_std': Array(nan, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/epoch_eval_time': 10.335094928741455, 'eval/sps': 12384.985419344093}
{'eval/walltime': 62.555784702301025, 'training/sps': 50169.029411749005, 'training/walltime': 126.66613793373108, 'training/entropy_loss': Array(nan, dtype=float32), 'training/policy_loss': Array(nan, dtype=float32), 'training/total_loss': Array(nan, dtype=float32), 'training/v_loss': Array(nan, dtype=float32), 'eval/episode_distance_from_origin': Array(nan, dtype=float32), 'eval/episode_forward_reward': Array(nan, dtype=float32), 'eval/episode_reward': Array(nan, dtype=float32), 'eval/episode_reward_alive': Array(5000., dtype=float32), 'eval/episode_reward_linvel': Array(nan, dtype=float32), 'eval/episode_reward_quadctrl': Array(nan, dtype=float32), 'eval/episode_x_position': Array(nan, dtype=float32), 'eval/episode_x_velocity': Array(nan, dtype=float32), 'eval/episode_y_position': Array(nan, dtype=float32), 'eval/episode_y_velocity': Array(nan, dtype=float32), 'eval/episode_distance_from_origin_std': Array(nan, dtype=float32), 'eval/episode_forward_reward_std': Array(nan, dtype=float32), 'eval/episode_reward_std': Array(nan, dtype=float32), 'eval/episode_reward_alive_std': Array(0., dtype=float32), 'eval/episode_reward_linvel_std': Array(nan, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(nan, dtype=float32), 'eval/episode_x_position_std': Array(nan, dtype=float32), 'eval/episode_x_velocity_std': Array(nan, dtype=float32), 'eval/episode_y_position_std': Array(nan, dtype=float32), 'eval/episode_y_velocity_std': Array(nan, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/epoch_eval_time': 10.330647706985474, 'eval/sps': 12390.317009208218}
{'eval/walltime': 72.88914561271667, 'training/sps': 50091.23285935546, 'training/walltime': 178.99944758415222, 'training/entropy_loss': Array(nan, dtype=float32), 'training/policy_loss': Array(nan, dtype=float32), 'training/total_loss': Array(nan, dtype=float32), 'training/v_loss': Array(nan, dtype=float32), 'eval/episode_distance_from_origin': Array(nan, dtype=float32), 'eval/episode_forward_reward': Array(nan, dtype=float32), 'eval/episode_reward': Array(nan, dtype=float32), 'eval/episode_reward_alive': Array(5000., dtype=float32), 'eval/episode_reward_linvel': Array(nan, dtype=float32), 'eval/episode_reward_quadctrl': Array(nan, dtype=float32), 'eval/episode_x_position': Array(nan, dtype=float32), 'eval/episode_x_velocity': Array(nan, dtype=float32), 'eval/episode_y_position': Array(nan, dtype=float32), 'eval/episode_y_velocity': Array(nan, dtype=float32), 'eval/episode_distance_from_origin_std': Array(nan, dtype=float32), 'eval/episode_forward_reward_std': Array(nan, dtype=float32), 'eval/episode_reward_std': Array(nan, dtype=float32), 'eval/episode_reward_alive_std': Array(0., dtype=float32), 'eval/episode_reward_linvel_std': Array(nan, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(nan, dtype=float32), 'eval/episode_x_position_std': Array(nan, dtype=float32), 'eval/episode_x_velocity_std': Array(nan, dtype=float32), 'eval/episode_y_position_std': Array(nan, dtype=float32), 'eval/episode_y_velocity_std': Array(nan, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/epoch_eval_time': 10.33336091041565, 'eval/sps': 12387.063716218476}
{'eval/walltime': 83.24642086029053, 'training/sps': 49935.717255886455, 'training/walltime': 231.49573969841003, 'training/entropy_loss': Array(nan, dtype=float32), 'training/policy_loss': Array(nan, dtype=float32), 'training/total_loss': Array(nan, dtype=float32), 'training/v_loss': Array(nan, dtype=float32), 'eval/episode_distance_from_origin': Array(nan, dtype=float32), 'eval/episode_forward_reward': Array(nan, dtype=float32), 'eval/episode_reward': Array(nan, dtype=float32), 'eval/episode_reward_alive': Array(5000., dtype=float32), 'eval/episode_reward_linvel': Array(nan, dtype=float32), 'eval/episode_reward_quadctrl': Array(nan, dtype=float32), 'eval/episode_x_position': Array(nan, dtype=float32), 'eval/episode_x_velocity': Array(nan, dtype=float32), 'eval/episode_y_position': Array(nan, dtype=float32), 'eval/episode_y_velocity': Array(nan, dtype=float32), 'eval/episode_distance_from_origin_std': Array(nan, dtype=float32), 'eval/episode_forward_reward_std': Array(nan, dtype=float32), 'eval/episode_reward_std': Array(nan, dtype=float32), 'eval/episode_reward_alive_std': Array(0., dtype=float32), 'eval/episode_reward_linvel_std': Array(nan, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(nan, dtype=float32), 'eval/episode_x_position_std': Array(nan, dtype=float32), 'eval/episode_x_velocity_std': Array(nan, dtype=float32), 'eval/episode_y_position_std': Array(nan, dtype=float32), 'eval/episode_y_velocity_std': Array(nan, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/epoch_eval_time': 10.357275247573853, 'eval/sps': 12358.462717304288}

--------

Iteration9-Robot1

--------

{'eval/walltime': 51.41647529602051, 'training/sps': 34532.224455062606, 'training/walltime': 75.9128623008728, 'training/entropy_loss': Array(-0.012, dtype=float32), 'training/policy_loss': Array(0.009, dtype=float32), 'training/total_loss': Array(0.131, dtype=float32), 'training/v_loss': Array(0.135, dtype=float32), 'eval/episode_distance_from_origin': Array(41.266, dtype=float32), 'eval/episode_forward_reward': Array(10.736, dtype=float32), 'eval/episode_reward': Array(216.699, dtype=float32), 'eval/episode_reward_alive': Array(235.586, dtype=float32), 'eval/episode_reward_linvel': Array(10.736, dtype=float32), 'eval/episode_reward_quadctrl': Array(-29.624, dtype=float32), 'eval/episode_x_position': Array(4.055, dtype=float32), 'eval/episode_x_velocity': Array(8.589, dtype=float32), 'eval/episode_y_position': Array(0.641, dtype=float32), 'eval/episode_y_velocity': Array(1.45, dtype=float32), 'eval/episode_distance_from_origin_std': Array(8.807, dtype=float32), 'eval/episode_forward_reward_std': Array(11.201, dtype=float32), 'eval/episode_reward_std': Array(47.24, dtype=float32), 'eval/episode_reward_alive_std': Array(50.236, dtype=float32), 'eval/episode_reward_linvel_std': Array(11.201, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(6.291, dtype=float32), 'eval/episode_x_position_std': Array(3.538, dtype=float32), 'eval/episode_x_velocity_std': Array(8.961, dtype=float32), 'eval/episode_y_position_std': Array(3.665, dtype=float32), 'eval/episode_y_velocity_std': Array(8.756, dtype=float32), 'eval/avg_episode_length': Array(47.117, dtype=float32), 'eval/epoch_eval_time': 10.36205506324768, 'eval/sps': 12352.761997375661}
{'eval/walltime': 61.76939511299133, 'training/sps': 49190.05891439328, 'training/walltime': 129.2049310207367, 'training/entropy_loss': Array(-0.01, dtype=float32), 'training/policy_loss': Array(-0.016, dtype=float32), 'training/total_loss': Array(0.078, dtype=float32), 'training/v_loss': Array(0.104, dtype=float32), 'eval/episode_distance_from_origin': Array(72.216, dtype=float32), 'eval/episode_forward_reward': Array(33.32, dtype=float32), 'eval/episode_reward': Array(367.723, dtype=float32), 'eval/episode_reward_alive': Array(380.234, dtype=float32), 'eval/episode_reward_linvel': Array(33.32, dtype=float32), 'eval/episode_reward_quadctrl': Array(-45.831, dtype=float32), 'eval/episode_x_position': Array(22.082, dtype=float32), 'eval/episode_x_velocity': Array(26.656, dtype=float32), 'eval/episode_y_position': Array(6.703, dtype=float32), 'eval/episode_y_velocity': Array(7.832, dtype=float32), 'eval/episode_distance_from_origin_std': Array(24.954, dtype=float32), 'eval/episode_forward_reward_std': Array(25.095, dtype=float32), 'eval/episode_reward_std': Array(105.431, dtype=float32), 'eval/episode_reward_alive_std': Array(103.925, dtype=float32), 'eval/episode_reward_linvel_std': Array(25.095, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(12.033, dtype=float32), 'eval/episode_x_position_std': Array(18.767, dtype=float32), 'eval/episode_x_velocity_std': Array(20.076, dtype=float32), 'eval/episode_y_position_std': Array(9.956, dtype=float32), 'eval/episode_y_velocity_std': Array(13.48, dtype=float32), 'eval/avg_episode_length': Array(76.047, dtype=float32), 'eval/epoch_eval_time': 10.352919816970825, 'eval/sps': 12363.661871521352}
{'eval/walltime': 72.15089988708496, 'training/sps': 49021.06581120617, 'training/walltime': 182.6807165145874, 'training/entropy_loss': Array(-0.009, dtype=float32), 'training/policy_loss': Array(-0.013, dtype=float32), 'training/total_loss': Array(0.124, dtype=float32), 'training/v_loss': Array(0.147, dtype=float32), 'eval/episode_distance_from_origin': Array(171.886, dtype=float32), 'eval/episode_forward_reward': Array(97.351, dtype=float32), 'eval/episode_reward': Array(640.864, dtype=float32), 'eval/episode_reward_alive': Array(616.562, dtype=float32), 'eval/episode_reward_linvel': Array(97.351, dtype=float32), 'eval/episode_reward_quadctrl': Array(-73.05, dtype=float32), 'eval/episode_x_position': Array(113.569, dtype=float32), 'eval/episode_x_velocity': Array(77.881, dtype=float32), 'eval/episode_y_position': Array(16.852, dtype=float32), 'eval/episode_y_velocity': Array(10.065, dtype=float32), 'eval/episode_distance_from_origin_std': Array(121.705, dtype=float32), 'eval/episode_forward_reward_std': Array(59.637, dtype=float32), 'eval/episode_reward_std': Array(249.809, dtype=float32), 'eval/episode_reward_alive_std': Array(224.051, dtype=float32), 'eval/episode_reward_linvel_std': Array(59.637, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(26.68, dtype=float32), 'eval/episode_x_position_std': Array(115.417, dtype=float32), 'eval/episode_x_velocity_std': Array(47.709, dtype=float32), 'eval/episode_y_position_std': Array(22.576, dtype=float32), 'eval/episode_y_velocity_std': Array(18.855, dtype=float32), 'eval/avg_episode_length': Array(123.312, dtype=float32), 'eval/epoch_eval_time': 10.381504774093628, 'eval/sps': 12329.619143403537}
{'eval/walltime': 82.52308297157288, 'training/sps': 48902.16241278467, 'training/walltime': 236.28652596473694, 'training/entropy_loss': Array(-0.008, dtype=float32), 'training/policy_loss': Array(-0.011, dtype=float32), 'training/total_loss': Array(0.161, dtype=float32), 'training/v_loss': Array(0.18, dtype=float32), 'eval/episode_distance_from_origin': Array(572.723, dtype=float32), 'eval/episode_forward_reward': Array(232.956, dtype=float32), 'eval/episode_reward': Array(1124.516, dtype=float32), 'eval/episode_reward_alive': Array(1010.664, dtype=float32), 'eval/episode_reward_linvel': Array(232.956, dtype=float32), 'eval/episode_reward_quadctrl': Array(-119.104, dtype=float32), 'eval/episode_x_position': Array(508.182, dtype=float32), 'eval/episode_x_velocity': Array(186.365, dtype=float32), 'eval/episode_y_position': Array(58.461, dtype=float32), 'eval/episode_y_velocity': Array(15.917, dtype=float32), 'eval/episode_distance_from_origin_std': Array(682.683, dtype=float32), 'eval/episode_forward_reward_std': Array(132.782, dtype=float32), 'eval/episode_reward_std': Array(570.873, dtype=float32), 'eval/episode_reward_alive_std': Array(502.759, dtype=float32), 'eval/episode_reward_linvel_std': Array(132.782, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(58.471, dtype=float32), 'eval/episode_x_position_std': Array(672.873, dtype=float32), 'eval/episode_x_velocity_std': Array(106.226, dtype=float32), 'eval/episode_y_position_std': Array(87.934, dtype=float32), 'eval/episode_y_velocity_std': Array(27.939, dtype=float32), 'eval/avg_episode_length': Array(202.133, dtype=float32), 'eval/epoch_eval_time': 10.372183084487915, 'eval/sps': 12340.700020175114}
time to jit: 0:00:56.849844
time to train: 0:04:37.981202

--------

Iteration9-Robot2

--------

{'eval/walltime': 52.891520261764526, 'training/sps': 35161.902114386736, 'training/walltime': 74.55341839790344, 'training/entropy_loss': Array(nan, dtype=float32), 'training/policy_loss': Array(nan, dtype=float32), 'training/total_loss': Array(nan, dtype=float32), 'training/v_loss': Array(nan, dtype=float32), 'eval/episode_distance_from_origin': Array(nan, dtype=float32), 'eval/episode_forward_reward': Array(nan, dtype=float32), 'eval/episode_reward': Array(nan, dtype=float32), 'eval/episode_reward_alive': Array(5000., dtype=float32), 'eval/episode_reward_linvel': Array(nan, dtype=float32), 'eval/episode_reward_quadctrl': Array(nan, dtype=float32), 'eval/episode_x_position': Array(nan, dtype=float32), 'eval/episode_x_velocity': Array(nan, dtype=float32), 'eval/episode_y_position': Array(nan, dtype=float32), 'eval/episode_y_velocity': Array(nan, dtype=float32), 'eval/episode_distance_from_origin_std': Array(nan, dtype=float32), 'eval/episode_forward_reward_std': Array(nan, dtype=float32), 'eval/episode_reward_std': Array(nan, dtype=float32), 'eval/episode_reward_alive_std': Array(0., dtype=float32), 'eval/episode_reward_linvel_std': Array(nan, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(nan, dtype=float32), 'eval/episode_x_position_std': Array(nan, dtype=float32), 'eval/episode_x_velocity_std': Array(nan, dtype=float32), 'eval/episode_y_position_std': Array(nan, dtype=float32), 'eval/episode_y_velocity_std': Array(nan, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/epoch_eval_time': 10.339715957641602, 'eval/sps': 12379.450318013927}
{'eval/walltime': 63.227099657058716, 'training/sps': 50127.21819603428, 'training/walltime': 126.84915900230408, 'training/entropy_loss': Array(nan, dtype=float32), 'training/policy_loss': Array(nan, dtype=float32), 'training/total_loss': Array(nan, dtype=float32), 'training/v_loss': Array(nan, dtype=float32), 'eval/episode_distance_from_origin': Array(nan, dtype=float32), 'eval/episode_forward_reward': Array(nan, dtype=float32), 'eval/episode_reward': Array(nan, dtype=float32), 'eval/episode_reward_alive': Array(5000., dtype=float32), 'eval/episode_reward_linvel': Array(nan, dtype=float32), 'eval/episode_reward_quadctrl': Array(nan, dtype=float32), 'eval/episode_x_position': Array(nan, dtype=float32), 'eval/episode_x_velocity': Array(nan, dtype=float32), 'eval/episode_y_position': Array(nan, dtype=float32), 'eval/episode_y_velocity': Array(nan, dtype=float32), 'eval/episode_distance_from_origin_std': Array(nan, dtype=float32), 'eval/episode_forward_reward_std': Array(nan, dtype=float32), 'eval/episode_reward_std': Array(nan, dtype=float32), 'eval/episode_reward_alive_std': Array(0., dtype=float32), 'eval/episode_reward_linvel_std': Array(nan, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(nan, dtype=float32), 'eval/episode_x_position_std': Array(nan, dtype=float32), 'eval/episode_x_velocity_std': Array(nan, dtype=float32), 'eval/episode_y_position_std': Array(nan, dtype=float32), 'eval/episode_y_velocity_std': Array(nan, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/epoch_eval_time': 10.33557939529419, 'eval/sps': 12384.404889606736}
{'eval/walltime': 73.5746419429779, 'training/sps': 50002.03179258822, 'training/walltime': 179.2758285999298, 'training/entropy_loss': Array(nan, dtype=float32), 'training/policy_loss': Array(nan, dtype=float32), 'training/total_loss': Array(nan, dtype=float32), 'training/v_loss': Array(nan, dtype=float32), 'eval/episode_distance_from_origin': Array(nan, dtype=float32), 'eval/episode_forward_reward': Array(nan, dtype=float32), 'eval/episode_reward': Array(nan, dtype=float32), 'eval/episode_reward_alive': Array(5000., dtype=float32), 'eval/episode_reward_linvel': Array(nan, dtype=float32), 'eval/episode_reward_quadctrl': Array(nan, dtype=float32), 'eval/episode_x_position': Array(nan, dtype=float32), 'eval/episode_x_velocity': Array(nan, dtype=float32), 'eval/episode_y_position': Array(nan, dtype=float32), 'eval/episode_y_velocity': Array(nan, dtype=float32), 'eval/episode_distance_from_origin_std': Array(nan, dtype=float32), 'eval/episode_forward_reward_std': Array(nan, dtype=float32), 'eval/episode_reward_std': Array(nan, dtype=float32), 'eval/episode_reward_alive_std': Array(0., dtype=float32), 'eval/episode_reward_linvel_std': Array(nan, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(nan, dtype=float32), 'eval/episode_x_position_std': Array(nan, dtype=float32), 'eval/episode_x_velocity_std': Array(nan, dtype=float32), 'eval/episode_y_position_std': Array(nan, dtype=float32), 'eval/episode_y_velocity_std': Array(nan, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/epoch_eval_time': 10.34754228591919, 'eval/sps': 12370.087163033955}
{'eval/walltime': 83.92811417579651, 'training/sps': 49963.210188326455, 'training/walltime': 231.74323391914368, 'training/entropy_loss': Array(nan, dtype=float32), 'training/policy_loss': Array(nan, dtype=float32), 'training/total_loss': Array(nan, dtype=float32), 'training/v_loss': Array(nan, dtype=float32), 'eval/episode_distance_from_origin': Array(nan, dtype=float32), 'eval/episode_forward_reward': Array(nan, dtype=float32), 'eval/episode_reward': Array(nan, dtype=float32), 'eval/episode_reward_alive': Array(5000., dtype=float32), 'eval/episode_reward_linvel': Array(nan, dtype=float32), 'eval/episode_reward_quadctrl': Array(nan, dtype=float32), 'eval/episode_x_position': Array(nan, dtype=float32), 'eval/episode_x_velocity': Array(nan, dtype=float32), 'eval/episode_y_position': Array(nan, dtype=float32), 'eval/episode_y_velocity': Array(nan, dtype=float32), 'eval/episode_distance_from_origin_std': Array(nan, dtype=float32), 'eval/episode_forward_reward_std': Array(nan, dtype=float32), 'eval/episode_reward_std': Array(nan, dtype=float32), 'eval/episode_reward_alive_std': Array(0., dtype=float32), 'eval/episode_reward_linvel_std': Array(nan, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(nan, dtype=float32), 'eval/episode_x_position_std': Array(nan, dtype=float32), 'eval/episode_x_velocity_std': Array(nan, dtype=float32), 'eval/episode_y_position_std': Array(nan, dtype=float32), 'eval/episode_y_velocity_std': Array(nan, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/epoch_eval_time': 10.353472232818604, 'eval/sps': 12363.002200775072}

--------

Iteration9-Robot3

--------

{'eval/walltime': 51.6934449672699, 'training/sps': 35012.121860536514, 'training/walltime': 74.87235450744629, 'training/entropy_loss': Array(nan, dtype=float32), 'training/policy_loss': Array(nan, dtype=float32), 'training/total_loss': Array(nan, dtype=float32), 'training/v_loss': Array(nan, dtype=float32), 'eval/episode_distance_from_origin': Array(nan, dtype=float32), 'eval/episode_forward_reward': Array(nan, dtype=float32), 'eval/episode_reward': Array(nan, dtype=float32), 'eval/episode_reward_alive': Array(5000., dtype=float32), 'eval/episode_reward_linvel': Array(nan, dtype=float32), 'eval/episode_reward_quadctrl': Array(nan, dtype=float32), 'eval/episode_x_position': Array(nan, dtype=float32), 'eval/episode_x_velocity': Array(nan, dtype=float32), 'eval/episode_y_position': Array(nan, dtype=float32), 'eval/episode_y_velocity': Array(nan, dtype=float32), 'eval/episode_distance_from_origin_std': Array(nan, dtype=float32), 'eval/episode_forward_reward_std': Array(nan, dtype=float32), 'eval/episode_reward_std': Array(nan, dtype=float32), 'eval/episode_reward_alive_std': Array(0., dtype=float32), 'eval/episode_reward_linvel_std': Array(nan, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(nan, dtype=float32), 'eval/episode_x_position_std': Array(nan, dtype=float32), 'eval/episode_x_velocity_std': Array(nan, dtype=float32), 'eval/episode_y_position_std': Array(nan, dtype=float32), 'eval/episode_y_velocity_std': Array(nan, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/epoch_eval_time': 10.355640172958374, 'eval/sps': 12360.414021940014}
{'eval/walltime': 62.04866099357605, 'training/sps': 50171.25683984538, 'training/walltime': 127.12219166755676, 'training/entropy_loss': Array(nan, dtype=float32), 'training/policy_loss': Array(nan, dtype=float32), 'training/total_loss': Array(nan, dtype=float32), 'training/v_loss': Array(nan, dtype=float32), 'eval/episode_distance_from_origin': Array(nan, dtype=float32), 'eval/episode_forward_reward': Array(nan, dtype=float32), 'eval/episode_reward': Array(nan, dtype=float32), 'eval/episode_reward_alive': Array(5000., dtype=float32), 'eval/episode_reward_linvel': Array(nan, dtype=float32), 'eval/episode_reward_quadctrl': Array(nan, dtype=float32), 'eval/episode_x_position': Array(nan, dtype=float32), 'eval/episode_x_velocity': Array(nan, dtype=float32), 'eval/episode_y_position': Array(nan, dtype=float32), 'eval/episode_y_velocity': Array(nan, dtype=float32), 'eval/episode_distance_from_origin_std': Array(nan, dtype=float32), 'eval/episode_forward_reward_std': Array(nan, dtype=float32), 'eval/episode_reward_std': Array(nan, dtype=float32), 'eval/episode_reward_alive_std': Array(0., dtype=float32), 'eval/episode_reward_linvel_std': Array(nan, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(nan, dtype=float32), 'eval/episode_x_position_std': Array(nan, dtype=float32), 'eval/episode_x_velocity_std': Array(nan, dtype=float32), 'eval/episode_y_position_std': Array(nan, dtype=float32), 'eval/episode_y_velocity_std': Array(nan, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/epoch_eval_time': 10.355216026306152, 'eval/sps': 12360.920300921945}
{'eval/walltime': 72.40533185005188, 'training/sps': 50045.38550836327, 'training/walltime': 179.50344467163086, 'training/entropy_loss': Array(nan, dtype=float32), 'training/policy_loss': Array(nan, dtype=float32), 'training/total_loss': Array(nan, dtype=float32), 'training/v_loss': Array(nan, dtype=float32), 'eval/episode_distance_from_origin': Array(nan, dtype=float32), 'eval/episode_forward_reward': Array(nan, dtype=float32), 'eval/episode_reward': Array(nan, dtype=float32), 'eval/episode_reward_alive': Array(5000., dtype=float32), 'eval/episode_reward_linvel': Array(nan, dtype=float32), 'eval/episode_reward_quadctrl': Array(nan, dtype=float32), 'eval/episode_x_position': Array(nan, dtype=float32), 'eval/episode_x_velocity': Array(nan, dtype=float32), 'eval/episode_y_position': Array(nan, dtype=float32), 'eval/episode_y_velocity': Array(nan, dtype=float32), 'eval/episode_distance_from_origin_std': Array(nan, dtype=float32), 'eval/episode_forward_reward_std': Array(nan, dtype=float32), 'eval/episode_reward_std': Array(nan, dtype=float32), 'eval/episode_reward_alive_std': Array(0., dtype=float32), 'eval/episode_reward_linvel_std': Array(nan, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(nan, dtype=float32), 'eval/episode_x_position_std': Array(nan, dtype=float32), 'eval/episode_x_velocity_std': Array(nan, dtype=float32), 'eval/episode_y_position_std': Array(nan, dtype=float32), 'eval/episode_y_velocity_std': Array(nan, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/epoch_eval_time': 10.35667085647583, 'eval/sps': 12359.183928295262}
{'eval/walltime': 82.77313113212585, 'training/sps': 49838.61243880466, 'training/walltime': 232.10201978683472, 'training/entropy_loss': Array(nan, dtype=float32), 'training/policy_loss': Array(nan, dtype=float32), 'training/total_loss': Array(nan, dtype=float32), 'training/v_loss': Array(nan, dtype=float32), 'eval/episode_distance_from_origin': Array(nan, dtype=float32), 'eval/episode_forward_reward': Array(nan, dtype=float32), 'eval/episode_reward': Array(nan, dtype=float32), 'eval/episode_reward_alive': Array(5000., dtype=float32), 'eval/episode_reward_linvel': Array(nan, dtype=float32), 'eval/episode_reward_quadctrl': Array(nan, dtype=float32), 'eval/episode_x_position': Array(nan, dtype=float32), 'eval/episode_x_velocity': Array(nan, dtype=float32), 'eval/episode_y_position': Array(nan, dtype=float32), 'eval/episode_y_velocity': Array(nan, dtype=float32), 'eval/episode_distance_from_origin_std': Array(nan, dtype=float32), 'eval/episode_forward_reward_std': Array(nan, dtype=float32), 'eval/episode_reward_std': Array(nan, dtype=float32), 'eval/episode_reward_alive_std': Array(0., dtype=float32), 'eval/episode_reward_linvel_std': Array(nan, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(nan, dtype=float32), 'eval/episode_x_position_std': Array(nan, dtype=float32), 'eval/episode_x_velocity_std': Array(nan, dtype=float32), 'eval/episode_y_position_std': Array(nan, dtype=float32), 'eval/episode_y_velocity_std': Array(nan, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/epoch_eval_time': 10.367799282073975, 'eval/sps': 12345.91802151429}
10

--------

Iteration10-Robot0

--------

{'eval/walltime': 50.439129114151, 'training/sps': 35240.06356823578, 'training/walltime': 74.38806104660034, 'training/entropy_loss': Array(nan, dtype=float32), 'training/policy_loss': Array(nan, dtype=float32), 'training/total_loss': Array(nan, dtype=float32), 'training/v_loss': Array(nan, dtype=float32), 'eval/episode_distance_from_origin': Array(nan, dtype=float32), 'eval/episode_forward_reward': Array(nan, dtype=float32), 'eval/episode_reward': Array(nan, dtype=float32), 'eval/episode_reward_alive': Array(5000., dtype=float32), 'eval/episode_reward_linvel': Array(nan, dtype=float32), 'eval/episode_reward_quadctrl': Array(nan, dtype=float32), 'eval/episode_x_position': Array(nan, dtype=float32), 'eval/episode_x_velocity': Array(nan, dtype=float32), 'eval/episode_y_position': Array(nan, dtype=float32), 'eval/episode_y_velocity': Array(nan, dtype=float32), 'eval/episode_distance_from_origin_std': Array(nan, dtype=float32), 'eval/episode_forward_reward_std': Array(nan, dtype=float32), 'eval/episode_reward_std': Array(nan, dtype=float32), 'eval/episode_reward_alive_std': Array(0., dtype=float32), 'eval/episode_reward_linvel_std': Array(nan, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(nan, dtype=float32), 'eval/episode_x_position_std': Array(nan, dtype=float32), 'eval/episode_x_velocity_std': Array(nan, dtype=float32), 'eval/episode_y_position_std': Array(nan, dtype=float32), 'eval/episode_y_velocity_std': Array(nan, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/epoch_eval_time': 10.341253280639648, 'eval/sps': 12377.609998164815}
{'eval/walltime': 60.77616810798645, 'training/sps': 50258.300605416196, 'training/walltime': 126.54740524291992, 'training/entropy_loss': Array(nan, dtype=float32), 'training/policy_loss': Array(nan, dtype=float32), 'training/total_loss': Array(nan, dtype=float32), 'training/v_loss': Array(nan, dtype=float32), 'eval/episode_distance_from_origin': Array(nan, dtype=float32), 'eval/episode_forward_reward': Array(nan, dtype=float32), 'eval/episode_reward': Array(nan, dtype=float32), 'eval/episode_reward_alive': Array(5000., dtype=float32), 'eval/episode_reward_linvel': Array(nan, dtype=float32), 'eval/episode_reward_quadctrl': Array(nan, dtype=float32), 'eval/episode_x_position': Array(nan, dtype=float32), 'eval/episode_x_velocity': Array(nan, dtype=float32), 'eval/episode_y_position': Array(nan, dtype=float32), 'eval/episode_y_velocity': Array(nan, dtype=float32), 'eval/episode_distance_from_origin_std': Array(nan, dtype=float32), 'eval/episode_forward_reward_std': Array(nan, dtype=float32), 'eval/episode_reward_std': Array(nan, dtype=float32), 'eval/episode_reward_alive_std': Array(0., dtype=float32), 'eval/episode_reward_linvel_std': Array(nan, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(nan, dtype=float32), 'eval/episode_x_position_std': Array(nan, dtype=float32), 'eval/episode_x_velocity_std': Array(nan, dtype=float32), 'eval/episode_y_position_std': Array(nan, dtype=float32), 'eval/episode_y_velocity_std': Array(nan, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/epoch_eval_time': 10.33703899383545, 'eval/sps': 12382.65620129067}
{'eval/walltime': 71.1345009803772, 'training/sps': 50128.21301640079, 'training/walltime': 178.84210801124573, 'training/entropy_loss': Array(nan, dtype=float32), 'training/policy_loss': Array(nan, dtype=float32), 'training/total_loss': Array(nan, dtype=float32), 'training/v_loss': Array(nan, dtype=float32), 'eval/episode_distance_from_origin': Array(nan, dtype=float32), 'eval/episode_forward_reward': Array(nan, dtype=float32), 'eval/episode_reward': Array(nan, dtype=float32), 'eval/episode_reward_alive': Array(5000., dtype=float32), 'eval/episode_reward_linvel': Array(nan, dtype=float32), 'eval/episode_reward_quadctrl': Array(nan, dtype=float32), 'eval/episode_x_position': Array(nan, dtype=float32), 'eval/episode_x_velocity': Array(nan, dtype=float32), 'eval/episode_y_position': Array(nan, dtype=float32), 'eval/episode_y_velocity': Array(nan, dtype=float32), 'eval/episode_distance_from_origin_std': Array(nan, dtype=float32), 'eval/episode_forward_reward_std': Array(nan, dtype=float32), 'eval/episode_reward_std': Array(nan, dtype=float32), 'eval/episode_reward_alive_std': Array(0., dtype=float32), 'eval/episode_reward_linvel_std': Array(nan, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(nan, dtype=float32), 'eval/episode_x_position_std': Array(nan, dtype=float32), 'eval/episode_x_velocity_std': Array(nan, dtype=float32), 'eval/episode_y_position_std': Array(nan, dtype=float32), 'eval/episode_y_velocity_std': Array(nan, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/epoch_eval_time': 10.358332872390747, 'eval/sps': 12357.200871693656}
{'eval/walltime': 81.48649644851685, 'training/sps': 49930.63791675651, 'training/walltime': 231.34374046325684, 'training/entropy_loss': Array(nan, dtype=float32), 'training/policy_loss': Array(nan, dtype=float32), 'training/total_loss': Array(nan, dtype=float32), 'training/v_loss': Array(nan, dtype=float32), 'eval/episode_distance_from_origin': Array(nan, dtype=float32), 'eval/episode_forward_reward': Array(nan, dtype=float32), 'eval/episode_reward': Array(nan, dtype=float32), 'eval/episode_reward_alive': Array(5000., dtype=float32), 'eval/episode_reward_linvel': Array(nan, dtype=float32), 'eval/episode_reward_quadctrl': Array(nan, dtype=float32), 'eval/episode_x_position': Array(nan, dtype=float32), 'eval/episode_x_velocity': Array(nan, dtype=float32), 'eval/episode_y_position': Array(nan, dtype=float32), 'eval/episode_y_velocity': Array(nan, dtype=float32), 'eval/episode_distance_from_origin_std': Array(nan, dtype=float32), 'eval/episode_forward_reward_std': Array(nan, dtype=float32), 'eval/episode_reward_std': Array(nan, dtype=float32), 'eval/episode_reward_alive_std': Array(0., dtype=float32), 'eval/episode_reward_linvel_std': Array(nan, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(nan, dtype=float32), 'eval/episode_x_position_std': Array(nan, dtype=float32), 'eval/episode_x_velocity_std': Array(nan, dtype=float32), 'eval/episode_y_position_std': Array(nan, dtype=float32), 'eval/episode_y_velocity_std': Array(nan, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/epoch_eval_time': 10.351995468139648, 'eval/sps': 12364.76584576817}

--------

Iteration10-Robot1

--------

{'eval/walltime': 50.866148471832275, 'training/sps': 35150.883926077746, 'training/walltime': 74.57678747177124, 'training/entropy_loss': Array(nan, dtype=float32), 'training/policy_loss': Array(nan, dtype=float32), 'training/total_loss': Array(nan, dtype=float32), 'training/v_loss': Array(nan, dtype=float32), 'eval/episode_distance_from_origin': Array(nan, dtype=float32), 'eval/episode_forward_reward': Array(nan, dtype=float32), 'eval/episode_reward': Array(nan, dtype=float32), 'eval/episode_reward_alive': Array(5000., dtype=float32), 'eval/episode_reward_linvel': Array(nan, dtype=float32), 'eval/episode_reward_quadctrl': Array(nan, dtype=float32), 'eval/episode_x_position': Array(nan, dtype=float32), 'eval/episode_x_velocity': Array(nan, dtype=float32), 'eval/episode_y_position': Array(nan, dtype=float32), 'eval/episode_y_velocity': Array(nan, dtype=float32), 'eval/episode_distance_from_origin_std': Array(nan, dtype=float32), 'eval/episode_forward_reward_std': Array(nan, dtype=float32), 'eval/episode_reward_std': Array(nan, dtype=float32), 'eval/episode_reward_alive_std': Array(0., dtype=float32), 'eval/episode_reward_linvel_std': Array(nan, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(nan, dtype=float32), 'eval/episode_x_position_std': Array(nan, dtype=float32), 'eval/episode_x_velocity_std': Array(nan, dtype=float32), 'eval/episode_y_position_std': Array(nan, dtype=float32), 'eval/episode_y_velocity_std': Array(nan, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/epoch_eval_time': 10.330793380737305, 'eval/sps': 12390.142294266338}
{'eval/walltime': 61.18969225883484, 'training/sps': 50225.552498120385, 'training/walltime': 126.77014064788818, 'training/entropy_loss': Array(nan, dtype=float32), 'training/policy_loss': Array(nan, dtype=float32), 'training/total_loss': Array(nan, dtype=float32), 'training/v_loss': Array(nan, dtype=float32), 'eval/episode_distance_from_origin': Array(nan, dtype=float32), 'eval/episode_forward_reward': Array(nan, dtype=float32), 'eval/episode_reward': Array(nan, dtype=float32), 'eval/episode_reward_alive': Array(5000., dtype=float32), 'eval/episode_reward_linvel': Array(nan, dtype=float32), 'eval/episode_reward_quadctrl': Array(nan, dtype=float32), 'eval/episode_x_position': Array(nan, dtype=float32), 'eval/episode_x_velocity': Array(nan, dtype=float32), 'eval/episode_y_position': Array(nan, dtype=float32), 'eval/episode_y_velocity': Array(nan, dtype=float32), 'eval/episode_distance_from_origin_std': Array(nan, dtype=float32), 'eval/episode_forward_reward_std': Array(nan, dtype=float32), 'eval/episode_reward_std': Array(nan, dtype=float32), 'eval/episode_reward_alive_std': Array(0., dtype=float32), 'eval/episode_reward_linvel_std': Array(nan, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(nan, dtype=float32), 'eval/episode_x_position_std': Array(nan, dtype=float32), 'eval/episode_x_velocity_std': Array(nan, dtype=float32), 'eval/episode_y_position_std': Array(nan, dtype=float32), 'eval/episode_y_velocity_std': Array(nan, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/epoch_eval_time': 10.323543787002563, 'eval/sps': 12398.843133803837}
{'eval/walltime': 71.56062483787537, 'training/sps': 50091.544816426634, 'training/walltime': 179.1031243801117, 'training/entropy_loss': Array(nan, dtype=float32), 'training/policy_loss': Array(nan, dtype=float32), 'training/total_loss': Array(nan, dtype=float32), 'training/v_loss': Array(nan, dtype=float32), 'eval/episode_distance_from_origin': Array(nan, dtype=float32), 'eval/episode_forward_reward': Array(nan, dtype=float32), 'eval/episode_reward': Array(nan, dtype=float32), 'eval/episode_reward_alive': Array(5000., dtype=float32), 'eval/episode_reward_linvel': Array(nan, dtype=float32), 'eval/episode_reward_quadctrl': Array(nan, dtype=float32), 'eval/episode_x_position': Array(nan, dtype=float32), 'eval/episode_x_velocity': Array(nan, dtype=float32), 'eval/episode_y_position': Array(nan, dtype=float32), 'eval/episode_y_velocity': Array(nan, dtype=float32), 'eval/episode_distance_from_origin_std': Array(nan, dtype=float32), 'eval/episode_forward_reward_std': Array(nan, dtype=float32), 'eval/episode_reward_std': Array(nan, dtype=float32), 'eval/episode_reward_alive_std': Array(0., dtype=float32), 'eval/episode_reward_linvel_std': Array(nan, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(nan, dtype=float32), 'eval/episode_x_position_std': Array(nan, dtype=float32), 'eval/episode_x_velocity_std': Array(nan, dtype=float32), 'eval/episode_y_position_std': Array(nan, dtype=float32), 'eval/episode_y_velocity_std': Array(nan, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/epoch_eval_time': 10.370932579040527, 'eval/sps': 12342.188036077465}
{'eval/walltime': 81.89771223068237, 'training/sps': 49958.88682799238, 'training/walltime': 231.57507014274597, 'training/entropy_loss': Array(nan, dtype=float32), 'training/policy_loss': Array(nan, dtype=float32), 'training/total_loss': Array(nan, dtype=float32), 'training/v_loss': Array(nan, dtype=float32), 'eval/episode_distance_from_origin': Array(nan, dtype=float32), 'eval/episode_forward_reward': Array(nan, dtype=float32), 'eval/episode_reward': Array(nan, dtype=float32), 'eval/episode_reward_alive': Array(5000., dtype=float32), 'eval/episode_reward_linvel': Array(nan, dtype=float32), 'eval/episode_reward_quadctrl': Array(nan, dtype=float32), 'eval/episode_x_position': Array(nan, dtype=float32), 'eval/episode_x_velocity': Array(nan, dtype=float32), 'eval/episode_y_position': Array(nan, dtype=float32), 'eval/episode_y_velocity': Array(nan, dtype=float32), 'eval/episode_distance_from_origin_std': Array(nan, dtype=float32), 'eval/episode_forward_reward_std': Array(nan, dtype=float32), 'eval/episode_reward_std': Array(nan, dtype=float32), 'eval/episode_reward_alive_std': Array(0., dtype=float32), 'eval/episode_reward_linvel_std': Array(nan, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(nan, dtype=float32), 'eval/episode_x_position_std': Array(nan, dtype=float32), 'eval/episode_x_velocity_std': Array(nan, dtype=float32), 'eval/episode_y_position_std': Array(nan, dtype=float32), 'eval/episode_y_velocity_std': Array(nan, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/epoch_eval_time': 10.337087392807007, 'eval/sps': 12382.598224821815}

--------

Iteration10-Robot2

--------

{'eval/walltime': 50.99949526786804, 'training/sps': 34578.0357751173, 'training/walltime': 75.8122878074646, 'training/entropy_loss': Array(-0.008, dtype=float32), 'training/policy_loss': Array(0.018, dtype=float32), 'training/total_loss': Array(0.171, dtype=float32), 'training/v_loss': Array(0.161, dtype=float32), 'eval/episode_distance_from_origin': Array(64.379, dtype=float32), 'eval/episode_forward_reward': Array(43.049, dtype=float32), 'eval/episode_reward': Array(323.224, dtype=float32), 'eval/episode_reward_alive': Array(319.102, dtype=float32), 'eval/episode_reward_linvel': Array(43.049, dtype=float32), 'eval/episode_reward_quadctrl': Array(-38.927, dtype=float32), 'eval/episode_x_position': Array(25.305, dtype=float32), 'eval/episode_x_velocity': Array(34.439, dtype=float32), 'eval/episode_y_position': Array(6.608, dtype=float32), 'eval/episode_y_velocity': Array(9.826, dtype=float32), 'eval/episode_distance_from_origin_std': Array(30.748, dtype=float32), 'eval/episode_forward_reward_std': Array(26.957, dtype=float32), 'eval/episode_reward_std': Array(111.394, dtype=float32), 'eval/episode_reward_alive_std': Array(103.95, dtype=float32), 'eval/episode_reward_linvel_std': Array(26.957, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(13.059, dtype=float32), 'eval/episode_x_position_std': Array(24.776, dtype=float32), 'eval/episode_x_velocity_std': Array(21.566, dtype=float32), 'eval/episode_y_position_std': Array(9.027, dtype=float32), 'eval/episode_y_velocity_std': Array(11.642, dtype=float32), 'eval/avg_episode_length': Array(63.82, dtype=float32), 'eval/epoch_eval_time': 10.329912662506104, 'eval/sps': 12391.198665657099}
{'eval/walltime': 61.324676275253296, 'training/sps': 49201.64552798533, 'training/walltime': 129.09180665016174, 'training/entropy_loss': Array(-0.007, dtype=float32), 'training/policy_loss': Array(-0.015, dtype=float32), 'training/total_loss': Array(0.241, dtype=float32), 'training/v_loss': Array(0.262, dtype=float32), 'eval/episode_distance_from_origin': Array(500.405, dtype=float32), 'eval/episode_forward_reward': Array(235.828, dtype=float32), 'eval/episode_reward': Array(990.553, dtype=float32), 'eval/episode_reward_alive': Array(865.977, dtype=float32), 'eval/episode_reward_linvel': Array(235.828, dtype=float32), 'eval/episode_reward_quadctrl': Array(-111.251, dtype=float32), 'eval/episode_x_position': Array(445.211, dtype=float32), 'eval/episode_x_velocity': Array(188.662, dtype=float32), 'eval/episode_y_position': Array(57.909, dtype=float32), 'eval/episode_y_velocity': Array(21.623, dtype=float32), 'eval/episode_distance_from_origin_std': Array(524.595, dtype=float32), 'eval/episode_forward_reward_std': Array(143.469, dtype=float32), 'eval/episode_reward_std': Array(503.289, dtype=float32), 'eval/episode_reward_alive_std': Array(418.906, dtype=float32), 'eval/episode_reward_linvel_std': Array(143.469, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(55.681, dtype=float32), 'eval/episode_x_position_std': Array(515.163, dtype=float32), 'eval/episode_x_velocity_std': Array(114.775, dtype=float32), 'eval/episode_y_position_std': Array(74.534, dtype=float32), 'eval/episode_y_velocity_std': Array(23.464, dtype=float32), 'eval/avg_episode_length': Array(173.195, dtype=float32), 'eval/epoch_eval_time': 10.325181007385254, 'eval/sps': 12396.87710156809}
{'eval/walltime': 71.65698337554932, 'training/sps': 49075.85639737685, 'training/walltime': 182.50788927078247, 'training/entropy_loss': Array(-0.005, dtype=float32), 'training/policy_loss': Array(-0.01, dtype=float32), 'training/total_loss': Array(0.21, dtype=float32), 'training/v_loss': Array(0.224, dtype=float32), 'eval/episode_distance_from_origin': Array(7084.868, dtype=float32), 'eval/episode_forward_reward': Array(1019.449, dtype=float32), 'eval/episode_reward': Array(3380.504, dtype=float32), 'eval/episode_reward_alive': Array(2715.586, dtype=float32), 'eval/episode_reward_linvel': Array(1019.449, dtype=float32), 'eval/episode_reward_quadctrl': Array(-354.531, dtype=float32), 'eval/episode_x_position': Array(6972.691, dtype=float32), 'eval/episode_x_velocity': Array(815.561, dtype=float32), 'eval/episode_y_position': Array(705.416, dtype=float32), 'eval/episode_y_velocity': Array(72.469, dtype=float32), 'eval/episode_distance_from_origin_std': Array(7029.287, dtype=float32), 'eval/episode_forward_reward_std': Array(612.129, dtype=float32), 'eval/episode_reward_std': Array(1913.864, dtype=float32), 'eval/episode_reward_alive_std': Array(1501.498, dtype=float32), 'eval/episode_reward_linvel_std': Array(612.129, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(197.726, dtype=float32), 'eval/episode_x_position_std': Array(6975.165, dtype=float32), 'eval/episode_x_velocity_std': Array(489.705, dtype=float32), 'eval/episode_y_position_std': Array(935.013, dtype=float32), 'eval/episode_y_velocity_std': Array(78.44, dtype=float32), 'eval/avg_episode_length': Array(543.117, dtype=float32), 'eval/epoch_eval_time': 10.33230710029602, 'eval/sps': 12388.32709456853}
{'eval/walltime': 81.99880909919739, 'training/sps': 48988.872469109345, 'training/walltime': 236.01881670951843, 'training/entropy_loss': Array(-0.003, dtype=float32), 'training/policy_loss': Array(-0.008, dtype=float32), 'training/total_loss': Array(0.106, dtype=float32), 'training/v_loss': Array(0.117, dtype=float32), 'eval/episode_distance_from_origin': Array(15632.863, dtype=float32), 'eval/episode_forward_reward': Array(1827.627, dtype=float32), 'eval/episode_reward': Array(5187.976, dtype=float32), 'eval/episode_reward_alive': Array(3858.164, dtype=float32), 'eval/episode_reward_linvel': Array(1827.627, dtype=float32), 'eval/episode_reward_quadctrl': Array(-497.815, dtype=float32), 'eval/episode_x_position': Array(15451.174, dtype=float32), 'eval/episode_x_velocity': Array(1462.106, dtype=float32), 'eval/episode_y_position': Array(1823.847, dtype=float32), 'eval/episode_y_velocity': Array(164.113, dtype=float32), 'eval/episode_distance_from_origin_std': Array(9023.76, dtype=float32), 'eval/episode_forward_reward_std': Array(721.169, dtype=float32), 'eval/episode_reward_std': Array(1986.906, dtype=float32), 'eval/episode_reward_alive_std': Array(1455.511, dtype=float32), 'eval/episode_reward_linvel_std': Array(721.169, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(187.747, dtype=float32), 'eval/episode_x_position_std': Array(8942.486, dtype=float32), 'eval/episode_x_velocity_std': Array(576.938, dtype=float32), 'eval/episode_y_position_std': Array(1347.228, dtype=float32), 'eval/episode_y_velocity_std': Array(95.208, dtype=float32), 'eval/avg_episode_length': Array(771.633, dtype=float32), 'eval/epoch_eval_time': 10.341825723648071, 'eval/sps': 12376.92486997819}
time to jit: 0:00:55.355830
time to train: 0:04:37.578972

--------

Iteration10-Robot3

--------

{'eval/walltime': 51.344512701034546, 'training/sps': 34570.82784280819, 'training/walltime': 75.82809448242188, 'training/entropy_loss': Array(-0.008, dtype=float32), 'training/policy_loss': Array(0.018, dtype=float32), 'training/total_loss': Array(0.203, dtype=float32), 'training/v_loss': Array(0.193, dtype=float32), 'eval/episode_distance_from_origin': Array(87.909, dtype=float32), 'eval/episode_forward_reward': Array(55.057, dtype=float32), 'eval/episode_reward': Array(396.925, dtype=float32), 'eval/episode_reward_alive': Array(395.078, dtype=float32), 'eval/episode_reward_linvel': Array(55.057, dtype=float32), 'eval/episode_reward_quadctrl': Array(-53.21, dtype=float32), 'eval/episode_x_position': Array(44.382, dtype=float32), 'eval/episode_x_velocity': Array(44.045, dtype=float32), 'eval/episode_y_position': Array(9.86, dtype=float32), 'eval/episode_y_velocity': Array(10.701, dtype=float32), 'eval/episode_distance_from_origin_std': Array(56.596, dtype=float32), 'eval/episode_forward_reward_std': Array(36.318, dtype=float32), 'eval/episode_reward_std': Array(156.408, dtype=float32), 'eval/episode_reward_alive_std': Array(149.279, dtype=float32), 'eval/episode_reward_linvel_std': Array(36.318, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(20.278, dtype=float32), 'eval/episode_x_position_std': Array(49.653, dtype=float32), 'eval/episode_x_velocity_std': Array(29.054, dtype=float32), 'eval/episode_y_position_std': Array(16.319, dtype=float32), 'eval/episode_y_velocity_std': Array(12.81, dtype=float32), 'eval/avg_episode_length': Array(79.016, dtype=float32), 'eval/epoch_eval_time': 10.30359148979187, 'eval/sps': 12422.852762244514}
{'eval/walltime': 61.69122290611267, 'training/sps': 49176.0037332757, 'training/walltime': 129.13539481163025, 'training/entropy_loss': Array(-0.006, dtype=float32), 'training/policy_loss': Array(-0.013, dtype=float32), 'training/total_loss': Array(0.243, dtype=float32), 'training/v_loss': Array(0.262, dtype=float32), 'eval/episode_distance_from_origin': Array(1186.061, dtype=float32), 'eval/episode_forward_reward': Array(343.536, dtype=float32), 'eval/episode_reward': Array(1431.785, dtype=float32), 'eval/episode_reward_alive': Array(1265.742, dtype=float32), 'eval/episode_reward_linvel': Array(343.536, dtype=float32), 'eval/episode_reward_quadctrl': Array(-177.494, dtype=float32), 'eval/episode_x_position': Array(1114.299, dtype=float32), 'eval/episode_x_velocity': Array(274.829, dtype=float32), 'eval/episode_y_position': Array(149.313, dtype=float32), 'eval/episode_y_velocity': Array(32.054, dtype=float32), 'eval/episode_distance_from_origin_std': Array(1792.51, dtype=float32), 'eval/episode_forward_reward_std': Array(247.818, dtype=float32), 'eval/episode_reward_std': Array(923.303, dtype=float32), 'eval/episode_reward_alive_std': Array(792.448, dtype=float32), 'eval/episode_reward_linvel_std': Array(247.818, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(111.182, dtype=float32), 'eval/episode_x_position_std': Array(1772.136, dtype=float32), 'eval/episode_x_velocity_std': Array(198.255, dtype=float32), 'eval/episode_y_position_std': Array(244.891, dtype=float32), 'eval/episode_y_velocity_std': Array(43.144, dtype=float32), 'eval/avg_episode_length': Array(253.148, dtype=float32), 'eval/epoch_eval_time': 10.346710205078125, 'eval/sps': 12371.081963538332}
{'eval/walltime': 72.02700304985046, 'training/sps': 49008.91375019837, 'training/walltime': 182.6244399547577, 'training/entropy_loss': Array(-0.004, dtype=float32), 'training/policy_loss': Array(-0.009, dtype=float32), 'training/total_loss': Array(0.178, dtype=float32), 'training/v_loss': Array(0.19, dtype=float32), 'eval/episode_distance_from_origin': Array(8024.426, dtype=float32), 'eval/episode_forward_reward': Array(1074.444, dtype=float32), 'eval/episode_reward': Array(3475.064, dtype=float32), 'eval/episode_reward_alive': Array(2793.789, dtype=float32), 'eval/episode_reward_linvel': Array(1074.444, dtype=float32), 'eval/episode_reward_quadctrl': Array(-393.168, dtype=float32), 'eval/episode_x_position': Array(7881.964, dtype=float32), 'eval/episode_x_velocity': Array(859.557, dtype=float32), 'eval/episode_y_position': Array(1078.167, dtype=float32), 'eval/episode_y_velocity': Array(107.315, dtype=float32), 'eval/episode_distance_from_origin_std': Array(7637.221, dtype=float32), 'eval/episode_forward_reward_std': Array(676.486, dtype=float32), 'eval/episode_reward_std': Array(2110.502, dtype=float32), 'eval/episode_reward_alive_std': Array(1670.729, dtype=float32), 'eval/episode_reward_linvel_std': Array(676.486, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(234.468, dtype=float32), 'eval/episode_x_position_std': Array(7554.021, dtype=float32), 'eval/episode_x_velocity_std': Array(541.191, dtype=float32), 'eval/episode_y_position_std': Array(1141.381, dtype=float32), 'eval/episode_y_velocity_std': Array(87.983, dtype=float32), 'eval/avg_episode_length': Array(558.758, dtype=float32), 'eval/epoch_eval_time': 10.335780143737793, 'eval/sps': 12384.1643514014}
{'eval/walltime': 82.36667776107788, 'training/sps': 48968.59785815177, 'training/walltime': 236.15752267837524, 'training/entropy_loss': Array(-0.003, dtype=float32), 'training/policy_loss': Array(-0.007, dtype=float32), 'training/total_loss': Array(0.114, dtype=float32), 'training/v_loss': Array(0.124, dtype=float32), 'eval/episode_distance_from_origin': Array(14104.912, dtype=float32), 'eval/episode_forward_reward': Array(1652.272, dtype=float32), 'eval/episode_reward': Array(4649.008, dtype=float32), 'eval/episode_reward_alive': Array(3488.75, dtype=float32), 'eval/episode_reward_linvel': Array(1652.272, dtype=float32), 'eval/episode_reward_quadctrl': Array(-492.014, dtype=float32), 'eval/episode_x_position': Array(13830.866, dtype=float32), 'eval/episode_x_velocity': Array(1321.822, dtype=float32), 'eval/episode_y_position': Array(2403.171, dtype=float32), 'eval/episode_y_velocity': Array(224.503, dtype=float32), 'eval/episode_distance_from_origin_std': Array(10080.95, dtype=float32), 'eval/episode_forward_reward_std': Array(850.445, dtype=float32), 'eval/episode_reward_std': Array(2314.535, dtype=float32), 'eval/episode_reward_alive_std': Array(1707.224, dtype=float32), 'eval/episode_reward_linvel_std': Array(850.445, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(241.563, dtype=float32), 'eval/episode_x_position_std': Array(9921.567, dtype=float32), 'eval/episode_x_velocity_std': Array(680.359, dtype=float32), 'eval/episode_y_position_std': Array(1831.78, dtype=float32), 'eval/episode_y_velocity_std': Array(126.473, dtype=float32), 'eval/avg_episode_length': Array(697.75, dtype=float32), 'eval/epoch_eval_time': 10.339674711227417, 'eval/sps': 12379.499701379405}
time to jit: 0:00:55.877995
time to train: 0:04:37.727949
11

--------

Iteration11-Robot0

--------

{'eval/walltime': 50.798359394073486, 'training/sps': 34390.24079207053, 'training/walltime': 76.22627639770508, 'training/entropy_loss': Array(-0.003, dtype=float32), 'training/policy_loss': Array(-0.008, dtype=float32), 'training/total_loss': Array(0.153, dtype=float32), 'training/v_loss': Array(0.164, dtype=float32), 'eval/episode_distance_from_origin': Array(11260.973, dtype=float32), 'eval/episode_forward_reward': Array(1433.364, dtype=float32), 'eval/episode_reward': Array(4197.736, dtype=float32), 'eval/episode_reward_alive': Array(3241.016, dtype=float32), 'eval/episode_reward_linvel': Array(1433.364, dtype=float32), 'eval/episode_reward_quadctrl': Array(-476.642, dtype=float32), 'eval/episode_x_position': Array(11082.227, dtype=float32), 'eval/episode_x_velocity': Array(1146.694, dtype=float32), 'eval/episode_y_position': Array(1500.245, dtype=float32), 'eval/episode_y_velocity': Array(148.716, dtype=float32), 'eval/episode_distance_from_origin_std': Array(8841.79, dtype=float32), 'eval/episode_forward_reward_std': Array(753.645, dtype=float32), 'eval/episode_reward_std': Array(2106.421, dtype=float32), 'eval/episode_reward_alive_std': Array(1591.529, dtype=float32), 'eval/episode_reward_linvel_std': Array(753.645, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(236.605, dtype=float32), 'eval/episode_x_position_std': Array(8748.553, dtype=float32), 'eval/episode_x_velocity_std': Array(602.918, dtype=float32), 'eval/episode_y_position_std': Array(1367.054, dtype=float32), 'eval/episode_y_velocity_std': Array(95.932, dtype=float32), 'eval/avg_episode_length': Array(648.203, dtype=float32), 'eval/epoch_eval_time': 10.337563276290894, 'eval/sps': 12382.028199388807}
{'eval/walltime': 61.14414858818054, 'training/sps': 49333.21792201441, 'training/walltime': 129.36369800567627, 'training/entropy_loss': Array(-0.001, dtype=float32), 'training/policy_loss': Array(-0.005, dtype=float32), 'training/total_loss': Array(0.116, dtype=float32), 'training/v_loss': Array(0.122, dtype=float32), 'eval/episode_distance_from_origin': Array(17667.96, dtype=float32), 'eval/episode_forward_reward': Array(2058.388, dtype=float32), 'eval/episode_reward': Array(5313.95, dtype=float32), 'eval/episode_reward_alive': Array(3824.453, dtype=float32), 'eval/episode_reward_linvel': Array(2058.388, dtype=float32), 'eval/episode_reward_quadctrl': Array(-568.891, dtype=float32), 'eval/episode_x_position': Array(17458.469, dtype=float32), 'eval/episode_x_velocity': Array(1646.716, dtype=float32), 'eval/episode_y_position': Array(2186.301, dtype=float32), 'eval/episode_y_velocity': Array(197.306, dtype=float32), 'eval/episode_distance_from_origin_std': Array(10317.331, dtype=float32), 'eval/episode_forward_reward_std': Array(856.937, dtype=float32), 'eval/episode_reward_std': Array(2133.632, dtype=float32), 'eval/episode_reward_alive_std': Array(1503.585, dtype=float32), 'eval/episode_reward_linvel_std': Array(856.937, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(225.168, dtype=float32), 'eval/episode_x_position_std': Array(10232.201, dtype=float32), 'eval/episode_x_velocity_std': Array(685.553, dtype=float32), 'eval/episode_y_position_std': Array(1478.219, dtype=float32), 'eval/episode_y_velocity_std': Array(100.359, dtype=float32), 'eval/avg_episode_length': Array(764.891, dtype=float32), 'eval/epoch_eval_time': 10.345789194107056, 'eval/sps': 12372.183271712958}
{'eval/walltime': 71.51383924484253, 'training/sps': 49085.62802724988, 'training/walltime': 182.7691469192505, 'training/entropy_loss': Array(0.001, dtype=float32), 'training/policy_loss': Array(-0.005, dtype=float32), 'training/total_loss': Array(0.102, dtype=float32), 'training/v_loss': Array(0.106, dtype=float32), 'eval/episode_distance_from_origin': Array(19947.32, dtype=float32), 'eval/episode_forward_reward': Array(2390.47, dtype=float32), 'eval/episode_reward': Array(5526.776, dtype=float32), 'eval/episode_reward_alive': Array(3691.172, dtype=float32), 'eval/episode_reward_linvel': Array(2390.47, dtype=float32), 'eval/episode_reward_quadctrl': Array(-554.864, dtype=float32), 'eval/episode_x_position': Array(19720.002, dtype=float32), 'eval/episode_x_velocity': Array(1912.382, dtype=float32), 'eval/episode_y_position': Array(2519.465, dtype=float32), 'eval/episode_y_velocity': Array(230.278, dtype=float32), 'eval/episode_distance_from_origin_std': Array(12390.337, dtype=float32), 'eval/episode_forward_reward_std': Array(1039.945, dtype=float32), 'eval/episode_reward_std': Array(2315.427, dtype=float32), 'eval/episode_reward_alive_std': Array(1504.384, dtype=float32), 'eval/episode_reward_linvel_std': Array(1039.945, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(228.141, dtype=float32), 'eval/episode_x_position_std': Array(12285.664, dtype=float32), 'eval/episode_x_velocity_std': Array(831.959, dtype=float32), 'eval/episode_y_position_std': Array(1725.585, dtype=float32), 'eval/episode_y_velocity_std': Array(119.107, dtype=float32), 'eval/avg_episode_length': Array(738.234, dtype=float32), 'eval/epoch_eval_time': 10.369690656661987, 'eval/sps': 12343.6661939155}
{'eval/walltime': 81.8609299659729, 'training/sps': 49020.77338304393, 'training/walltime': 236.24525141716003, 'training/entropy_loss': Array(0.002, dtype=float32), 'training/policy_loss': Array(-0.004, dtype=float32), 'training/total_loss': Array(0.131, dtype=float32), 'training/v_loss': Array(0.132, dtype=float32), 'eval/episode_distance_from_origin': Array(24460.438, dtype=float32), 'eval/episode_forward_reward': Array(2817.446, dtype=float32), 'eval/episode_reward': Array(6161.281, dtype=float32), 'eval/episode_reward_alive': Array(3920.859, dtype=float32), 'eval/episode_reward_linvel': Array(2817.446, dtype=float32), 'eval/episode_reward_quadctrl': Array(-577.022, dtype=float32), 'eval/episode_x_position': Array(24238.137, dtype=float32), 'eval/episode_x_velocity': Array(2253.963, dtype=float32), 'eval/episode_y_position': Array(2723.958, dtype=float32), 'eval/episode_y_velocity': Array(232.277, dtype=float32), 'eval/episode_distance_from_origin_std': Array(13836.537, dtype=float32), 'eval/episode_forward_reward_std': Array(1146.558, dtype=float32), 'eval/episode_reward_std': Array(2420.274, dtype=float32), 'eval/episode_reward_alive_std': Array(1496.182, dtype=float32), 'eval/episode_reward_linvel_std': Array(1146.558, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(221.926, dtype=float32), 'eval/episode_x_position_std': Array(13740.579, dtype=float32), 'eval/episode_x_velocity_std': Array(917.25, dtype=float32), 'eval/episode_y_position_std': Array(1827.596, dtype=float32), 'eval/episode_y_velocity_std': Array(117.628, dtype=float32), 'eval/avg_episode_length': Array(784.172, dtype=float32), 'eval/epoch_eval_time': 10.347090721130371, 'eval/sps': 12370.627014857815}
time to jit: 0:00:54.146820
time to train: 0:04:37.901157

--------

Iteration11-Robot1

--------

{'eval/walltime': 51.82269811630249, 'training/sps': 34299.40036347933, 'training/walltime': 76.42815828323364, 'training/entropy_loss': Array(-0.001, dtype=float32), 'training/policy_loss': Array(-0.008, dtype=float32), 'training/total_loss': Array(0.414, dtype=float32), 'training/v_loss': Array(0.424, dtype=float32), 'eval/episode_distance_from_origin': Array(10057.739, dtype=float32), 'eval/episode_forward_reward': Array(1446.803, dtype=float32), 'eval/episode_reward': Array(3854.107, dtype=float32), 'eval/episode_reward_alive': Array(2817.07, dtype=float32), 'eval/episode_reward_linvel': Array(1446.803, dtype=float32), 'eval/episode_reward_quadctrl': Array(-409.765, dtype=float32), 'eval/episode_x_position': Array(9983.741, dtype=float32), 'eval/episode_x_velocity': Array(1157.445, dtype=float32), 'eval/episode_y_position': Array(384.067, dtype=float32), 'eval/episode_y_velocity': Array(24.518, dtype=float32), 'eval/episode_distance_from_origin_std': Array(9493.583, dtype=float32), 'eval/episode_forward_reward_std': Array(838.778, dtype=float32), 'eval/episode_reward_std': Array(2130.006, dtype=float32), 'eval/episode_reward_alive_std': Array(1517.22, dtype=float32), 'eval/episode_reward_linvel_std': Array(838.778, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(223.527, dtype=float32), 'eval/episode_x_position_std': Array(9470.689, dtype=float32), 'eval/episode_x_velocity_std': Array(671.024, dtype=float32), 'eval/episode_y_position_std': Array(745.797, dtype=float32), 'eval/episode_y_velocity_std': Array(65.876, dtype=float32), 'eval/avg_episode_length': Array(563.414, dtype=float32), 'eval/epoch_eval_time': 10.35347032546997, 'eval/sps': 12363.004478325942}
{'eval/walltime': 62.18812322616577, 'training/sps': 49210.467806454086, 'training/walltime': 129.69812536239624, 'training/entropy_loss': Array(0., dtype=float32), 'training/policy_loss': Array(-0.005, dtype=float32), 'training/total_loss': Array(0.194, dtype=float32), 'training/v_loss': Array(0.199, dtype=float32), 'eval/episode_distance_from_origin': Array(16570.633, dtype=float32), 'eval/episode_forward_reward': Array(2110.891, dtype=float32), 'eval/episode_reward': Array(4931.458, dtype=float32), 'eval/episode_reward_alive': Array(3321.016, dtype=float32), 'eval/episode_reward_linvel': Array(2110.891, dtype=float32), 'eval/episode_reward_quadctrl': Array(-500.448, dtype=float32), 'eval/episode_x_position': Array(16502.062, dtype=float32), 'eval/episode_x_velocity': Array(1688.717, dtype=float32), 'eval/episode_y_position': Array(248.815, dtype=float32), 'eval/episode_y_velocity': Array(14.8, dtype=float32), 'eval/episode_distance_from_origin_std': Array(12696.251, dtype=float32), 'eval/episode_forward_reward_std': Array(1087.682, dtype=float32), 'eval/episode_reward_std': Array(2440.287, dtype=float32), 'eval/episode_reward_alive_std': Array(1598.016, dtype=float32), 'eval/episode_reward_linvel_std': Array(1087.682, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(244.329, dtype=float32), 'eval/episode_x_position_std': Array(12682.856, dtype=float32), 'eval/episode_x_velocity_std': Array(870.149, dtype=float32), 'eval/episode_y_position_std': Array(771.359, dtype=float32), 'eval/episode_y_velocity_std': Array(69.601, dtype=float32), 'eval/avg_episode_length': Array(664.203, dtype=float32), 'eval/epoch_eval_time': 10.365425109863281, 'eval/sps': 12348.745820198039}
{'eval/walltime': 72.55542588233948, 'training/sps': 49007.49780382319, 'training/walltime': 183.18871593475342, 'training/entropy_loss': Array(0.002, dtype=float32), 'training/policy_loss': Array(-0.004, dtype=float32), 'training/total_loss': Array(0.203, dtype=float32), 'training/v_loss': Array(0.205, dtype=float32), 'eval/episode_distance_from_origin': Array(23032.082, dtype=float32), 'eval/episode_forward_reward': Array(2718.999, dtype=float32), 'eval/episode_reward': Array(5869.859, dtype=float32), 'eval/episode_reward_alive': Array(3719.102, dtype=float32), 'eval/episode_reward_linvel': Array(2718.999, dtype=float32), 'eval/episode_reward_quadctrl': Array(-568.239, dtype=float32), 'eval/episode_x_position': Array(22943.639, dtype=float32), 'eval/episode_x_velocity': Array(2175.206, dtype=float32), 'eval/episode_y_position': Array(978.901, dtype=float32), 'eval/episode_y_velocity': Array(78.455, dtype=float32), 'eval/episode_distance_from_origin_std': Array(14599.263, dtype=float32), 'eval/episode_forward_reward_std': Array(1247.282, dtype=float32), 'eval/episode_reward_std': Array(2594.894, dtype=float32), 'eval/episode_reward_alive_std': Array(1596.272, dtype=float32), 'eval/episode_reward_linvel_std': Array(1247.282, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(247.901, dtype=float32), 'eval/episode_x_position_std': Array(14577.099, dtype=float32), 'eval/episode_x_velocity_std': Array(997.829, dtype=float32), 'eval/episode_y_position_std': Array(1071.817, dtype=float32), 'eval/episode_y_velocity_std': Array(69.778, dtype=float32), 'eval/avg_episode_length': Array(743.82, dtype=float32), 'eval/epoch_eval_time': 10.367302656173706, 'eval/sps': 12346.50942921747}
{'eval/walltime': 82.90932059288025, 'training/sps': 48886.275611424564, 'training/walltime': 236.81194591522217, 'training/entropy_loss': Array(0.004, dtype=float32), 'training/policy_loss': Array(-0.003, dtype=float32), 'training/total_loss': Array(0.164, dtype=float32), 'training/v_loss': Array(0.164, dtype=float32), 'eval/episode_distance_from_origin': Array(28208.055, dtype=float32), 'eval/episode_forward_reward': Array(3267.128, dtype=float32), 'eval/episode_reward': Array(6668.742, dtype=float32), 'eval/episode_reward_alive': Array(4009.844, dtype=float32), 'eval/episode_reward_linvel': Array(3267.128, dtype=float32), 'eval/episode_reward_quadctrl': Array(-608.227, dtype=float32), 'eval/episode_x_position': Array(28122.543, dtype=float32), 'eval/episode_x_velocity': Array(2613.71, dtype=float32), 'eval/episode_y_position': Array(1149.603, dtype=float32), 'eval/episode_y_velocity': Array(79.183, dtype=float32), 'eval/episode_distance_from_origin_std': Array(14544.902, dtype=float32), 'eval/episode_forward_reward_std': Array(1223.807, dtype=float32), 'eval/episode_reward_std': Array(2428.974, dtype=float32), 'eval/episode_reward_alive_std': Array(1423.361, dtype=float32), 'eval/episode_reward_linvel_std': Array(1223.807, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(217.56, dtype=float32), 'eval/episode_x_position_std': Array(14525.09, dtype=float32), 'eval/episode_x_velocity_std': Array(979.049, dtype=float32), 'eval/episode_y_position_std': Array(1031.441, dtype=float32), 'eval/episode_y_velocity_std': Array(75.463, dtype=float32), 'eval/avg_episode_length': Array(801.969, dtype=float32), 'eval/epoch_eval_time': 10.353894710540771, 'eval/sps': 12362.49774393492}
time to jit: 0:00:55.827875
time to train: 0:04:38.508877

--------

Iteration11-Robot2

--------

{'eval/walltime': 51.5966272354126, 'training/sps': 35308.71106516771, 'training/walltime': 74.24343514442444, 'training/entropy_loss': Array(-0.003, dtype=float32), 'training/policy_loss': Array(-0.012, dtype=float32), 'training/total_loss': Array(0.329, dtype=float32), 'training/v_loss': Array(0.343, dtype=float32), 'eval/episode_distance_from_origin': Array(2730.026, dtype=float32), 'eval/episode_forward_reward': Array(631.192, dtype=float32), 'eval/episode_reward': Array(1934.01, dtype=float32), 'eval/episode_reward_alive': Array(1548.516, dtype=float32), 'eval/episode_reward_linvel': Array(631.192, dtype=float32), 'eval/episode_reward_quadctrl': Array(-245.697, dtype=float32), 'eval/episode_x_position': Array(2664.69, dtype=float32), 'eval/episode_x_velocity': Array(504.954, dtype=float32), 'eval/episode_y_position': Array(70.695, dtype=float32), 'eval/episode_y_velocity': Array(5.068, dtype=float32), 'eval/episode_distance_from_origin_std': Array(3837.934, dtype=float32), 'eval/episode_forward_reward_std': Array(478.556, dtype=float32), 'eval/episode_reward_std': Array(1334.951, dtype=float32), 'eval/episode_reward_alive_std': Array(1025.963, dtype=float32), 'eval/episode_reward_linvel_std': Array(478.556, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(166.482, dtype=float32), 'eval/episode_x_position_std': Array(3812.998, dtype=float32), 'eval/episode_x_velocity_std': Array(382.845, dtype=float32), 'eval/episode_y_position_std': Array(406.063, dtype=float32), 'eval/episode_y_velocity_std': Array(48.903, dtype=float32), 'eval/avg_episode_length': Array(309.703, dtype=float32), 'eval/epoch_eval_time': 10.318255424499512, 'eval/sps': 12405.197849248692}
time to jit: 0:00:55.760776
time to train: 0:01:24.644737

--------

Iteration11-Robot3

--------

{'eval/walltime': 51.79984092712402, 'training/sps': 35258.84192216385, 'training/walltime': 74.34844303131104, 'training/entropy_loss': Array(-0.001, dtype=float32), 'training/policy_loss': Array(-0.008, dtype=float32), 'training/total_loss': Array(0.388, dtype=float32), 'training/v_loss': Array(0.397, dtype=float32), 'eval/episode_distance_from_origin': Array(18885.916, dtype=float32), 'eval/episode_forward_reward': Array(2219.614, dtype=float32), 'eval/episode_reward': Array(5616.812, dtype=float32), 'eval/episode_reward_alive': Array(3900.195, dtype=float32), 'eval/episode_reward_linvel': Array(2219.614, dtype=float32), 'eval/episode_reward_quadctrl': Array(-502.996, dtype=float32), 'eval/episode_x_position': Array(18788.344, dtype=float32), 'eval/episode_x_velocity': Array(1775.696, dtype=float32), 'eval/episode_y_position': Array(1079.754, dtype=float32), 'eval/episode_y_velocity': Array(89.843, dtype=float32), 'eval/episode_distance_from_origin_std': Array(10674.062, dtype=float32), 'eval/episode_forward_reward_std': Array(836.011, dtype=float32), 'eval/episode_reward_std': Array(2051.065, dtype=float32), 'eval/episode_reward_alive_std': Array(1397.583, dtype=float32), 'eval/episode_reward_linvel_std': Array(836.011, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(181.195, dtype=float32), 'eval/episode_x_position_std': Array(10648.72, dtype=float32), 'eval/episode_x_velocity_std': Array(668.812, dtype=float32), 'eval/episode_y_position_std': Array(955.264, dtype=float32), 'eval/episode_y_velocity_std': Array(72.671, dtype=float32), 'eval/avg_episode_length': Array(780.039, dtype=float32), 'eval/epoch_eval_time': 10.360486268997192, 'eval/sps': 12354.632463828295}
{'eval/walltime': 62.16381287574768, 'training/sps': 49308.449593393016, 'training/walltime': 127.51255631446838, 'training/entropy_loss': Array(-0.001, dtype=float32), 'training/policy_loss': Array(-0.006, dtype=float32), 'training/total_loss': Array(0.112, dtype=float32), 'training/v_loss': Array(0.118, dtype=float32), 'eval/episode_distance_from_origin': Array(25706.738, dtype=float32), 'eval/episode_forward_reward': Array(2849.229, dtype=float32), 'eval/episode_reward': Array(6665.398, dtype=float32), 'eval/episode_reward_alive': Array(4379.023, dtype=float32), 'eval/episode_reward_linvel': Array(2849.229, dtype=float32), 'eval/episode_reward_quadctrl': Array(-562.852, dtype=float32), 'eval/episode_x_position': Array(25496.396, dtype=float32), 'eval/episode_x_velocity': Array(2279.39, dtype=float32), 'eval/episode_y_position': Array(2704.566, dtype=float32), 'eval/episode_y_velocity': Array(240.39, dtype=float32), 'eval/episode_distance_from_origin_std': Array(10208.077, dtype=float32), 'eval/episode_forward_reward_std': Array(783.605, dtype=float32), 'eval/episode_reward_std': Array(1779.134, dtype=float32), 'eval/episode_reward_alive_std': Array(1146.123, dtype=float32), 'eval/episode_reward_linvel_std': Array(783.605, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(149.019, dtype=float32), 'eval/episode_x_position_std': Array(10149.355, dtype=float32), 'eval/episode_x_velocity_std': Array(626.886, dtype=float32), 'eval/episode_y_position_std': Array(1354.543, dtype=float32), 'eval/episode_y_velocity_std': Array(91.154, dtype=float32), 'eval/avg_episode_length': Array(875.805, dtype=float32), 'eval/epoch_eval_time': 10.363971948623657, 'eval/sps': 12350.477272084714}
{'eval/walltime': 72.54532217979431, 'training/sps': 49091.925418531806, 'training/walltime': 180.9111545085907, 'training/entropy_loss': Array(0.001, dtype=float32), 'training/policy_loss': Array(-0.004, dtype=float32), 'training/total_loss': Array(0.094, dtype=float32), 'training/v_loss': Array(0.098, dtype=float32), 'eval/episode_distance_from_origin': Array(29165.518, dtype=float32), 'eval/episode_forward_reward': Array(3259.427, dtype=float32), 'eval/episode_reward': Array(7034.339, dtype=float32), 'eval/episode_reward_alive': Array(4336.602, dtype=float32), 'eval/episode_reward_linvel': Array(3259.427, dtype=float32), 'eval/episode_reward_quadctrl': Array(-561.686, dtype=float32), 'eval/episode_x_position': Array(28872.03, dtype=float32), 'eval/episode_x_velocity': Array(2607.549, dtype=float32), 'eval/episode_y_position': Array(3692.229, dtype=float32), 'eval/episode_y_velocity': Array(320.694, dtype=float32), 'eval/episode_distance_from_origin_std': Array(11843.472, dtype=float32), 'eval/episode_forward_reward_std': Array(886.408, dtype=float32), 'eval/episode_reward_std': Array(1860.987, dtype=float32), 'eval/episode_reward_alive_std': Array(1121.843, dtype=float32), 'eval/episode_reward_linvel_std': Array(886.408, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(146.074, dtype=float32), 'eval/episode_x_position_std': Array(11744.283, dtype=float32), 'eval/episode_x_velocity_std': Array(709.13, dtype=float32), 'eval/episode_y_position_std': Array(1662.176, dtype=float32), 'eval/episode_y_velocity_std': Array(100.416, dtype=float32), 'eval/avg_episode_length': Array(867.32, dtype=float32), 'eval/epoch_eval_time': 10.38150930404663, 'eval/sps': 12329.613763396292}
{'eval/walltime': 82.91314196586609, 'training/sps': 49011.12389975147, 'training/walltime': 234.39778757095337, 'training/entropy_loss': Array(0.002, dtype=float32), 'training/policy_loss': Array(-0.004, dtype=float32), 'training/total_loss': Array(0.089, dtype=float32), 'training/v_loss': Array(0.09, dtype=float32), 'eval/episode_distance_from_origin': Array(36355.1, dtype=float32), 'eval/episode_forward_reward': Array(3902.938, dtype=float32), 'eval/episode_reward': Array(7930.462, dtype=float32), 'eval/episode_reward_alive': Array(4623.633, dtype=float32), 'eval/episode_reward_linvel': Array(3902.938, dtype=float32), 'eval/episode_reward_quadctrl': Array(-596.105, dtype=float32), 'eval/episode_x_position': Array(36067.887, dtype=float32), 'eval/episode_x_velocity': Array(3122.36, dtype=float32), 'eval/episode_y_position': Array(4089.165, dtype=float32), 'eval/episode_y_velocity': Array(343.052, dtype=float32), 'eval/episode_distance_from_origin_std': Array(10951.832, dtype=float32), 'eval/episode_forward_reward_std': Array(862.513, dtype=float32), 'eval/episode_reward_std': Array(1706.823, dtype=float32), 'eval/episode_reward_alive_std': Array(970.28, dtype=float32), 'eval/episode_reward_linvel_std': Array(862.513, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(124.774, dtype=float32), 'eval/episode_x_position_std': Array(10878.185, dtype=float32), 'eval/episode_x_velocity_std': Array(690.014, dtype=float32), 'eval/episode_y_position_std': Array(1476.107, dtype=float32), 'eval/episode_y_velocity_std': Array(104.915, dtype=float32), 'eval/avg_episode_length': Array(924.727, dtype=float32), 'eval/epoch_eval_time': 10.367819786071777, 'eval/sps': 12345.893605515439}
time to jit: 0:00:56.193433
time to train: 0:04:36.132176
12

--------

Iteration12-Robot0

--------

{'eval/walltime': 51.606916189193726, 'training/sps': 35557.053589171395, 'training/walltime': 73.72489380836487, 'training/entropy_loss': Array(0.005, dtype=float32), 'training/policy_loss': Array(-0.007, dtype=float32), 'training/total_loss': Array(0.769, dtype=float32), 'training/v_loss': Array(0.771, dtype=float32), 'eval/episode_distance_from_origin': Array(12737.612, dtype=float32), 'eval/episode_forward_reward': Array(1732.53, dtype=float32), 'eval/episode_reward': Array(3927.653, dtype=float32), 'eval/episode_reward_alive': Array(2561.953, dtype=float32), 'eval/episode_reward_linvel': Array(1732.53, dtype=float32), 'eval/episode_reward_quadctrl': Array(-366.83, dtype=float32), 'eval/episode_x_position': Array(12407.219, dtype=float32), 'eval/episode_x_velocity': Array(1386.028, dtype=float32), 'eval/episode_y_position': Array(2648.171, dtype=float32), 'eval/episode_y_velocity': Array(302.182, dtype=float32), 'eval/episode_distance_from_origin_std': Array(13490.119, dtype=float32), 'eval/episode_forward_reward_std': Array(1252.116, dtype=float32), 'eval/episode_reward_std': Array(2704.038, dtype=float32), 'eval/episode_reward_alive_std': Array(1699.468, dtype=float32), 'eval/episode_reward_linvel_std': Array(1252.116, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(247.035, dtype=float32), 'eval/episode_x_position_std': Array(13183.27, dtype=float32), 'eval/episode_x_velocity_std': Array(1001.696, dtype=float32), 'eval/episode_y_position_std': Array(2862.917, dtype=float32), 'eval/episode_y_velocity_std': Array(222.564, dtype=float32), 'eval/avg_episode_length': Array(512.391, dtype=float32), 'eval/epoch_eval_time': 10.347287893295288, 'eval/sps': 12370.39128706759}
{'eval/walltime': 61.95374655723572, 'training/sps': 49271.686263124306, 'training/walltime': 126.92867469787598, 'training/entropy_loss': Array(0.005, dtype=float32), 'training/policy_loss': Array(-0.003, dtype=float32), 'training/total_loss': Array(0.19, dtype=float32), 'training/v_loss': Array(0.188, dtype=float32), 'eval/episode_distance_from_origin': Array(30356.082, dtype=float32), 'eval/episode_forward_reward': Array(3313.596, dtype=float32), 'eval/episode_reward': Array(6933.347, dtype=float32), 'eval/episode_reward_alive': Array(4207.188, dtype=float32), 'eval/episode_reward_linvel': Array(3313.596, dtype=float32), 'eval/episode_reward_quadctrl': Array(-587.434, dtype=float32), 'eval/episode_x_position': Array(29804.684, dtype=float32), 'eval/episode_x_velocity': Array(2650.885, dtype=float32), 'eval/episode_y_position': Array(5438.879, dtype=float32), 'eval/episode_y_velocity': Array(488.726, dtype=float32), 'eval/episode_distance_from_origin_std': Array(13748.271, dtype=float32), 'eval/episode_forward_reward_std': Array(1185.885, dtype=float32), 'eval/episode_reward_std': Array(2412.764, dtype=float32), 'eval/episode_reward_alive_std': Array(1427.849, dtype=float32), 'eval/episode_reward_linvel_std': Array(1185.885, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(200.363, dtype=float32), 'eval/episode_x_position_std': Array(13518.388, dtype=float32), 'eval/episode_x_velocity_std': Array(948.712, dtype=float32), 'eval/episode_y_position_std': Array(2607.341, dtype=float32), 'eval/episode_y_velocity_std': Array(180.726, dtype=float32), 'eval/avg_episode_length': Array(841.438, dtype=float32), 'eval/epoch_eval_time': 10.346830368041992, 'eval/sps': 12370.9382919189}
{'eval/walltime': 72.32079291343689, 'training/sps': 49185.0372788221, 'training/walltime': 180.22618436813354, 'training/entropy_loss': Array(0.006, dtype=float32), 'training/policy_loss': Array(-0., dtype=float32), 'training/total_loss': Array(0.132, dtype=float32), 'training/v_loss': Array(0.126, dtype=float32), 'eval/episode_distance_from_origin': Array(32813.29, dtype=float32), 'eval/episode_forward_reward': Array(3672.921, dtype=float32), 'eval/episode_reward': Array(7033.439, dtype=float32), 'eval/episode_reward_alive': Array(3915.625, dtype=float32), 'eval/episode_reward_linvel': Array(3672.921, dtype=float32), 'eval/episode_reward_quadctrl': Array(-555.104, dtype=float32), 'eval/episode_x_position': Array(32314.037, dtype=float32), 'eval/episode_x_velocity': Array(2938.346, dtype=float32), 'eval/episode_y_position': Array(5335.485, dtype=float32), 'eval/episode_y_velocity': Array(503.945, dtype=float32), 'eval/episode_distance_from_origin_std': Array(18708.928, dtype=float32), 'eval/episode_forward_reward_std': Array(1623.5, dtype=float32), 'eval/episode_reward_std': Array(3020.116, dtype=float32), 'eval/episode_reward_alive_std': Array(1628.77, dtype=float32), 'eval/episode_reward_linvel_std': Array(1623.5, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(231.83, dtype=float32), 'eval/episode_x_position_std': Array(18440.596, dtype=float32), 'eval/episode_x_velocity_std': Array(1298.806, dtype=float32), 'eval/episode_y_position_std': Array(3278.663, dtype=float32), 'eval/episode_y_velocity_std': Array(244.31, dtype=float32), 'eval/avg_episode_length': Array(783.125, dtype=float32), 'eval/epoch_eval_time': 10.367046356201172, 'eval/sps': 12346.814666593564}
{'eval/walltime': 82.69179081916809, 'training/sps': 49008.38008477571, 'training/walltime': 233.71581196784973, 'training/entropy_loss': Array(0.007, dtype=float32), 'training/policy_loss': Array(0.001, dtype=float32), 'training/total_loss': Array(0.163, dtype=float32), 'training/v_loss': Array(0.154, dtype=float32), 'eval/episode_distance_from_origin': Array(42368.28, dtype=float32), 'eval/episode_forward_reward': Array(4572.529, dtype=float32), 'eval/episode_reward': Array(8334.35, dtype=float32), 'eval/episode_reward_alive': Array(4383.008, dtype=float32), 'eval/episode_reward_linvel': Array(4572.529, dtype=float32), 'eval/episode_reward_quadctrl': Array(-621.184, dtype=float32), 'eval/episode_x_position': Array(41815.22, dtype=float32), 'eval/episode_x_velocity': Array(3658.035, dtype=float32), 'eval/episode_y_position': Array(6411.25, dtype=float32), 'eval/episode_y_velocity': Array(595.929, dtype=float32), 'eval/episode_distance_from_origin_std': Array(16863.357, dtype=float32), 'eval/episode_forward_reward_std': Array(1465.982, dtype=float32), 'eval/episode_reward_std': Array(2609.719, dtype=float32), 'eval/episode_reward_alive_std': Array(1334.142, dtype=float32), 'eval/episode_reward_linvel_std': Array(1465.982, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(189.982, dtype=float32), 'eval/episode_x_position_std': Array(16641.736, dtype=float32), 'eval/episode_x_velocity_std': Array(1172.79, dtype=float32), 'eval/episode_y_position_std': Array(2932.37, dtype=float32), 'eval/episode_y_velocity_std': Array(224.507, dtype=float32), 'eval/avg_episode_length': Array(876.602, dtype=float32), 'eval/epoch_eval_time': 10.370997905731201, 'eval/sps': 12342.110292902951}
time to jit: 0:00:55.033880
time to train: 0:04:35.419121

--------

Iteration12-Robot1

--------

{'eval/walltime': 51.798439264297485, 'training/sps': 35500.81327405766, 'training/walltime': 73.84168863296509, 'training/entropy_loss': Array(0.002, dtype=float32), 'training/policy_loss': Array(-0.006, dtype=float32), 'training/total_loss': Array(0.554, dtype=float32), 'training/v_loss': Array(0.559, dtype=float32), 'eval/episode_distance_from_origin': Array(14503.799, dtype=float32), 'eval/episode_forward_reward': Array(1940.54, dtype=float32), 'eval/episode_reward': Array(4396.51, dtype=float32), 'eval/episode_reward_alive': Array(2905.156, dtype=float32), 'eval/episode_reward_linvel': Array(1940.54, dtype=float32), 'eval/episode_reward_quadctrl': Array(-449.185, dtype=float32), 'eval/episode_x_position': Array(14439.751, dtype=float32), 'eval/episode_x_velocity': Array(1552.436, dtype=float32), 'eval/episode_y_position': Array(-174.083, dtype=float32), 'eval/episode_y_velocity': Array(-35.227, dtype=float32), 'eval/episode_distance_from_origin_std': Array(13644.96, dtype=float32), 'eval/episode_forward_reward_std': Array(1215.73, dtype=float32), 'eval/episode_reward_std': Array(2618.484, dtype=float32), 'eval/episode_reward_alive_std': Array(1663.845, dtype=float32), 'eval/episode_reward_linvel_std': Array(1215.73, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(260.103, dtype=float32), 'eval/episode_x_position_std': Array(13629.691, dtype=float32), 'eval/episode_x_velocity_std': Array(972.588, dtype=float32), 'eval/episode_y_position_std': Array(732.296, dtype=float32), 'eval/episode_y_velocity_std': Array(76.454, dtype=float32), 'eval/avg_episode_length': Array(581.031, dtype=float32), 'eval/epoch_eval_time': 10.342060327529907, 'eval/sps': 12376.644106326874}
{'eval/walltime': 62.1509485244751, 'training/sps': 49210.820428616564, 'training/walltime': 127.11127400398254, 'training/entropy_loss': Array(0.004, dtype=float32), 'training/policy_loss': Array(-0.003, dtype=float32), 'training/total_loss': Array(0.233, dtype=float32), 'training/v_loss': Array(0.233, dtype=float32), 'eval/episode_distance_from_origin': Array(27272.537, dtype=float32), 'eval/episode_forward_reward': Array(3170.171, dtype=float32), 'eval/episode_reward': Array(6523.392, dtype=float32), 'eval/episode_reward_alive': Array(3973.438, dtype=float32), 'eval/episode_reward_linvel': Array(3170.171, dtype=float32), 'eval/episode_reward_quadctrl': Array(-620.214, dtype=float32), 'eval/episode_x_position': Array(27173.398, dtype=float32), 'eval/episode_x_velocity': Array(2536.144, dtype=float32), 'eval/episode_y_position': Array(-1193.121, dtype=float32), 'eval/episode_y_velocity': Array(-131.626, dtype=float32), 'eval/episode_distance_from_origin_std': Array(14566.532, dtype=float32), 'eval/episode_forward_reward_std': Array(1226.956, dtype=float32), 'eval/episode_reward_std': Array(2442.566, dtype=float32), 'eval/episode_reward_alive_std': Array(1443.154, dtype=float32), 'eval/episode_reward_linvel_std': Array(1226.956, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(226.469, dtype=float32), 'eval/episode_x_position_std': Array(14533.708, dtype=float32), 'eval/episode_x_velocity_std': Array(981.569, dtype=float32), 'eval/episode_y_position_std': Array(1340.387, dtype=float32), 'eval/episode_y_velocity_std': Array(110.289, dtype=float32), 'eval/avg_episode_length': Array(794.688, dtype=float32), 'eval/epoch_eval_time': 10.352509260177612, 'eval/sps': 12364.152186018327}
{'eval/walltime': 72.5106110572815, 'training/sps': 49097.723892262235, 'training/walltime': 180.50356578826904, 'training/entropy_loss': Array(0.005, dtype=float32), 'training/policy_loss': Array(-0.002, dtype=float32), 'training/total_loss': Array(0.142, dtype=float32), 'training/v_loss': Array(0.139, dtype=float32), 'eval/episode_distance_from_origin': Array(33248.406, dtype=float32), 'eval/episode_forward_reward': Array(3774.926, dtype=float32), 'eval/episode_reward': Array(7227.348, dtype=float32), 'eval/episode_reward_alive': Array(4093.359, dtype=float32), 'eval/episode_reward_linvel': Array(3774.926, dtype=float32), 'eval/episode_reward_quadctrl': Array(-640.934, dtype=float32), 'eval/episode_x_position': Array(33102.05, dtype=float32), 'eval/episode_x_velocity': Array(3019.95, dtype=float32), 'eval/episode_y_position': Array(-2346.315, dtype=float32), 'eval/episode_y_velocity': Array(-227.577, dtype=float32), 'eval/episode_distance_from_origin_std': Array(16832.195, dtype=float32), 'eval/episode_forward_reward_std': Array(1410.159, dtype=float32), 'eval/episode_reward_std': Array(2614.025, dtype=float32), 'eval/episode_reward_alive_std': Array(1429.633, dtype=float32), 'eval/episode_reward_linvel_std': Array(1410.159, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(225.118, dtype=float32), 'eval/episode_x_position_std': Array(16777.035, dtype=float32), 'eval/episode_x_velocity_std': Array(1128.132, dtype=float32), 'eval/episode_y_position_std': Array(1624.476, dtype=float32), 'eval/episode_y_velocity_std': Array(123.641, dtype=float32), 'eval/avg_episode_length': Array(818.672, dtype=float32), 'eval/epoch_eval_time': 10.359662532806396, 'eval/sps': 12355.614827669995}
{'eval/walltime': 82.86363101005554, 'training/sps': 49008.90741517431, 'training/walltime': 233.99261784553528, 'training/entropy_loss': Array(0.007, dtype=float32), 'training/policy_loss': Array(-0.001, dtype=float32), 'training/total_loss': Array(0.155, dtype=float32), 'training/v_loss': Array(0.149, dtype=float32), 'eval/episode_distance_from_origin': Array(38286.023, dtype=float32), 'eval/episode_forward_reward': Array(4340.899, dtype=float32), 'eval/episode_reward': Array(7868.675, dtype=float32), 'eval/episode_reward_alive': Array(4183.398, dtype=float32), 'eval/episode_reward_linvel': Array(4340.899, dtype=float32), 'eval/episode_reward_quadctrl': Array(-655.619, dtype=float32), 'eval/episode_x_position': Array(38100.64, dtype=float32), 'eval/episode_x_velocity': Array(3472.729, dtype=float32), 'eval/episode_y_position': Array(-3004.245, dtype=float32), 'eval/episode_y_velocity': Array(-295.1, dtype=float32), 'eval/episode_distance_from_origin_std': Array(17584.89, dtype=float32), 'eval/episode_forward_reward_std': Array(1493.579, dtype=float32), 'eval/episode_reward_std': Array(2633.873, dtype=float32), 'eval/episode_reward_alive_std': Array(1353.76, dtype=float32), 'eval/episode_reward_linvel_std': Array(1493.579, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(212.826, dtype=float32), 'eval/episode_x_position_std': Array(17519.725, dtype=float32), 'eval/episode_x_velocity_std': Array(1194.868, dtype=float32), 'eval/episode_y_position_std': Array(1880.551, dtype=float32), 'eval/episode_y_velocity_std': Array(138.439, dtype=float32), 'eval/avg_episode_length': Array(836.68, dtype=float32), 'eval/epoch_eval_time': 10.353019952774048, 'eval/sps': 12363.542288518718}
time to jit: 0:00:55.768355
time to train: 0:04:35.673346

--------

Iteration12-Robot2

--------

{'eval/walltime': 52.066548109054565, 'training/sps': 35411.51217973569, 'training/walltime': 74.02790331840515, 'training/entropy_loss': Array(0.001, dtype=float32), 'training/policy_loss': Array(-0.011, dtype=float32), 'training/total_loss': Array(0.709, dtype=float32), 'training/v_loss': Array(0.72, dtype=float32), 'eval/episode_distance_from_origin': Array(1264.407, dtype=float32), 'eval/episode_forward_reward': Array(611.651, dtype=float32), 'eval/episode_reward': Array(1444.364, dtype=float32), 'eval/episode_reward_alive': Array(975.039, dtype=float32), 'eval/episode_reward_linvel': Array(611.651, dtype=float32), 'eval/episode_reward_quadctrl': Array(-142.326, dtype=float32), 'eval/episode_x_position': Array(1209.556, dtype=float32), 'eval/episode_x_velocity': Array(489.321, dtype=float32), 'eval/episode_y_position': Array(-153.844, dtype=float32), 'eval/episode_y_velocity': Array(-87.905, dtype=float32), 'eval/episode_distance_from_origin_std': Array(1342.441, dtype=float32), 'eval/episode_forward_reward_std': Array(318.323, dtype=float32), 'eval/episode_reward_std': Array(648.492, dtype=float32), 'eval/episode_reward_alive_std': Array(391.224, dtype=float32), 'eval/episode_reward_linvel_std': Array(318.323, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(60.147, dtype=float32), 'eval/episode_x_position_std': Array(1319.891, dtype=float32), 'eval/episode_x_velocity_std': Array(254.659, dtype=float32), 'eval/episode_y_position_std': Array(243.412, dtype=float32), 'eval/episode_y_velocity_std': Array(72.468, dtype=float32), 'eval/avg_episode_length': Array(195.008, dtype=float32), 'eval/epoch_eval_time': 10.325713634490967, 'eval/sps': 12396.237638475832}
time to jit: 0:00:56.258291
time to train: 0:01:24.438876

--------

Iteration12-Robot3

--------

{'eval/walltime': 52.24756479263306, 'training/sps': 35380.59763848737, 'training/walltime': 74.09258675575256, 'training/entropy_loss': Array(0.002, dtype=float32), 'training/policy_loss': Array(-0.01, dtype=float32), 'training/total_loss': Array(0.995, dtype=float32), 'training/v_loss': Array(1.003, dtype=float32), 'eval/episode_distance_from_origin': Array(5485.95, dtype=float32), 'eval/episode_forward_reward': Array(1185.99, dtype=float32), 'eval/episode_reward': Array(2596.673, dtype=float32), 'eval/episode_reward_alive': Array(1670.859, dtype=float32), 'eval/episode_reward_linvel': Array(1185.99, dtype=float32), 'eval/episode_reward_quadctrl': Array(-260.175, dtype=float32), 'eval/episode_x_position': Array(5431.577, dtype=float32), 'eval/episode_x_velocity': Array(948.793, dtype=float32), 'eval/episode_y_position': Array(159.765, dtype=float32), 'eval/episode_y_velocity': Array(11.635, dtype=float32), 'eval/episode_distance_from_origin_std': Array(8519.75, dtype=float32), 'eval/episode_forward_reward_std': Array(928.533, dtype=float32), 'eval/episode_reward_std': Array(1850.727, dtype=float32), 'eval/episode_reward_alive_std': Array(1101.106, dtype=float32), 'eval/episode_reward_linvel_std': Array(928.533, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(178.336, dtype=float32), 'eval/episode_x_position_std': Array(8509.026, dtype=float32), 'eval/episode_x_velocity_std': Array(742.829, dtype=float32), 'eval/episode_y_position_std': Array(399.776, dtype=float32), 'eval/episode_y_velocity_std': Array(52.178, dtype=float32), 'eval/avg_episode_length': Array(334.172, dtype=float32), 'eval/epoch_eval_time': 10.326983213424683, 'eval/sps': 12394.713669487222}
0

--------

Iteration0-Robot0

--------

{'eval/walltime': 49.60443615913391, 'training/sps': 34818.69093214847, 'training/walltime': 75.28829860687256, 'training/entropy_loss': Array(-0.013, dtype=float32), 'training/policy_loss': Array(-0.001, dtype=float32), 'training/total_loss': Array(0.084, dtype=float32), 'training/v_loss': Array(0.098, dtype=float32), 'eval/episode_distance_from_origin': Array(40.19, dtype=float32), 'eval/episode_forward_reward': Array(12.264, dtype=float32), 'eval/episode_reward': Array(210.831, dtype=float32), 'eval/episode_reward_alive': Array(223.477, dtype=float32), 'eval/episode_reward_linvel': Array(12.264, dtype=float32), 'eval/episode_reward_quadctrl': Array(-24.91, dtype=float32), 'eval/episode_x_position': Array(4.488, dtype=float32), 'eval/episode_x_velocity': Array(9.811, dtype=float32), 'eval/episode_y_position': Array(0.59, dtype=float32), 'eval/episode_y_velocity': Array(1.506, dtype=float32), 'eval/episode_distance_from_origin_std': Array(6.406, dtype=float32), 'eval/episode_forward_reward_std': Array(12.495, dtype=float32), 'eval/episode_reward_std': Array(31.086, dtype=float32), 'eval/episode_reward_alive_std': Array(34.076, dtype=float32), 'eval/episode_reward_linvel_std': Array(12.495, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(3.907, dtype=float32), 'eval/episode_x_position_std': Array(2.837, dtype=float32), 'eval/episode_x_velocity_std': Array(9.996, dtype=float32), 'eval/episode_y_position_std': Array(3.778, dtype=float32), 'eval/episode_y_velocity_std': Array(11.533, dtype=float32), 'eval/avg_episode_length': Array(44.695, dtype=float32), 'eval/epoch_eval_time': 10.213805675506592, 'eval/sps': 12532.057498112854}
{'eval/walltime': 59.935251235961914, 'training/sps': 49665.957519699004, 'training/walltime': 128.06972336769104, 'training/entropy_loss': Array(-0.012, dtype=float32), 'training/policy_loss': Array(-0.014, dtype=float32), 'training/total_loss': Array(0.013, dtype=float32), 'training/v_loss': Array(0.038, dtype=float32), 'eval/episode_distance_from_origin': Array(52.262, dtype=float32), 'eval/episode_forward_reward': Array(18.964, dtype=float32), 'eval/episode_reward': Array(273.56, dtype=float32), 'eval/episode_reward_alive': Array(283.945, dtype=float32), 'eval/episode_reward_linvel': Array(18.964, dtype=float32), 'eval/episode_reward_quadctrl': Array(-29.35, dtype=float32), 'eval/episode_x_position': Array(8.397, dtype=float32), 'eval/episode_x_velocity': Array(15.171, dtype=float32), 'eval/episode_y_position': Array(-0.195, dtype=float32), 'eval/episode_y_velocity': Array(-0.897, dtype=float32), 'eval/episode_distance_from_origin_std': Array(10.14, dtype=float32), 'eval/episode_forward_reward_std': Array(16.386, dtype=float32), 'eval/episode_reward_std': Array(49.069, dtype=float32), 'eval/episode_reward_alive_std': Array(54.517, dtype=float32), 'eval/episode_reward_linvel_std': Array(16.386, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(5.859, dtype=float32), 'eval/episode_x_position_std': Array(4.347, dtype=float32), 'eval/episode_x_velocity_std': Array(13.108, dtype=float32), 'eval/episode_y_position_std': Array(4.354, dtype=float32), 'eval/episode_y_velocity_std': Array(10.197, dtype=float32), 'eval/avg_episode_length': Array(56.789, dtype=float32), 'eval/epoch_eval_time': 10.330815076828003, 'eval/sps': 12390.116273313588}
{'eval/walltime': 70.2666482925415, 'training/sps': 49377.89751450235, 'training/walltime': 181.15906357765198, 'training/entropy_loss': Array(-0.011, dtype=float32), 'training/policy_loss': Array(-0.011, dtype=float32), 'training/total_loss': Array(0.011, dtype=float32), 'training/v_loss': Array(0.032, dtype=float32), 'eval/episode_distance_from_origin': Array(59.247, dtype=float32), 'eval/episode_forward_reward': Array(21.027, dtype=float32), 'eval/episode_reward': Array(309.777, dtype=float32), 'eval/episode_reward_alive': Array(319.609, dtype=float32), 'eval/episode_reward_linvel': Array(21.027, dtype=float32), 'eval/episode_reward_quadctrl': Array(-30.859, dtype=float32), 'eval/episode_x_position': Array(10.702, dtype=float32), 'eval/episode_x_velocity': Array(16.822, dtype=float32), 'eval/episode_y_position': Array(0.537, dtype=float32), 'eval/episode_y_velocity': Array(0.41, dtype=float32), 'eval/episode_distance_from_origin_std': Array(13.393, dtype=float32), 'eval/episode_forward_reward_std': Array(21.051, dtype=float32), 'eval/episode_reward_std': Array(66.769, dtype=float32), 'eval/episode_reward_alive_std': Array(71.238, dtype=float32), 'eval/episode_reward_linvel_std': Array(21.051, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(7.162, dtype=float32), 'eval/episode_x_position_std': Array(6.495, dtype=float32), 'eval/episode_x_velocity_std': Array(16.841, dtype=float32), 'eval/episode_y_position_std': Array(3.554, dtype=float32), 'eval/episode_y_velocity_std': Array(7.704, dtype=float32), 'eval/avg_episode_length': Array(63.922, dtype=float32), 'eval/epoch_eval_time': 10.33139705657959, 'eval/sps': 12389.41832348634}
{'eval/walltime': 80.6526050567627, 'training/sps': 49197.173167752655, 'training/walltime': 234.44342589378357, 'training/entropy_loss': Array(-0.009, dtype=float32), 'training/policy_loss': Array(-0.01, dtype=float32), 'training/total_loss': Array(0.014, dtype=float32), 'training/v_loss': Array(0.034, dtype=float32), 'eval/episode_distance_from_origin': Array(71.849, dtype=float32), 'eval/episode_forward_reward': Array(35.18, dtype=float32), 'eval/episode_reward': Array(375.826, dtype=float32), 'eval/episode_reward_alive': Array(375.938, dtype=float32), 'eval/episode_reward_linvel': Array(35.18, dtype=float32), 'eval/episode_reward_quadctrl': Array(-35.291, dtype=float32), 'eval/episode_x_position': Array(19.408, dtype=float32), 'eval/episode_x_velocity': Array(28.144, dtype=float32), 'eval/episode_y_position': Array(0.513, dtype=float32), 'eval/episode_y_velocity': Array(-0.066, dtype=float32), 'eval/episode_distance_from_origin_std': Array(17.485, dtype=float32), 'eval/episode_forward_reward_std': Array(19.041, dtype=float32), 'eval/episode_reward_std': Array(81.875, dtype=float32), 'eval/episode_reward_alive_std': Array(89.582, dtype=float32), 'eval/episode_reward_linvel_std': Array(19.041, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(8.503, dtype=float32), 'eval/episode_x_position_std': Array(8.724, dtype=float32), 'eval/episode_x_velocity_std': Array(15.233, dtype=float32), 'eval/episode_y_position_std': Array(4.634, dtype=float32), 'eval/episode_y_velocity_std': Array(9.798, dtype=float32), 'eval/avg_episode_length': Array(75.188, dtype=float32), 'eval/epoch_eval_time': 10.385956764221191, 'eval/sps': 12324.333993084776}
time to jit: 0:00:55.072820
time to train: 0:04:36.347629

--------

Iteration0-Robot1

--------

{'eval/walltime': 51.0875084400177, 'training/sps': 34775.35317598534, 'training/walltime': 75.38212442398071, 'training/entropy_loss': Array(-0.013, dtype=float32), 'training/policy_loss': Array(0.003, dtype=float32), 'training/total_loss': Array(0.1, dtype=float32), 'training/v_loss': Array(0.11, dtype=float32), 'eval/episode_distance_from_origin': Array(38.242, dtype=float32), 'eval/episode_forward_reward': Array(10.241, dtype=float32), 'eval/episode_reward': Array(213.814, dtype=float32), 'eval/episode_reward_alive': Array(229.141, dtype=float32), 'eval/episode_reward_linvel': Array(10.241, dtype=float32), 'eval/episode_reward_quadctrl': Array(-25.567, dtype=float32), 'eval/episode_x_position': Array(3.502, dtype=float32), 'eval/episode_x_velocity': Array(8.193, dtype=float32), 'eval/episode_y_position': Array(0.56, dtype=float32), 'eval/episode_y_velocity': Array(0.747, dtype=float32), 'eval/episode_distance_from_origin_std': Array(7.731, dtype=float32), 'eval/episode_forward_reward_std': Array(11.249, dtype=float32), 'eval/episode_reward_std': Array(41.259, dtype=float32), 'eval/episode_reward_alive_std': Array(45.988, dtype=float32), 'eval/episode_reward_linvel_std': Array(11.249, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(5.403, dtype=float32), 'eval/episode_x_position_std': Array(2.901, dtype=float32), 'eval/episode_x_velocity_std': Array(8.999, dtype=float32), 'eval/episode_y_position_std': Array(3.536, dtype=float32), 'eval/episode_y_velocity_std': Array(10.591, dtype=float32), 'eval/avg_episode_length': Array(45.828, dtype=float32), 'eval/epoch_eval_time': 10.302961587905884, 'eval/sps': 12423.612269917867}
{'eval/walltime': 61.49658536911011, 'training/sps': 49234.572052184834, 'training/walltime': 128.62601161003113, 'training/entropy_loss': Array(-0.012, dtype=float32), 'training/policy_loss': Array(-0.014, dtype=float32), 'training/total_loss': Array(0.017, dtype=float32), 'training/v_loss': Array(0.043, dtype=float32), 'eval/episode_distance_from_origin': Array(46.991, dtype=float32), 'eval/episode_forward_reward': Array(15.675, dtype=float32), 'eval/episode_reward': Array(262.854, dtype=float32), 'eval/episode_reward_alive': Array(275.859, dtype=float32), 'eval/episode_reward_linvel': Array(15.675, dtype=float32), 'eval/episode_reward_quadctrl': Array(-28.681, dtype=float32), 'eval/episode_x_position': Array(6.564, dtype=float32), 'eval/episode_x_velocity': Array(12.54, dtype=float32), 'eval/episode_y_position': Array(0.287, dtype=float32), 'eval/episode_y_velocity': Array(0.006, dtype=float32), 'eval/episode_distance_from_origin_std': Array(8.857, dtype=float32), 'eval/episode_forward_reward_std': Array(15.03, dtype=float32), 'eval/episode_reward_std': Array(47.204, dtype=float32), 'eval/episode_reward_alive_std': Array(49.954, dtype=float32), 'eval/episode_reward_linvel_std': Array(15.03, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(5.532, dtype=float32), 'eval/episode_x_position_std': Array(4.527, dtype=float32), 'eval/episode_x_velocity_std': Array(12.024, dtype=float32), 'eval/episode_y_position_std': Array(3.83, dtype=float32), 'eval/episode_y_velocity_std': Array(9.52, dtype=float32), 'eval/avg_episode_length': Array(55.172, dtype=float32), 'eval/epoch_eval_time': 10.409076929092407, 'eval/sps': 12296.959746954299}
{'eval/walltime': 71.87745118141174, 'training/sps': 49032.540359976294, 'training/walltime': 182.08928275108337, 'training/entropy_loss': Array(-0.01, dtype=float32), 'training/policy_loss': Array(-0.011, dtype=float32), 'training/total_loss': Array(0.015, dtype=float32), 'training/v_loss': Array(0.037, dtype=float32), 'eval/episode_distance_from_origin': Array(57.597, dtype=float32), 'eval/episode_forward_reward': Array(19.412, dtype=float32), 'eval/episode_reward': Array(321.755, dtype=float32), 'eval/episode_reward_alive': Array(334.961, dtype=float32), 'eval/episode_reward_linvel': Array(19.412, dtype=float32), 'eval/episode_reward_quadctrl': Array(-32.619, dtype=float32), 'eval/episode_x_position': Array(9.788, dtype=float32), 'eval/episode_x_velocity': Array(15.53, dtype=float32), 'eval/episode_y_position': Array(2.31, dtype=float32), 'eval/episode_y_velocity': Array(3.512, dtype=float32), 'eval/episode_distance_from_origin_std': Array(14.777, dtype=float32), 'eval/episode_forward_reward_std': Array(16.144, dtype=float32), 'eval/episode_reward_std': Array(76.109, dtype=float32), 'eval/episode_reward_alive_std': Array(82.409, dtype=float32), 'eval/episode_reward_linvel_std': Array(16.144, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(8.367, dtype=float32), 'eval/episode_x_position_std': Array(6.527, dtype=float32), 'eval/episode_x_velocity_std': Array(12.915, dtype=float32), 'eval/episode_y_position_std': Array(5.139, dtype=float32), 'eval/episode_y_velocity_std': Array(10.677, dtype=float32), 'eval/avg_episode_length': Array(66.992, dtype=float32), 'eval/epoch_eval_time': 10.380865812301636, 'eval/sps': 12330.37805462394}
{'eval/walltime': 82.29538655281067, 'training/sps': 48973.99334190569, 'training/walltime': 235.6164677143097, 'training/entropy_loss': Array(-0.009, dtype=float32), 'training/policy_loss': Array(-0.011, dtype=float32), 'training/total_loss': Array(0.015, dtype=float32), 'training/v_loss': Array(0.036, dtype=float32), 'eval/episode_distance_from_origin': Array(76.838, dtype=float32), 'eval/episode_forward_reward': Array(26.159, dtype=float32), 'eval/episode_reward': Array(420.043, dtype=float32), 'eval/episode_reward_alive': Array(434.531, dtype=float32), 'eval/episode_reward_linvel': Array(26.159, dtype=float32), 'eval/episode_reward_quadctrl': Array(-40.647, dtype=float32), 'eval/episode_x_position': Array(17.155, dtype=float32), 'eval/episode_x_velocity': Array(20.927, dtype=float32), 'eval/episode_y_position': Array(4.554, dtype=float32), 'eval/episode_y_velocity': Array(2.012, dtype=float32), 'eval/episode_distance_from_origin_std': Array(23.033, dtype=float32), 'eval/episode_forward_reward_std': Array(19.859, dtype=float32), 'eval/episode_reward_std': Array(108.836, dtype=float32), 'eval/episode_reward_alive_std': Array(117.69, dtype=float32), 'eval/episode_reward_linvel_std': Array(19.859, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(11.422, dtype=float32), 'eval/episode_x_position_std': Array(12.766, dtype=float32), 'eval/episode_x_velocity_std': Array(15.887, dtype=float32), 'eval/episode_y_position_std': Array(9.436, dtype=float32), 'eval/episode_y_velocity_std': Array(12.897, dtype=float32), 'eval/avg_episode_length': Array(86.906, dtype=float32), 'eval/epoch_eval_time': 10.417935371398926, 'eval/sps': 12286.503557260223}
time to jit: 0:00:55.300466
time to train: 0:04:37.452426

--------

Iteration0-Robot2

--------

{'eval/walltime': 51.701125621795654, 'training/sps': 34354.05406033004, 'training/walltime': 76.30656909942627, 'training/entropy_loss': Array(-0.013, dtype=float32), 'training/policy_loss': Array(0.001, dtype=float32), 'training/total_loss': Array(0.079, dtype=float32), 'training/v_loss': Array(0.091, dtype=float32), 'eval/episode_distance_from_origin': Array(40.961, dtype=float32), 'eval/episode_forward_reward': Array(13.803, dtype=float32), 'eval/episode_reward': Array(217.628, dtype=float32), 'eval/episode_reward_alive': Array(229.219, dtype=float32), 'eval/episode_reward_linvel': Array(13.803, dtype=float32), 'eval/episode_reward_quadctrl': Array(-25.394, dtype=float32), 'eval/episode_x_position': Array(4.634, dtype=float32), 'eval/episode_x_velocity': Array(11.043, dtype=float32), 'eval/episode_y_position': Array(0.589, dtype=float32), 'eval/episode_y_velocity': Array(1.065, dtype=float32), 'eval/episode_distance_from_origin_std': Array(7.258, dtype=float32), 'eval/episode_forward_reward_std': Array(13.893, dtype=float32), 'eval/episode_reward_std': Array(34.849, dtype=float32), 'eval/episode_reward_alive_std': Array(38.828, dtype=float32), 'eval/episode_reward_linvel_std': Array(13.893, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(4.5, dtype=float32), 'eval/episode_x_position_std': Array(3.304, dtype=float32), 'eval/episode_x_velocity_std': Array(11.114, dtype=float32), 'eval/episode_y_position_std': Array(3.886, dtype=float32), 'eval/episode_y_velocity_std': Array(11.793, dtype=float32), 'eval/avg_episode_length': Array(45.844, dtype=float32), 'eval/epoch_eval_time': 10.373122453689575, 'eval/sps': 12339.582471088268}
{'eval/walltime': 62.12902760505676, 'training/sps': 49168.70210908382, 'training/walltime': 129.62178564071655, 'training/entropy_loss': Array(-0.012, dtype=float32), 'training/policy_loss': Array(-0.014, dtype=float32), 'training/total_loss': Array(0.009, dtype=float32), 'training/v_loss': Array(0.034, dtype=float32), 'eval/episode_distance_from_origin': Array(53.367, dtype=float32), 'eval/episode_forward_reward': Array(18.858, dtype=float32), 'eval/episode_reward': Array(281.735, dtype=float32), 'eval/episode_reward_alive': Array(292.891, dtype=float32), 'eval/episode_reward_linvel': Array(18.858, dtype=float32), 'eval/episode_reward_quadctrl': Array(-30.014, dtype=float32), 'eval/episode_x_position': Array(8.024, dtype=float32), 'eval/episode_x_velocity': Array(15.087, dtype=float32), 'eval/episode_y_position': Array(0.114, dtype=float32), 'eval/episode_y_velocity': Array(-0.683, dtype=float32), 'eval/episode_distance_from_origin_std': Array(10.468, dtype=float32), 'eval/episode_forward_reward_std': Array(18.891, dtype=float32), 'eval/episode_reward_std': Array(50.018, dtype=float32), 'eval/episode_reward_alive_std': Array(57.824, dtype=float32), 'eval/episode_reward_linvel_std': Array(18.891, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(6.219, dtype=float32), 'eval/episode_x_position_std': Array(4.756, dtype=float32), 'eval/episode_x_velocity_std': Array(15.112, dtype=float32), 'eval/episode_y_position_std': Array(4.075, dtype=float32), 'eval/episode_y_velocity_std': Array(9.912, dtype=float32), 'eval/avg_episode_length': Array(58.578, dtype=float32), 'eval/epoch_eval_time': 10.427901983261108, 'eval/sps': 12274.76056118152}
{'eval/walltime': 72.51060771942139, 'training/sps': 48972.36499632047, 'training/walltime': 183.15075039863586, 'training/entropy_loss': Array(-0.01, dtype=float32), 'training/policy_loss': Array(-0.01, dtype=float32), 'training/total_loss': Array(0.008, dtype=float32), 'training/v_loss': Array(0.029, dtype=float32), 'eval/episode_distance_from_origin': Array(62.511, dtype=float32), 'eval/episode_forward_reward': Array(22.823, dtype=float32), 'eval/episode_reward': Array(331.572, dtype=float32), 'eval/episode_reward_alive': Array(340.547, dtype=float32), 'eval/episode_reward_linvel': Array(22.823, dtype=float32), 'eval/episode_reward_quadctrl': Array(-31.798, dtype=float32), 'eval/episode_x_position': Array(10.726, dtype=float32), 'eval/episode_x_velocity': Array(18.259, dtype=float32), 'eval/episode_y_position': Array(0.961, dtype=float32), 'eval/episode_y_velocity': Array(0.976, dtype=float32), 'eval/episode_distance_from_origin_std': Array(14.564, dtype=float32), 'eval/episode_forward_reward_std': Array(20.98, dtype=float32), 'eval/episode_reward_std': Array(71.278, dtype=float32), 'eval/episode_reward_alive_std': Array(79.69, dtype=float32), 'eval/episode_reward_linvel_std': Array(20.98, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(7.977, dtype=float32), 'eval/episode_x_position_std': Array(6.101, dtype=float32), 'eval/episode_x_velocity_std': Array(16.784, dtype=float32), 'eval/episode_y_position_std': Array(3.752, dtype=float32), 'eval/episode_y_velocity_std': Array(7.066, dtype=float32), 'eval/avg_episode_length': Array(68.109, dtype=float32), 'eval/epoch_eval_time': 10.381580114364624, 'eval/sps': 12329.529665998622}
{'eval/walltime': 82.88903331756592, 'training/sps': 48912.04623768093, 'training/walltime': 236.7457275390625, 'training/entropy_loss': Array(-0.009, dtype=float32), 'training/policy_loss': Array(-0.009, dtype=float32), 'training/total_loss': Array(0.006, dtype=float32), 'training/v_loss': Array(0.025, dtype=float32), 'eval/episode_distance_from_origin': Array(68.334, dtype=float32), 'eval/episode_forward_reward': Array(30.696, dtype=float32), 'eval/episode_reward': Array(368.107, dtype=float32), 'eval/episode_reward_alive': Array(369.297, dtype=float32), 'eval/episode_reward_linvel': Array(30.696, dtype=float32), 'eval/episode_reward_quadctrl': Array(-31.886, dtype=float32), 'eval/episode_x_position': Array(14.025, dtype=float32), 'eval/episode_x_velocity': Array(24.557, dtype=float32), 'eval/episode_y_position': Array(-0.131, dtype=float32), 'eval/episode_y_velocity': Array(-0.542, dtype=float32), 'eval/episode_distance_from_origin_std': Array(13.096, dtype=float32), 'eval/episode_forward_reward_std': Array(17.955, dtype=float32), 'eval/episode_reward_std': Array(64.877, dtype=float32), 'eval/episode_reward_alive_std': Array(70.236, dtype=float32), 'eval/episode_reward_linvel_std': Array(17.955, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(6.494, dtype=float32), 'eval/episode_x_position_std': Array(5.869, dtype=float32), 'eval/episode_x_velocity_std': Array(14.364, dtype=float32), 'eval/episode_y_position_std': Array(3.747, dtype=float32), 'eval/episode_y_velocity_std': Array(7.096, dtype=float32), 'eval/avg_episode_length': Array(73.859, dtype=float32), 'eval/epoch_eval_time': 10.378425598144531, 'eval/sps': 12333.277219126956}
time to jit: 0:00:55.952767
time to train: 0:04:38.538873

--------

Iteration0-Robot3

--------

{'eval/walltime': 51.89523243904114, 'training/sps': 34536.13643355313, 'training/walltime': 75.90426349639893, 'training/entropy_loss': Array(-0.013, dtype=float32), 'training/policy_loss': Array(0.002, dtype=float32), 'training/total_loss': Array(0.1, dtype=float32), 'training/v_loss': Array(0.11, dtype=float32), 'eval/episode_distance_from_origin': Array(36.376, dtype=float32), 'eval/episode_forward_reward': Array(8.184, dtype=float32), 'eval/episode_reward': Array(200.277, dtype=float32), 'eval/episode_reward_alive': Array(216.758, dtype=float32), 'eval/episode_reward_linvel': Array(8.184, dtype=float32), 'eval/episode_reward_quadctrl': Array(-24.665, dtype=float32), 'eval/episode_x_position': Array(2.973, dtype=float32), 'eval/episode_x_velocity': Array(6.547, dtype=float32), 'eval/episode_y_position': Array(1.305, dtype=float32), 'eval/episode_y_velocity': Array(3.141, dtype=float32), 'eval/episode_distance_from_origin_std': Array(8.085, dtype=float32), 'eval/episode_forward_reward_std': Array(11.613, dtype=float32), 'eval/episode_reward_std': Array(41.125, dtype=float32), 'eval/episode_reward_alive_std': Array(49.012, dtype=float32), 'eval/episode_reward_linvel_std': Array(11.613, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(5.975, dtype=float32), 'eval/episode_x_position_std': Array(2.733, dtype=float32), 'eval/episode_x_velocity_std': Array(9.29, dtype=float32), 'eval/episode_y_position_std': Array(3.132, dtype=float32), 'eval/episode_y_velocity_std': Array(9.312, dtype=float32), 'eval/avg_episode_length': Array(43.352, dtype=float32), 'eval/epoch_eval_time': 10.241052627563477, 'eval/sps': 12498.715186317075}
{'eval/walltime': 62.28730654716492, 'training/sps': 49125.55417801919, 'training/walltime': 129.26630783081055, 'training/entropy_loss': Array(-0.011, dtype=float32), 'training/policy_loss': Array(-0.015, dtype=float32), 'training/total_loss': Array(0.019, dtype=float32), 'training/v_loss': Array(0.045, dtype=float32), 'eval/episode_distance_from_origin': Array(47.882, dtype=float32), 'eval/episode_forward_reward': Array(13.904, dtype=float32), 'eval/episode_reward': Array(263.659, dtype=float32), 'eval/episode_reward_alive': Array(279.492, dtype=float32), 'eval/episode_reward_linvel': Array(13.904, dtype=float32), 'eval/episode_reward_quadctrl': Array(-29.737, dtype=float32), 'eval/episode_x_position': Array(6.001, dtype=float32), 'eval/episode_x_velocity': Array(11.123, dtype=float32), 'eval/episode_y_position': Array(0.499, dtype=float32), 'eval/episode_y_velocity': Array(0.547, dtype=float32), 'eval/episode_distance_from_origin_std': Array(8.902, dtype=float32), 'eval/episode_forward_reward_std': Array(13.643, dtype=float32), 'eval/episode_reward_std': Array(45.94, dtype=float32), 'eval/episode_reward_alive_std': Array(52.641, dtype=float32), 'eval/episode_reward_linvel_std': Array(13.643, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(6.203, dtype=float32), 'eval/episode_x_position_std': Array(3.558, dtype=float32), 'eval/episode_x_velocity_std': Array(10.915, dtype=float32), 'eval/episode_y_position_std': Array(4.1, dtype=float32), 'eval/episode_y_velocity_std': Array(10.351, dtype=float32), 'eval/avg_episode_length': Array(55.898, dtype=float32), 'eval/epoch_eval_time': 10.39207410812378, 'eval/sps': 12317.079215201013}
{'eval/walltime': 72.60579180717468, 'training/sps': 49039.32237587764, 'training/walltime': 182.7221851348877, 'training/entropy_loss': Array(-0.01, dtype=float32), 'training/policy_loss': Array(-0.01, dtype=float32), 'training/total_loss': Array(0.019, dtype=float32), 'training/v_loss': Array(0.039, dtype=float32), 'eval/episode_distance_from_origin': Array(55.544, dtype=float32), 'eval/episode_forward_reward': Array(19.043, dtype=float32), 'eval/episode_reward': Array(309.015, dtype=float32), 'eval/episode_reward_alive': Array(321.562, dtype=float32), 'eval/episode_reward_linvel': Array(19.043, dtype=float32), 'eval/episode_reward_quadctrl': Array(-31.591, dtype=float32), 'eval/episode_x_position': Array(9.01, dtype=float32), 'eval/episode_x_velocity': Array(15.235, dtype=float32), 'eval/episode_y_position': Array(2.228, dtype=float32), 'eval/episode_y_velocity': Array(2.952, dtype=float32), 'eval/episode_distance_from_origin_std': Array(12.93, dtype=float32), 'eval/episode_forward_reward_std': Array(15.088, dtype=float32), 'eval/episode_reward_std': Array(67.458, dtype=float32), 'eval/episode_reward_alive_std': Array(73.919, dtype=float32), 'eval/episode_reward_linvel_std': Array(15.088, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(7.855, dtype=float32), 'eval/episode_x_position_std': Array(5.274, dtype=float32), 'eval/episode_x_velocity_std': Array(12.071, dtype=float32), 'eval/episode_y_position_std': Array(4.702, dtype=float32), 'eval/episode_y_velocity_std': Array(8.889, dtype=float32), 'eval/avg_episode_length': Array(64.312, dtype=float32), 'eval/epoch_eval_time': 10.318485260009766, 'eval/sps': 12404.92153398481}
{'eval/walltime': 82.96119546890259, 'training/sps': 48961.66336405668, 'training/walltime': 236.26284980773926, 'training/entropy_loss': Array(-0.009, dtype=float32), 'training/policy_loss': Array(-0.01, dtype=float32), 'training/total_loss': Array(0.013, dtype=float32), 'training/v_loss': Array(0.032, dtype=float32), 'eval/episode_distance_from_origin': Array(68.02, dtype=float32), 'eval/episode_forward_reward': Array(21.006, dtype=float32), 'eval/episode_reward': Array(376.117, dtype=float32), 'eval/episode_reward_alive': Array(391.445, dtype=float32), 'eval/episode_reward_linvel': Array(21.006, dtype=float32), 'eval/episode_reward_quadctrl': Array(-36.334, dtype=float32), 'eval/episode_x_position': Array(12.055, dtype=float32), 'eval/episode_x_velocity': Array(16.805, dtype=float32), 'eval/episode_y_position': Array(3.459, dtype=float32), 'eval/episode_y_velocity': Array(3.896, dtype=float32), 'eval/episode_distance_from_origin_std': Array(14.453, dtype=float32), 'eval/episode_forward_reward_std': Array(17.704, dtype=float32), 'eval/episode_reward_std': Array(71.336, dtype=float32), 'eval/episode_reward_alive_std': Array(83.529, dtype=float32), 'eval/episode_reward_linvel_std': Array(17.704, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(8.895, dtype=float32), 'eval/episode_x_position_std': Array(5.875, dtype=float32), 'eval/episode_x_velocity_std': Array(14.163, dtype=float32), 'eval/episode_y_position_std': Array(6.073, dtype=float32), 'eval/episode_y_velocity_std': Array(10.056, dtype=float32), 'eval/avg_episode_length': Array(78.289, dtype=float32), 'eval/epoch_eval_time': 10.355403661727905, 'eval/sps': 12360.696326408766}
time to jit: 0:00:56.762923
time to train: 0:04:37.830888
1

--------

Iteration1-Robot0

--------

{'eval/walltime': 49.838353395462036, 'training/sps': 34621.79026574908, 'training/walltime': 75.716477394104, 'training/entropy_loss': Array(-0.012, dtype=float32), 'training/policy_loss': Array(-0.005, dtype=float32), 'training/total_loss': Array(0.069, dtype=float32), 'training/v_loss': Array(0.086, dtype=float32), 'eval/episode_distance_from_origin': Array(36.853, dtype=float32), 'eval/episode_forward_reward': Array(10.544, dtype=float32), 'eval/episode_reward': Array(201.502, dtype=float32), 'eval/episode_reward_alive': Array(213.555, dtype=float32), 'eval/episode_reward_linvel': Array(10.544, dtype=float32), 'eval/episode_reward_quadctrl': Array(-22.596, dtype=float32), 'eval/episode_x_position': Array(2.428, dtype=float32), 'eval/episode_x_velocity': Array(8.435, dtype=float32), 'eval/episode_y_position': Array(0.723, dtype=float32), 'eval/episode_y_velocity': Array(2.155, dtype=float32), 'eval/episode_distance_from_origin_std': Array(7.668, dtype=float32), 'eval/episode_forward_reward_std': Array(10.519, dtype=float32), 'eval/episode_reward_std': Array(38.991, dtype=float32), 'eval/episode_reward_alive_std': Array(43.809, dtype=float32), 'eval/episode_reward_linvel_std': Array(10.519, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(4.616, dtype=float32), 'eval/episode_x_position_std': Array(2.448, dtype=float32), 'eval/episode_x_velocity_std': Array(8.415, dtype=float32), 'eval/episode_y_position_std': Array(3.274, dtype=float32), 'eval/episode_y_velocity_std': Array(10.289, dtype=float32), 'eval/avg_episode_length': Array(42.711, dtype=float32), 'eval/epoch_eval_time': 10.3362557888031, 'eval/sps': 12383.594467414192}
{'eval/walltime': 60.20581531524658, 'training/sps': 49203.07139444992, 'training/walltime': 128.99445223808289, 'training/entropy_loss': Array(-0.01, dtype=float32), 'training/policy_loss': Array(-0.019, dtype=float32), 'training/total_loss': Array(0.008, dtype=float32), 'training/v_loss': Array(0.037, dtype=float32), 'eval/episode_distance_from_origin': Array(64.241, dtype=float32), 'eval/episode_forward_reward': Array(16.497, dtype=float32), 'eval/episode_reward': Array(342.074, dtype=float32), 'eval/episode_reward_alive': Array(361.445, dtype=float32), 'eval/episode_reward_linvel': Array(16.497, dtype=float32), 'eval/episode_reward_quadctrl': Array(-35.867, dtype=float32), 'eval/episode_x_position': Array(7.49, dtype=float32), 'eval/episode_x_velocity': Array(13.197, dtype=float32), 'eval/episode_y_position': Array(3.357, dtype=float32), 'eval/episode_y_velocity': Array(4.195, dtype=float32), 'eval/episode_distance_from_origin_std': Array(13.036, dtype=float32), 'eval/episode_forward_reward_std': Array(17.897, dtype=float32), 'eval/episode_reward_std': Array(63.971, dtype=float32), 'eval/episode_reward_alive_std': Array(72.206, dtype=float32), 'eval/episode_reward_linvel_std': Array(17.897, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(7.271, dtype=float32), 'eval/episode_x_position_std': Array(5.857, dtype=float32), 'eval/episode_x_velocity_std': Array(14.317, dtype=float32), 'eval/episode_y_position_std': Array(6.747, dtype=float32), 'eval/episode_y_velocity_std': Array(11.703, dtype=float32), 'eval/avg_episode_length': Array(72.289, dtype=float32), 'eval/epoch_eval_time': 10.367461919784546, 'eval/sps': 12346.319763734427}
{'eval/walltime': 70.5570662021637, 'training/sps': 49065.44974285603, 'training/walltime': 182.42186427116394, 'training/entropy_loss': Array(-0.009, dtype=float32), 'training/policy_loss': Array(-0.014, dtype=float32), 'training/total_loss': Array(0.008, dtype=float32), 'training/v_loss': Array(0.03, dtype=float32), 'eval/episode_distance_from_origin': Array(91.163, dtype=float32), 'eval/episode_forward_reward': Array(27.276, dtype=float32), 'eval/episode_reward': Array(479.409, dtype=float32), 'eval/episode_reward_alive': Array(498.594, dtype=float32), 'eval/episode_reward_linvel': Array(27.276, dtype=float32), 'eval/episode_reward_quadctrl': Array(-46.461, dtype=float32), 'eval/episode_x_position': Array(17.989, dtype=float32), 'eval/episode_x_velocity': Array(21.821, dtype=float32), 'eval/episode_y_position': Array(8.181, dtype=float32), 'eval/episode_y_velocity': Array(4.353, dtype=float32), 'eval/episode_distance_from_origin_std': Array(22.066, dtype=float32), 'eval/episode_forward_reward_std': Array(20.634, dtype=float32), 'eval/episode_reward_std': Array(107.287, dtype=float32), 'eval/episode_reward_alive_std': Array(118.117, dtype=float32), 'eval/episode_reward_linvel_std': Array(20.634, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(10.717, dtype=float32), 'eval/episode_x_position_std': Array(10.862, dtype=float32), 'eval/episode_x_velocity_std': Array(16.508, dtype=float32), 'eval/episode_y_position_std': Array(9.04, dtype=float32), 'eval/episode_y_velocity_std': Array(11.425, dtype=float32), 'eval/avg_episode_length': Array(99.719, dtype=float32), 'eval/epoch_eval_time': 10.351250886917114, 'eval/sps': 12365.655262184637}
{'eval/walltime': 80.9719648361206, 'training/sps': 48928.0874002416, 'training/walltime': 235.99927020072937, 'training/entropy_loss': Array(-0.008, dtype=float32), 'training/policy_loss': Array(-0.012, dtype=float32), 'training/total_loss': Array(0.017, dtype=float32), 'training/v_loss': Array(0.037, dtype=float32), 'eval/episode_distance_from_origin': Array(173.972, dtype=float32), 'eval/episode_forward_reward': Array(51.372, dtype=float32), 'eval/episode_reward': Array(798.036, dtype=float32), 'eval/episode_reward_alive': Array(820.82, dtype=float32), 'eval/episode_reward_linvel': Array(51.372, dtype=float32), 'eval/episode_reward_quadctrl': Array(-74.156, dtype=float32), 'eval/episode_x_position': Array(76.946, dtype=float32), 'eval/episode_x_velocity': Array(41.097, dtype=float32), 'eval/episode_y_position': Array(19.251, dtype=float32), 'eval/episode_y_velocity': Array(3.817, dtype=float32), 'eval/episode_distance_from_origin_std': Array(86.792, dtype=float32), 'eval/episode_forward_reward_std': Array(30.736, dtype=float32), 'eval/episode_reward_std': Array(295.478, dtype=float32), 'eval/episode_reward_alive_std': Array(307.443, dtype=float32), 'eval/episode_reward_linvel_std': Array(30.736, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(27.249, dtype=float32), 'eval/episode_x_position_std': Array(66.752, dtype=float32), 'eval/episode_x_velocity_std': Array(24.589, dtype=float32), 'eval/episode_y_position_std': Array(19.554, dtype=float32), 'eval/episode_y_velocity_std': Array(15.368, dtype=float32), 'eval/avg_episode_length': Array(164.164, dtype=float32), 'eval/epoch_eval_time': 10.41489863395691, 'eval/sps': 12290.086010310908}
time to jit: 0:00:53.375699
time to train: 0:04:37.719927

--------

Iteration1-Robot1

--------

{'eval/walltime': 51.76577854156494, 'training/sps': 34784.29316670499, 'training/walltime': 75.36275029182434, 'training/entropy_loss': Array(-0.011, dtype=float32), 'training/policy_loss': Array(-0.002, dtype=float32), 'training/total_loss': Array(0.071, dtype=float32), 'training/v_loss': Array(0.084, dtype=float32), 'eval/episode_distance_from_origin': Array(40.939, dtype=float32), 'eval/episode_forward_reward': Array(13.306, dtype=float32), 'eval/episode_reward': Array(237.469, dtype=float32), 'eval/episode_reward_alive': Array(252.695, dtype=float32), 'eval/episode_reward_linvel': Array(13.306, dtype=float32), 'eval/episode_reward_quadctrl': Array(-28.532, dtype=float32), 'eval/episode_x_position': Array(3.948, dtype=float32), 'eval/episode_x_velocity': Array(10.644, dtype=float32), 'eval/episode_y_position': Array(-0.573, dtype=float32), 'eval/episode_y_velocity': Array(-1.586, dtype=float32), 'eval/episode_distance_from_origin_std': Array(8.392, dtype=float32), 'eval/episode_forward_reward_std': Array(15.008, dtype=float32), 'eval/episode_reward_std': Array(42.546, dtype=float32), 'eval/episode_reward_alive_std': Array(51.17, dtype=float32), 'eval/episode_reward_linvel_std': Array(15.008, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(6.115, dtype=float32), 'eval/episode_x_position_std': Array(3.862, dtype=float32), 'eval/episode_x_velocity_std': Array(12.006, dtype=float32), 'eval/episode_y_position_std': Array(3.558, dtype=float32), 'eval/episode_y_velocity_std': Array(9.389, dtype=float32), 'eval/avg_episode_length': Array(50.539, dtype=float32), 'eval/epoch_eval_time': 10.39248013496399, 'eval/sps': 12316.5979956375}
{'eval/walltime': 62.10861301422119, 'training/sps': 49081.43898954175, 'training/walltime': 128.77275729179382, 'training/entropy_loss': Array(-0.01, dtype=float32), 'training/policy_loss': Array(-0.016, dtype=float32), 'training/total_loss': Array(0.004, dtype=float32), 'training/v_loss': Array(0.03, dtype=float32), 'eval/episode_distance_from_origin': Array(58.755, dtype=float32), 'eval/episode_forward_reward': Array(20.209, dtype=float32), 'eval/episode_reward': Array(335.669, dtype=float32), 'eval/episode_reward_alive': Array(353.438, dtype=float32), 'eval/episode_reward_linvel': Array(20.209, dtype=float32), 'eval/episode_reward_quadctrl': Array(-37.978, dtype=float32), 'eval/episode_x_position': Array(10.318, dtype=float32), 'eval/episode_x_velocity': Array(16.167, dtype=float32), 'eval/episode_y_position': Array(0.605, dtype=float32), 'eval/episode_y_velocity': Array(-0.018, dtype=float32), 'eval/episode_distance_from_origin_std': Array(12.463, dtype=float32), 'eval/episode_forward_reward_std': Array(22.22, dtype=float32), 'eval/episode_reward_std': Array(66.619, dtype=float32), 'eval/episode_reward_alive_std': Array(73.773, dtype=float32), 'eval/episode_reward_linvel_std': Array(22.22, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(8.004, dtype=float32), 'eval/episode_x_position_std': Array(7.117, dtype=float32), 'eval/episode_x_velocity_std': Array(17.776, dtype=float32), 'eval/episode_y_position_std': Array(5.16, dtype=float32), 'eval/episode_y_velocity_std': Array(9.941, dtype=float32), 'eval/avg_episode_length': Array(70.688, dtype=float32), 'eval/epoch_eval_time': 10.34283447265625, 'eval/sps': 12375.717733702355}
{'eval/walltime': 72.40742444992065, 'training/sps': 48911.938967798626, 'training/walltime': 182.36785197257996, 'training/entropy_loss': Array(-0.009, dtype=float32), 'training/policy_loss': Array(-0.014, dtype=float32), 'training/total_loss': Array(0.014, dtype=float32), 'training/v_loss': Array(0.037, dtype=float32), 'eval/episode_distance_from_origin': Array(90.872, dtype=float32), 'eval/episode_forward_reward': Array(46.917, dtype=float32), 'eval/episode_reward': Array(485.906, dtype=float32), 'eval/episode_reward_alive': Array(491.094, dtype=float32), 'eval/episode_reward_linvel': Array(46.917, dtype=float32), 'eval/episode_reward_quadctrl': Array(-52.105, dtype=float32), 'eval/episode_x_position': Array(34.426, dtype=float32), 'eval/episode_x_velocity': Array(37.533, dtype=float32), 'eval/episode_y_position': Array(-0.363, dtype=float32), 'eval/episode_y_velocity': Array(0.057, dtype=float32), 'eval/episode_distance_from_origin_std': Array(23.218, dtype=float32), 'eval/episode_forward_reward_std': Array(25.016, dtype=float32), 'eval/episode_reward_std': Array(98.669, dtype=float32), 'eval/episode_reward_alive_std': Array(96.875, dtype=float32), 'eval/episode_reward_linvel_std': Array(25.016, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(10.742, dtype=float32), 'eval/episode_x_position_std': Array(19.045, dtype=float32), 'eval/episode_x_velocity_std': Array(20.013, dtype=float32), 'eval/episode_y_position_std': Array(8.461, dtype=float32), 'eval/episode_y_velocity_std': Array(13.924, dtype=float32), 'eval/avg_episode_length': Array(98.219, dtype=float32), 'eval/epoch_eval_time': 10.298811435699463, 'eval/sps': 12428.618661402517}
{'eval/walltime': 82.80394268035889, 'training/sps': 48936.55787862216, 'training/walltime': 235.93598413467407, 'training/entropy_loss': Array(-0.007, dtype=float32), 'training/policy_loss': Array(-0.012, dtype=float32), 'training/total_loss': Array(0.028, dtype=float32), 'training/v_loss': Array(0.047, dtype=float32), 'eval/episode_distance_from_origin': Array(159.114, dtype=float32), 'eval/episode_forward_reward': Array(90.783, dtype=float32), 'eval/episode_reward': Array(704.571, dtype=float32), 'eval/episode_reward_alive': Array(684.609, dtype=float32), 'eval/episode_reward_linvel': Array(90.783, dtype=float32), 'eval/episode_reward_quadctrl': Array(-70.821, dtype=float32), 'eval/episode_x_position': Array(96.042, dtype=float32), 'eval/episode_x_velocity': Array(72.626, dtype=float32), 'eval/episode_y_position': Array(4.951, dtype=float32), 'eval/episode_y_velocity': Array(3.858, dtype=float32), 'eval/episode_distance_from_origin_std': Array(68.352, dtype=float32), 'eval/episode_forward_reward_std': Array(28.321, dtype=float32), 'eval/episode_reward_std': Array(175.282, dtype=float32), 'eval/episode_reward_alive_std': Array(171.842, dtype=float32), 'eval/episode_reward_linvel_std': Array(28.321, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(18.2, dtype=float32), 'eval/episode_x_position_std': Array(61.28, dtype=float32), 'eval/episode_x_velocity_std': Array(22.657, dtype=float32), 'eval/episode_y_position_std': Array(16.08, dtype=float32), 'eval/episode_y_velocity_std': Array(18.482, dtype=float32), 'eval/avg_episode_length': Array(136.922, dtype=float32), 'eval/epoch_eval_time': 10.396518230438232, 'eval/sps': 12311.814124968312}
time to jit: 0:00:55.483667
time to train: 0:04:37.625237

--------

Iteration1-Robot2

--------

{'eval/walltime': 51.60031080245972, 'training/sps': 34379.26009366825, 'training/walltime': 76.25062298774719, 'training/entropy_loss': Array(-0.011, dtype=float32), 'training/policy_loss': Array(0.01, dtype=float32), 'training/total_loss': Array(0.07, dtype=float32), 'training/v_loss': Array(0.07, dtype=float32), 'eval/episode_distance_from_origin': Array(49.736, dtype=float32), 'eval/episode_forward_reward': Array(10.252, dtype=float32), 'eval/episode_reward': Array(273.613, dtype=float32), 'eval/episode_reward_alive': Array(294.023, dtype=float32), 'eval/episode_reward_linvel': Array(10.252, dtype=float32), 'eval/episode_reward_quadctrl': Array(-30.662, dtype=float32), 'eval/episode_x_position': Array(4.265, dtype=float32), 'eval/episode_x_velocity': Array(8.202, dtype=float32), 'eval/episode_y_position': Array(0.621, dtype=float32), 'eval/episode_y_velocity': Array(2.13, dtype=float32), 'eval/episode_distance_from_origin_std': Array(10.362, dtype=float32), 'eval/episode_forward_reward_std': Array(16.661, dtype=float32), 'eval/episode_reward_std': Array(54.362, dtype=float32), 'eval/episode_reward_alive_std': Array(61.768, dtype=float32), 'eval/episode_reward_linvel_std': Array(16.661, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(6.773, dtype=float32), 'eval/episode_x_position_std': Array(4.201, dtype=float32), 'eval/episode_x_velocity_std': Array(13.329, dtype=float32), 'eval/episode_y_position_std': Array(3.802, dtype=float32), 'eval/episode_y_velocity_std': Array(8.877, dtype=float32), 'eval/avg_episode_length': Array(58.805, dtype=float32), 'eval/epoch_eval_time': 10.397396326065063, 'eval/sps': 12310.77435022063}
{'eval/walltime': 61.983478307724, 'training/sps': 49187.02195768648, 'training/walltime': 129.54598212242126, 'training/entropy_loss': Array(-0.009, dtype=float32), 'training/policy_loss': Array(-0.014, dtype=float32), 'training/total_loss': Array(0.006, dtype=float32), 'training/v_loss': Array(0.029, dtype=float32), 'eval/episode_distance_from_origin': Array(67.665, dtype=float32), 'eval/episode_forward_reward': Array(21.514, dtype=float32), 'eval/episode_reward': Array(369.659, dtype=float32), 'eval/episode_reward_alive': Array(386.836, dtype=float32), 'eval/episode_reward_linvel': Array(21.514, dtype=float32), 'eval/episode_reward_quadctrl': Array(-38.691, dtype=float32), 'eval/episode_x_position': Array(10.932, dtype=float32), 'eval/episode_x_velocity': Array(17.211, dtype=float32), 'eval/episode_y_position': Array(1.22, dtype=float32), 'eval/episode_y_velocity': Array(1.676, dtype=float32), 'eval/episode_distance_from_origin_std': Array(13.452, dtype=float32), 'eval/episode_forward_reward_std': Array(21.01, dtype=float32), 'eval/episode_reward_std': Array(67.367, dtype=float32), 'eval/episode_reward_alive_std': Array(77.658, dtype=float32), 'eval/episode_reward_linvel_std': Array(21.01, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(8.298, dtype=float32), 'eval/episode_x_position_std': Array(7.329, dtype=float32), 'eval/episode_x_velocity_std': Array(16.808, dtype=float32), 'eval/episode_y_position_std': Array(6.425, dtype=float32), 'eval/episode_y_velocity_std': Array(11.227, dtype=float32), 'eval/avg_episode_length': Array(77.367, dtype=float32), 'eval/epoch_eval_time': 10.383167505264282, 'eval/sps': 12327.644712955253}
{'eval/walltime': 72.36228489875793, 'training/sps': 49054.83958422869, 'training/walltime': 182.9849500656128, 'training/entropy_loss': Array(-0.008, dtype=float32), 'training/policy_loss': Array(-0.014, dtype=float32), 'training/total_loss': Array(0.008, dtype=float32), 'training/v_loss': Array(0.03, dtype=float32), 'eval/episode_distance_from_origin': Array(94.145, dtype=float32), 'eval/episode_forward_reward': Array(45.426, dtype=float32), 'eval/episode_reward': Array(504.045, dtype=float32), 'eval/episode_reward_alive': Array(506.016, dtype=float32), 'eval/episode_reward_linvel': Array(45.426, dtype=float32), 'eval/episode_reward_quadctrl': Array(-47.397, dtype=float32), 'eval/episode_x_position': Array(28.834, dtype=float32), 'eval/episode_x_velocity': Array(36.341, dtype=float32), 'eval/episode_y_position': Array(1.326, dtype=float32), 'eval/episode_y_velocity': Array(-0.257, dtype=float32), 'eval/episode_distance_from_origin_std': Array(18.464, dtype=float32), 'eval/episode_forward_reward_std': Array(19.399, dtype=float32), 'eval/episode_reward_std': Array(89.508, dtype=float32), 'eval/episode_reward_alive_std': Array(96.149, dtype=float32), 'eval/episode_reward_linvel_std': Array(19.399, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(8.445, dtype=float32), 'eval/episode_x_position_std': Array(11.387, dtype=float32), 'eval/episode_x_velocity_std': Array(15.519, dtype=float32), 'eval/episode_y_position_std': Array(7.482, dtype=float32), 'eval/episode_y_velocity_std': Array(12.024, dtype=float32), 'eval/avg_episode_length': Array(101.203, dtype=float32), 'eval/epoch_eval_time': 10.378806591033936, 'eval/sps': 12332.824480087807}
{'eval/walltime': 82.76721167564392, 'training/sps': 48934.068281115724, 'training/walltime': 236.55580759048462, 'training/entropy_loss': Array(-0.007, dtype=float32), 'training/policy_loss': Array(-0.012, dtype=float32), 'training/total_loss': Array(0.01, dtype=float32), 'training/v_loss': Array(0.029, dtype=float32), 'eval/episode_distance_from_origin': Array(139.934, dtype=float32), 'eval/episode_forward_reward': Array(67.751, dtype=float32), 'eval/episode_reward': Array(680.801, dtype=float32), 'eval/episode_reward_alive': Array(673.633, dtype=float32), 'eval/episode_reward_linvel': Array(67.751, dtype=float32), 'eval/episode_reward_quadctrl': Array(-60.583, dtype=float32), 'eval/episode_x_position': Array(64.192, dtype=float32), 'eval/episode_x_velocity': Array(54.201, dtype=float32), 'eval/episode_y_position': Array(-3.189, dtype=float32), 'eval/episode_y_velocity': Array(-5.128, dtype=float32), 'eval/episode_distance_from_origin_std': Array(35.805, dtype=float32), 'eval/episode_forward_reward_std': Array(23.047, dtype=float32), 'eval/episode_reward_std': Array(132.327, dtype=float32), 'eval/episode_reward_alive_std': Array(133.552, dtype=float32), 'eval/episode_reward_linvel_std': Array(23.047, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(12.116, dtype=float32), 'eval/episode_x_position_std': Array(27.797, dtype=float32), 'eval/episode_x_velocity_std': Array(18.438, dtype=float32), 'eval/episode_y_position_std': Array(12.008, dtype=float32), 'eval/episode_y_velocity_std': Array(12.544, dtype=float32), 'eval/avg_episode_length': Array(134.727, dtype=float32), 'eval/epoch_eval_time': 10.404926776885986, 'eval/sps': 12301.864563270687}
time to jit: 0:00:55.612336
time to train: 0:04:38.376004

--------

Iteration1-Robot3

--------

{'eval/walltime': 52.30129790306091, 'training/sps': 34641.02260553135, 'training/walltime': 75.67444038391113, 'training/entropy_loss': Array(-0.012, dtype=float32), 'training/policy_loss': Array(0., dtype=float32), 'training/total_loss': Array(0.081, dtype=float32), 'training/v_loss': Array(0.092, dtype=float32), 'eval/episode_distance_from_origin': Array(43.265, dtype=float32), 'eval/episode_forward_reward': Array(11.979, dtype=float32), 'eval/episode_reward': Array(242.31, dtype=float32), 'eval/episode_reward_alive': Array(259.102, dtype=float32), 'eval/episode_reward_linvel': Array(11.979, dtype=float32), 'eval/episode_reward_quadctrl': Array(-28.77, dtype=float32), 'eval/episode_x_position': Array(3.324, dtype=float32), 'eval/episode_x_velocity': Array(9.583, dtype=float32), 'eval/episode_y_position': Array(-0.078, dtype=float32), 'eval/episode_y_velocity': Array(0.124, dtype=float32), 'eval/episode_distance_from_origin_std': Array(11.008, dtype=float32), 'eval/episode_forward_reward_std': Array(15.255, dtype=float32), 'eval/episode_reward_std': Array(56.721, dtype=float32), 'eval/episode_reward_alive_std': Array(66.024, dtype=float32), 'eval/episode_reward_linvel_std': Array(15.255, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(7.002, dtype=float32), 'eval/episode_x_position_std': Array(4.07, dtype=float32), 'eval/episode_x_velocity_std': Array(12.204, dtype=float32), 'eval/episode_y_position_std': Array(4.143, dtype=float32), 'eval/episode_y_velocity_std': Array(10.224, dtype=float32), 'eval/avg_episode_length': Array(51.82, dtype=float32), 'eval/epoch_eval_time': 10.393136739730835, 'eval/sps': 12315.819872809157}
{'eval/walltime': 62.70020842552185, 'training/sps': 49151.191712510976, 'training/walltime': 129.00865077972412, 'training/entropy_loss': Array(-0.01, dtype=float32), 'training/policy_loss': Array(-0.015, dtype=float32), 'training/total_loss': Array(0.006, dtype=float32), 'training/v_loss': Array(0.031, dtype=float32), 'eval/episode_distance_from_origin': Array(60.252, dtype=float32), 'eval/episode_forward_reward': Array(16.673, dtype=float32), 'eval/episode_reward': Array(335.396, dtype=float32), 'eval/episode_reward_alive': Array(355.781, dtype=float32), 'eval/episode_reward_linvel': Array(16.673, dtype=float32), 'eval/episode_reward_quadctrl': Array(-37.058, dtype=float32), 'eval/episode_x_position': Array(7.566, dtype=float32), 'eval/episode_x_velocity': Array(13.338, dtype=float32), 'eval/episode_y_position': Array(1.759, dtype=float32), 'eval/episode_y_velocity': Array(3.139, dtype=float32), 'eval/episode_distance_from_origin_std': Array(12.009, dtype=float32), 'eval/episode_forward_reward_std': Array(16.869, dtype=float32), 'eval/episode_reward_std': Array(63.225, dtype=float32), 'eval/episode_reward_alive_std': Array(71.076, dtype=float32), 'eval/episode_reward_linvel_std': Array(16.869, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(7.737, dtype=float32), 'eval/episode_x_position_std': Array(5.415, dtype=float32), 'eval/episode_x_velocity_std': Array(13.495, dtype=float32), 'eval/episode_y_position_std': Array(6.914, dtype=float32), 'eval/episode_y_velocity_std': Array(12.867, dtype=float32), 'eval/avg_episode_length': Array(71.156, dtype=float32), 'eval/epoch_eval_time': 10.398910522460938, 'eval/sps': 12308.98176530404}
{'eval/walltime': 73.08968424797058, 'training/sps': 48966.12244507874, 'training/walltime': 182.54443979263306, 'training/entropy_loss': Array(-0.008, dtype=float32), 'training/policy_loss': Array(-0.013, dtype=float32), 'training/total_loss': Array(0.012, dtype=float32), 'training/v_loss': Array(0.034, dtype=float32), 'eval/episode_distance_from_origin': Array(89.501, dtype=float32), 'eval/episode_forward_reward': Array(30.859, dtype=float32), 'eval/episode_reward': Array(479.396, dtype=float32), 'eval/episode_reward_alive': Array(498.594, dtype=float32), 'eval/episode_reward_linvel': Array(30.859, dtype=float32), 'eval/episode_reward_quadctrl': Array(-50.056, dtype=float32), 'eval/episode_x_position': Array(23.846, dtype=float32), 'eval/episode_x_velocity': Array(24.687, dtype=float32), 'eval/episode_y_position': Array(6.663, dtype=float32), 'eval/episode_y_velocity': Array(4.643, dtype=float32), 'eval/episode_distance_from_origin_std': Array(23.634, dtype=float32), 'eval/episode_forward_reward_std': Array(21.708, dtype=float32), 'eval/episode_reward_std': Array(109.183, dtype=float32), 'eval/episode_reward_alive_std': Array(118.293, dtype=float32), 'eval/episode_reward_linvel_std': Array(21.708, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(11.9, dtype=float32), 'eval/episode_x_position_std': Array(14.575, dtype=float32), 'eval/episode_x_velocity_std': Array(17.366, dtype=float32), 'eval/episode_y_position_std': Array(10.745, dtype=float32), 'eval/episode_y_velocity_std': Array(15.061, dtype=float32), 'eval/avg_episode_length': Array(99.719, dtype=float32), 'eval/epoch_eval_time': 10.38947582244873, 'eval/sps': 12320.159571807084}
{'eval/walltime': 83.46840357780457, 'training/sps': 48948.30929679942, 'training/walltime': 236.09971141815186, 'training/entropy_loss': Array(-0.007, dtype=float32), 'training/policy_loss': Array(-0.011, dtype=float32), 'training/total_loss': Array(0.027, dtype=float32), 'training/v_loss': Array(0.046, dtype=float32), 'eval/episode_distance_from_origin': Array(145.228, dtype=float32), 'eval/episode_forward_reward': Array(54.064, dtype=float32), 'eval/episode_reward': Array(686.758, dtype=float32), 'eval/episode_reward_alive': Array(700.547, dtype=float32), 'eval/episode_reward_linvel': Array(54.064, dtype=float32), 'eval/episode_reward_quadctrl': Array(-67.854, dtype=float32), 'eval/episode_x_position': Array(69.021, dtype=float32), 'eval/episode_x_velocity': Array(43.252, dtype=float32), 'eval/episode_y_position': Array(5.162, dtype=float32), 'eval/episode_y_velocity': Array(0.781, dtype=float32), 'eval/episode_distance_from_origin_std': Array(63.112, dtype=float32), 'eval/episode_forward_reward_std': Array(24.112, dtype=float32), 'eval/episode_reward_std': Array(209.974, dtype=float32), 'eval/episode_reward_alive_std': Array(221.268, dtype=float32), 'eval/episode_reward_linvel_std': Array(24.112, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(21.152, dtype=float32), 'eval/episode_x_position_std': Array(51.346, dtype=float32), 'eval/episode_x_velocity_std': Array(19.289, dtype=float32), 'eval/episode_y_position_std': Array(18.29, dtype=float32), 'eval/episode_y_velocity_std': Array(19.075, dtype=float32), 'eval/avg_episode_length': Array(140.109, dtype=float32), 'eval/epoch_eval_time': 10.378719329833984, 'eval/sps': 12332.928170825433}
time to jit: 0:00:56.391497
time to train: 0:04:37.891171
2

--------

Iteration2-Robot0

--------

{'eval/walltime': 51.83934569358826, 'training/sps': 34666.15587809029, 'training/walltime': 75.61957573890686, 'training/entropy_loss': Array(-0.008, dtype=float32), 'training/policy_loss': Array(0.011, dtype=float32), 'training/total_loss': Array(0.102, dtype=float32), 'training/v_loss': Array(0.1, dtype=float32), 'eval/episode_distance_from_origin': Array(62.158, dtype=float32), 'eval/episode_forward_reward': Array(28.549, dtype=float32), 'eval/episode_reward': Array(350.45, dtype=float32), 'eval/episode_reward_alive': Array(359.531, dtype=float32), 'eval/episode_reward_linvel': Array(28.549, dtype=float32), 'eval/episode_reward_quadctrl': Array(-37.63, dtype=float32), 'eval/episode_x_position': Array(15.056, dtype=float32), 'eval/episode_x_velocity': Array(22.839, dtype=float32), 'eval/episode_y_position': Array(7.782, dtype=float32), 'eval/episode_y_velocity': Array(13.429, dtype=float32), 'eval/episode_distance_from_origin_std': Array(20.768, dtype=float32), 'eval/episode_forward_reward_std': Array(21.652, dtype=float32), 'eval/episode_reward_std': Array(103.122, dtype=float32), 'eval/episode_reward_alive_std': Array(109.36, dtype=float32), 'eval/episode_reward_linvel_std': Array(21.652, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(10.86, dtype=float32), 'eval/episode_x_position_std': Array(11.392, dtype=float32), 'eval/episode_x_velocity_std': Array(17.321, dtype=float32), 'eval/episode_y_position_std': Array(6.572, dtype=float32), 'eval/episode_y_velocity_std': Array(8.712, dtype=float32), 'eval/avg_episode_length': Array(71.906, dtype=float32), 'eval/epoch_eval_time': 10.327104330062866, 'eval/sps': 12394.56830385491}
{'eval/walltime': 62.26831865310669, 'training/sps': 49225.34223365864, 'training/walltime': 128.87344622612, 'training/entropy_loss': Array(-0.006, dtype=float32), 'training/policy_loss': Array(-0.012, dtype=float32), 'training/total_loss': Array(0.065, dtype=float32), 'training/v_loss': Array(0.084, dtype=float32), 'eval/episode_distance_from_origin': Array(116.677, dtype=float32), 'eval/episode_forward_reward': Array(60.064, dtype=float32), 'eval/episode_reward': Array(569.763, dtype=float32), 'eval/episode_reward_alive': Array(567.773, dtype=float32), 'eval/episode_reward_linvel': Array(60.064, dtype=float32), 'eval/episode_reward_quadctrl': Array(-58.075, dtype=float32), 'eval/episode_x_position': Array(56.352, dtype=float32), 'eval/episode_x_velocity': Array(48.051, dtype=float32), 'eval/episode_y_position': Array(15.654, dtype=float32), 'eval/episode_y_velocity': Array(14.54, dtype=float32), 'eval/episode_distance_from_origin_std': Array(48.213, dtype=float32), 'eval/episode_forward_reward_std': Array(26.973, dtype=float32), 'eval/episode_reward_std': Array(160.359, dtype=float32), 'eval/episode_reward_alive_std': Array(163.468, dtype=float32), 'eval/episode_reward_linvel_std': Array(26.973, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(16.058, dtype=float32), 'eval/episode_x_position_std': Array(37.416, dtype=float32), 'eval/episode_x_velocity_std': Array(21.578, dtype=float32), 'eval/episode_y_position_std': Array(18.863, dtype=float32), 'eval/episode_y_velocity_std': Array(13.116, dtype=float32), 'eval/avg_episode_length': Array(113.555, dtype=float32), 'eval/epoch_eval_time': 10.428972959518433, 'eval/sps': 12273.50003656645}
{'eval/walltime': 72.64683246612549, 'training/sps': 49050.617068349275, 'training/walltime': 182.3170144557953, 'training/entropy_loss': Array(-0.005, dtype=float32), 'training/policy_loss': Array(-0.01, dtype=float32), 'training/total_loss': Array(0.054, dtype=float32), 'training/v_loss': Array(0.07, dtype=float32), 'eval/episode_distance_from_origin': Array(243.989, dtype=float32), 'eval/episode_forward_reward': Array(106.938, dtype=float32), 'eval/episode_reward': Array(878.144, dtype=float32), 'eval/episode_reward_alive': Array(854.805, dtype=float32), 'eval/episode_reward_linvel': Array(106.938, dtype=float32), 'eval/episode_reward_quadctrl': Array(-83.598, dtype=float32), 'eval/episode_x_position': Array(172.098, dtype=float32), 'eval/episode_x_velocity': Array(85.55, dtype=float32), 'eval/episode_y_position': Array(35.405, dtype=float32), 'eval/episode_y_velocity': Array(22.194, dtype=float32), 'eval/episode_distance_from_origin_std': Array(213.093, dtype=float32), 'eval/episode_forward_reward_std': Array(47.854, dtype=float32), 'eval/episode_reward_std': Array(358.703, dtype=float32), 'eval/episode_reward_alive_std': Array(355.602, dtype=float32), 'eval/episode_reward_linvel_std': Array(47.854, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(33.262, dtype=float32), 'eval/episode_x_position_std': Array(195.727, dtype=float32), 'eval/episode_x_velocity_std': Array(38.283, dtype=float32), 'eval/episode_y_position_std': Array(64.633, dtype=float32), 'eval/episode_y_velocity_std': Array(20.03, dtype=float32), 'eval/avg_episode_length': Array(170.961, dtype=float32), 'eval/epoch_eval_time': 10.378513813018799, 'eval/sps': 12333.172389233312}
{'eval/walltime': 82.93703055381775, 'training/sps': 48967.02853367871, 'training/walltime': 235.85181283950806, 'training/entropy_loss': Array(-0.004, dtype=float32), 'training/policy_loss': Array(-0.009, dtype=float32), 'training/total_loss': Array(0.046, dtype=float32), 'training/v_loss': Array(0.059, dtype=float32), 'eval/episode_distance_from_origin': Array(684.269, dtype=float32), 'eval/episode_forward_reward': Array(213.097, dtype=float32), 'eval/episode_reward': Array(1429.03, dtype=float32), 'eval/episode_reward_alive': Array(1345.938, dtype=float32), 'eval/episode_reward_linvel': Array(213.097, dtype=float32), 'eval/episode_reward_quadctrl': Array(-130.005, dtype=float32), 'eval/episode_x_position': Array(597.597, dtype=float32), 'eval/episode_x_velocity': Array(170.478, dtype=float32), 'eval/episode_y_position': Array(120.616, dtype=float32), 'eval/episode_y_velocity': Array(40.021, dtype=float32), 'eval/episode_distance_from_origin_std': Array(653.766, dtype=float32), 'eval/episode_forward_reward_std': Array(99.657, dtype=float32), 'eval/episode_reward_std': Array(674.761, dtype=float32), 'eval/episode_reward_alive_std': Array(637.95, dtype=float32), 'eval/episode_reward_linvel_std': Array(99.657, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(59.874, dtype=float32), 'eval/episode_x_position_std': Array(624.821, dtype=float32), 'eval/episode_x_velocity_std': Array(79.725, dtype=float32), 'eval/episode_y_position_std': Array(165.674, dtype=float32), 'eval/episode_y_velocity_std': Array(35.324, dtype=float32), 'eval/avg_episode_length': Array(269.188, dtype=float32), 'eval/epoch_eval_time': 10.29019808769226, 'eval/sps': 12439.021961403856}
time to jit: 0:00:55.775102
time to train: 0:04:37.545007

--------

Iteration2-Robot1

--------

{'eval/walltime': 52.64766454696655, 'training/sps': 34512.415253408406, 'training/walltime': 75.95643424987793, 'training/entropy_loss': Array(-0.003, dtype=float32), 'training/policy_loss': Array(-0.003, dtype=float32), 'training/total_loss': Array(0.049, dtype=float32), 'training/v_loss': Array(0.055, dtype=float32), 'eval/episode_distance_from_origin': Array(58.543, dtype=float32), 'eval/episode_forward_reward': Array(31.132, dtype=float32), 'eval/episode_reward': Array(308.005, dtype=float32), 'eval/episode_reward_alive': Array(314.531, dtype=float32), 'eval/episode_reward_linvel': Array(31.132, dtype=float32), 'eval/episode_reward_quadctrl': Array(-37.659, dtype=float32), 'eval/episode_x_position': Array(16.566, dtype=float32), 'eval/episode_x_velocity': Array(24.906, dtype=float32), 'eval/episode_y_position': Array(6.971, dtype=float32), 'eval/episode_y_velocity': Array(3.569, dtype=float32), 'eval/episode_distance_from_origin_std': Array(10.944, dtype=float32), 'eval/episode_forward_reward_std': Array(11.573, dtype=float32), 'eval/episode_reward_std': Array(50.696, dtype=float32), 'eval/episode_reward_alive_std': Array(57.413, dtype=float32), 'eval/episode_reward_linvel_std': Array(11.573, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(6.52, dtype=float32), 'eval/episode_x_position_std': Array(5.365, dtype=float32), 'eval/episode_x_velocity_std': Array(9.258, dtype=float32), 'eval/episode_y_position_std': Array(5.378, dtype=float32), 'eval/episode_y_velocity_std': Array(10.493, dtype=float32), 'eval/avg_episode_length': Array(62.906, dtype=float32), 'eval/epoch_eval_time': 10.357356786727905, 'eval/sps': 12358.365424277108}
{'eval/walltime': 63.027968645095825, 'training/sps': 49183.130861758116, 'training/walltime': 129.2560098171234, 'training/entropy_loss': Array(-0.002, dtype=float32), 'training/policy_loss': Array(-0.008, dtype=float32), 'training/total_loss': Array(0.022, dtype=float32), 'training/v_loss': Array(0.032, dtype=float32), 'eval/episode_distance_from_origin': Array(72.23, dtype=float32), 'eval/episode_forward_reward': Array(39.764, dtype=float32), 'eval/episode_reward': Array(371.278, dtype=float32), 'eval/episode_reward_alive': Array(373.516, dtype=float32), 'eval/episode_reward_linvel': Array(39.764, dtype=float32), 'eval/episode_reward_quadctrl': Array(-42.001, dtype=float32), 'eval/episode_x_position': Array(26.667, dtype=float32), 'eval/episode_x_velocity': Array(31.811, dtype=float32), 'eval/episode_y_position': Array(9.212, dtype=float32), 'eval/episode_y_velocity': Array(1.538, dtype=float32), 'eval/episode_distance_from_origin_std': Array(11.628, dtype=float32), 'eval/episode_forward_reward_std': Array(15.178, dtype=float32), 'eval/episode_reward_std': Array(53.619, dtype=float32), 'eval/episode_reward_alive_std': Array(59.583, dtype=float32), 'eval/episode_reward_linvel_std': Array(15.178, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(6.812, dtype=float32), 'eval/episode_x_position_std': Array(7.095, dtype=float32), 'eval/episode_x_velocity_std': Array(12.142, dtype=float32), 'eval/episode_y_position_std': Array(5.822, dtype=float32), 'eval/episode_y_velocity_std': Array(11.442, dtype=float32), 'eval/avg_episode_length': Array(74.703, dtype=float32), 'eval/epoch_eval_time': 10.380304098129272, 'eval/sps': 12331.045294045674}
time to jit: 0:00:57.155873
time to train: 0:02:30.169523

--------

Iteration2-Robot2

--------

{'eval/walltime': 52.57614231109619, 'training/sps': 34641.48154280897, 'training/walltime': 75.67343783378601, 'training/entropy_loss': Array(-0.008, dtype=float32), 'training/policy_loss': Array(0.02, dtype=float32), 'training/total_loss': Array(0.151, dtype=float32), 'training/v_loss': Array(0.138, dtype=float32), 'eval/episode_distance_from_origin': Array(35.487, dtype=float32), 'eval/episode_forward_reward': Array(12.993, dtype=float32), 'eval/episode_reward': Array(210.754, dtype=float32), 'eval/episode_reward_alive': Array(223.242, dtype=float32), 'eval/episode_reward_linvel': Array(12.993, dtype=float32), 'eval/episode_reward_quadctrl': Array(-25.481, dtype=float32), 'eval/episode_x_position': Array(5.748, dtype=float32), 'eval/episode_x_velocity': Array(10.394, dtype=float32), 'eval/episode_y_position': Array(0.572, dtype=float32), 'eval/episode_y_velocity': Array(0.529, dtype=float32), 'eval/episode_distance_from_origin_std': Array(8.696, dtype=float32), 'eval/episode_forward_reward_std': Array(7.502, dtype=float32), 'eval/episode_reward_std': Array(51.205, dtype=float32), 'eval/episode_reward_alive_std': Array(58.529, dtype=float32), 'eval/episode_reward_linvel_std': Array(7.502, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(7.14, dtype=float32), 'eval/episode_x_position_std': Array(2.772, dtype=float32), 'eval/episode_x_velocity_std': Array(6.002, dtype=float32), 'eval/episode_y_position_std': Array(2.786, dtype=float32), 'eval/episode_y_velocity_std': Array(7.84, dtype=float32), 'eval/avg_episode_length': Array(44.648, dtype=float32), 'eval/epoch_eval_time': 10.274426221847534, 'eval/sps': 12458.116612665033}
{'eval/walltime': 62.909769773483276, 'training/sps': 49200.690446235785, 'training/walltime': 128.9539909362793, 'training/entropy_loss': Array(-0.008, dtype=float32), 'training/policy_loss': Array(-0.013, dtype=float32), 'training/total_loss': Array(0.062, dtype=float32), 'training/v_loss': Array(0.083, dtype=float32), 'eval/episode_distance_from_origin': Array(50.178, dtype=float32), 'eval/episode_forward_reward': Array(17.164, dtype=float32), 'eval/episode_reward': Array(296.027, dtype=float32), 'eval/episode_reward_alive': Array(310.625, dtype=float32), 'eval/episode_reward_linvel': Array(17.164, dtype=float32), 'eval/episode_reward_quadctrl': Array(-31.762, dtype=float32), 'eval/episode_x_position': Array(11.352, dtype=float32), 'eval/episode_x_velocity': Array(13.731, dtype=float32), 'eval/episode_y_position': Array(2.911, dtype=float32), 'eval/episode_y_velocity': Array(2.038, dtype=float32), 'eval/episode_distance_from_origin_std': Array(11.186, dtype=float32), 'eval/episode_forward_reward_std': Array(10.713, dtype=float32), 'eval/episode_reward_std': Array(63.557, dtype=float32), 'eval/episode_reward_alive_std': Array(70.014, dtype=float32), 'eval/episode_reward_linvel_std': Array(10.713, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(7.398, dtype=float32), 'eval/episode_x_position_std': Array(5.225, dtype=float32), 'eval/episode_x_velocity_std': Array(8.571, dtype=float32), 'eval/episode_y_position_std': Array(4.883, dtype=float32), 'eval/episode_y_velocity_std': Array(9.997, dtype=float32), 'eval/avg_episode_length': Array(62.125, dtype=float32), 'eval/epoch_eval_time': 10.333627462387085, 'eval/sps': 12386.744196643585}
time to jit: 0:00:57.276908
time to train: 0:02:29.753074

--------

Iteration2-Robot3

--------

{'eval/walltime': 51.51099133491516, 'training/sps': 34553.04396876585, 'training/walltime': 75.86712193489075, 'training/entropy_loss': Array(-0.008, dtype=float32), 'training/policy_loss': Array(0.014, dtype=float32), 'training/total_loss': Array(0.039, dtype=float32), 'training/v_loss': Array(0.033, dtype=float32), 'eval/episode_distance_from_origin': Array(59.86, dtype=float32), 'eval/episode_forward_reward': Array(10.394, dtype=float32), 'eval/episode_reward': Array(310.948, dtype=float32), 'eval/episode_reward_alive': Array(341.016, dtype=float32), 'eval/episode_reward_linvel': Array(10.394, dtype=float32), 'eval/episode_reward_quadctrl': Array(-40.462, dtype=float32), 'eval/episode_x_position': Array(4.259, dtype=float32), 'eval/episode_x_velocity': Array(8.315, dtype=float32), 'eval/episode_y_position': Array(2.406, dtype=float32), 'eval/episode_y_velocity': Array(6.132, dtype=float32), 'eval/episode_distance_from_origin_std': Array(12.249, dtype=float32), 'eval/episode_forward_reward_std': Array(19.297, dtype=float32), 'eval/episode_reward_std': Array(54.486, dtype=float32), 'eval/episode_reward_alive_std': Array(68.89, dtype=float32), 'eval/episode_reward_linvel_std': Array(19.297, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(7.893, dtype=float32), 'eval/episode_x_position_std': Array(4.948, dtype=float32), 'eval/episode_x_velocity_std': Array(15.437, dtype=float32), 'eval/episode_y_position_std': Array(4.391, dtype=float32), 'eval/episode_y_velocity_std': Array(9.274, dtype=float32), 'eval/avg_episode_length': Array(68.203, dtype=float32), 'eval/epoch_eval_time': 10.410915851593018, 'eval/sps': 12294.787684833145}
{'eval/walltime': 61.906909465789795, 'training/sps': 49233.971291206995, 'training/walltime': 129.1116588115692, 'training/entropy_loss': Array(-0.006, dtype=float32), 'training/policy_loss': Array(-0.013, dtype=float32), 'training/total_loss': Array(-0.003, dtype=float32), 'training/v_loss': Array(0.016, dtype=float32), 'eval/episode_distance_from_origin': Array(82.685, dtype=float32), 'eval/episode_forward_reward': Array(19.836, dtype=float32), 'eval/episode_reward': Array(434.305, dtype=float32), 'eval/episode_reward_alive': Array(465.742, dtype=float32), 'eval/episode_reward_linvel': Array(19.836, dtype=float32), 'eval/episode_reward_quadctrl': Array(-51.274, dtype=float32), 'eval/episode_x_position': Array(9.395, dtype=float32), 'eval/episode_x_velocity': Array(15.869, dtype=float32), 'eval/episode_y_position': Array(0.079, dtype=float32), 'eval/episode_y_velocity': Array(1.22, dtype=float32), 'eval/episode_distance_from_origin_std': Array(18.062, dtype=float32), 'eval/episode_forward_reward_std': Array(17.611, dtype=float32), 'eval/episode_reward_std': Array(86.678, dtype=float32), 'eval/episode_reward_alive_std': Array(100.569, dtype=float32), 'eval/episode_reward_linvel_std': Array(17.611, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(9.681, dtype=float32), 'eval/episode_x_position_std': Array(5.571, dtype=float32), 'eval/episode_x_velocity_std': Array(14.089, dtype=float32), 'eval/episode_y_position_std': Array(6.203, dtype=float32), 'eval/episode_y_velocity_std': Array(10.225, dtype=float32), 'eval/avg_episode_length': Array(93.148, dtype=float32), 'eval/epoch_eval_time': 10.395918130874634, 'eval/sps': 12312.524818741627}
{'eval/walltime': 72.30282068252563, 'training/sps': 49034.384387485115, 'training/walltime': 182.5729193687439, 'training/entropy_loss': Array(-0.005, dtype=float32), 'training/policy_loss': Array(-0.014, dtype=float32), 'training/total_loss': Array(-0.004, dtype=float32), 'training/v_loss': Array(0.015, dtype=float32), 'eval/episode_distance_from_origin': Array(243.105, dtype=float32), 'eval/episode_forward_reward': Array(13.315, dtype=float32), 'eval/episode_reward': Array(1238.302, dtype=float32), 'eval/episode_reward_alive': Array(1352.539, dtype=float32), 'eval/episode_reward_linvel': Array(13.315, dtype=float32), 'eval/episode_reward_quadctrl': Array(-127.552, dtype=float32), 'eval/episode_x_position': Array(37.111, dtype=float32), 'eval/episode_x_velocity': Array(10.652, dtype=float32), 'eval/episode_y_position': Array(-12.223, dtype=float32), 'eval/episode_y_velocity': Array(0.531, dtype=float32), 'eval/episode_distance_from_origin_std': Array(102.16, dtype=float32), 'eval/episode_forward_reward_std': Array(27.546, dtype=float32), 'eval/episode_reward_std': Array(503.976, dtype=float32), 'eval/episode_reward_alive_std': Array(543.201, dtype=float32), 'eval/episode_reward_linvel_std': Array(27.546, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(49.545, dtype=float32), 'eval/episode_x_position_std': Array(37.801, dtype=float32), 'eval/episode_x_velocity_std': Array(22.036, dtype=float32), 'eval/episode_y_position_std': Array(20.555, dtype=float32), 'eval/episode_y_velocity_std': Array(7.848, dtype=float32), 'eval/avg_episode_length': Array(270.508, dtype=float32), 'eval/epoch_eval_time': 10.39591121673584, 'eval/sps': 12312.533007586619}
{'eval/walltime': 82.72633647918701, 'training/sps': 49019.05385363552, 'training/walltime': 236.0508997440338, 'training/entropy_loss': Array(-0.005, dtype=float32), 'training/policy_loss': Array(-0.015, dtype=float32), 'training/total_loss': Array(-0.014, dtype=float32), 'training/v_loss': Array(0.006, dtype=float32), 'eval/episode_distance_from_origin': Array(891.771, dtype=float32), 'eval/episode_forward_reward': Array(29.38, dtype=float32), 'eval/episode_reward': Array(4450.688, dtype=float32), 'eval/episode_reward_alive': Array(4721.562, dtype=float32), 'eval/episode_reward_linvel': Array(29.38, dtype=float32), 'eval/episode_reward_quadctrl': Array(-300.254, dtype=float32), 'eval/episode_x_position': Array(252.349, dtype=float32), 'eval/episode_x_velocity': Array(23.504, dtype=float32), 'eval/episode_y_position': Array(-147.938, dtype=float32), 'eval/episode_y_velocity': Array(-13.291, dtype=float32), 'eval/episode_distance_from_origin_std': Array(164.462, dtype=float32), 'eval/episode_forward_reward_std': Array(10.915, dtype=float32), 'eval/episode_reward_std': Array(797.75, dtype=float32), 'eval/episode_reward_alive_std': Array(845.289, dtype=float32), 'eval/episode_reward_linvel_std': Array(10.915, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(50.984, dtype=float32), 'eval/episode_x_position_std': Array(74.873, dtype=float32), 'eval/episode_x_velocity_std': Array(8.732, dtype=float32), 'eval/episode_y_position_std': Array(76.392, dtype=float32), 'eval/episode_y_velocity_std': Array(8.038, dtype=float32), 'eval/avg_episode_length': Array(944.312, dtype=float32), 'eval/epoch_eval_time': 10.423515796661377, 'eval/sps': 12279.925746454766}
time to jit: 0:00:55.957913
time to train: 0:04:37.943092
3

--------

Iteration3-Robot0

--------

{'eval/walltime': 51.34349322319031, 'training/sps': 34414.527586462435, 'training/walltime': 76.17248249053955, 'training/entropy_loss': Array(-0.002, dtype=float32), 'training/policy_loss': Array(-0.013, dtype=float32), 'training/total_loss': Array(-0.003, dtype=float32), 'training/v_loss': Array(0.012, dtype=float32), 'eval/episode_distance_from_origin': Array(300.855, dtype=float32), 'eval/episode_forward_reward': Array(30.029, dtype=float32), 'eval/episode_reward': Array(1550.921, dtype=float32), 'eval/episode_reward_alive': Array(1670.039, dtype=float32), 'eval/episode_reward_linvel': Array(30.029, dtype=float32), 'eval/episode_reward_quadctrl': Array(-149.146, dtype=float32), 'eval/episode_x_position': Array(62.158, dtype=float32), 'eval/episode_x_velocity': Array(24.023, dtype=float32), 'eval/episode_y_position': Array(-26.407, dtype=float32), 'eval/episode_y_velocity': Array(-10.733, dtype=float32), 'eval/episode_distance_from_origin_std': Array(87.245, dtype=float32), 'eval/episode_forward_reward_std': Array(17.216, dtype=float32), 'eval/episode_reward_std': Array(429.25, dtype=float32), 'eval/episode_reward_alive_std': Array(465.447, dtype=float32), 'eval/episode_reward_linvel_std': Array(17.216, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(40.825, dtype=float32), 'eval/episode_x_position_std': Array(32.784, dtype=float32), 'eval/episode_x_velocity_std': Array(13.773, dtype=float32), 'eval/episode_y_position_std': Array(18.784, dtype=float32), 'eval/episode_y_velocity_std': Array(10.944, dtype=float32), 'eval/avg_episode_length': Array(334.008, dtype=float32), 'eval/epoch_eval_time': 10.332359552383423, 'eval/sps': 12388.264205388936}
{'eval/walltime': 61.70549297332764, 'training/sps': 49235.74517671232, 'training/walltime': 129.41510105133057, 'training/entropy_loss': Array(-0.002, dtype=float32), 'training/policy_loss': Array(-0.014, dtype=float32), 'training/total_loss': Array(-0.012, dtype=float32), 'training/v_loss': Array(0.004, dtype=float32), 'eval/episode_distance_from_origin': Array(905.485, dtype=float32), 'eval/episode_forward_reward': Array(15.581, dtype=float32), 'eval/episode_reward': Array(4786.914, dtype=float32), 'eval/episode_reward_alive': Array(5000., dtype=float32), 'eval/episode_reward_linvel': Array(15.581, dtype=float32), 'eval/episode_reward_quadctrl': Array(-228.666, dtype=float32), 'eval/episode_x_position': Array(183.674, dtype=float32), 'eval/episode_x_velocity': Array(12.464, dtype=float32), 'eval/episode_y_position': Array(-98.424, dtype=float32), 'eval/episode_y_velocity': Array(-7.2, dtype=float32), 'eval/episode_distance_from_origin_std': Array(8.033, dtype=float32), 'eval/episode_forward_reward_std': Array(2.183, dtype=float32), 'eval/episode_reward_std': Array(9.35, dtype=float32), 'eval/episode_reward_alive_std': Array(0., dtype=float32), 'eval/episode_reward_linvel_std': Array(2.183, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(9.401, dtype=float32), 'eval/episode_x_position_std': Array(30.498, dtype=float32), 'eval/episode_x_velocity_std': Array(1.746, dtype=float32), 'eval/episode_y_position_std': Array(37.773, dtype=float32), 'eval/episode_y_velocity_std': Array(2.835, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/epoch_eval_time': 10.361999750137329, 'eval/sps': 12352.827937320071}
{'eval/walltime': 72.05538654327393, 'training/sps': 48989.57203608025, 'training/walltime': 182.9252643585205, 'training/entropy_loss': Array(0.003, dtype=float32), 'training/policy_loss': Array(-0.037, dtype=float32), 'training/total_loss': Array(-0.034, dtype=float32), 'training/v_loss': Array(0., dtype=float32), 'eval/episode_distance_from_origin': Array(893.555, dtype=float32), 'eval/episode_forward_reward': Array(10.428, dtype=float32), 'eval/episode_reward': Array(4871.847, dtype=float32), 'eval/episode_reward_alive': Array(4999.336, dtype=float32), 'eval/episode_reward_linvel': Array(10.428, dtype=float32), 'eval/episode_reward_quadctrl': Array(-137.917, dtype=float32), 'eval/episode_x_position': Array(136.463, dtype=float32), 'eval/episode_x_velocity': Array(8.342, dtype=float32), 'eval/episode_y_position': Array(-67.276, dtype=float32), 'eval/episode_y_velocity': Array(-5.026, dtype=float32), 'eval/episode_distance_from_origin_std': Array(5.444, dtype=float32), 'eval/episode_forward_reward_std': Array(3.172, dtype=float32), 'eval/episode_reward_std': Array(8.366, dtype=float32), 'eval/episode_reward_alive_std': Array(7.484, dtype=float32), 'eval/episode_reward_linvel_std': Array(3.172, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(5.138, dtype=float32), 'eval/episode_x_position_std': Array(29.137, dtype=float32), 'eval/episode_x_velocity_std': Array(2.538, dtype=float32), 'eval/episode_y_position_std': Array(26.29, dtype=float32), 'eval/episode_y_velocity_std': Array(1.527, dtype=float32), 'eval/avg_episode_length': Array(999.867, dtype=float32), 'eval/epoch_eval_time': 10.349893569946289, 'eval/sps': 12367.276932362141}
{'eval/walltime': 82.40296196937561, 'training/sps': 48885.46444772725, 'training/walltime': 236.54938411712646, 'training/entropy_loss': Array(0.008, dtype=float32), 'training/policy_loss': Array(-0.037, dtype=float32), 'training/total_loss': Array(-0.029, dtype=float32), 'training/v_loss': Array(0., dtype=float32), 'eval/episode_distance_from_origin': Array(890.764, dtype=float32), 'eval/episode_forward_reward': Array(10.596, dtype=float32), 'eval/episode_reward': Array(4909.426, dtype=float32), 'eval/episode_reward_alive': Array(4991.836, dtype=float32), 'eval/episode_reward_linvel': Array(10.596, dtype=float32), 'eval/episode_reward_quadctrl': Array(-93.005, dtype=float32), 'eval/episode_x_position': Array(139.649, dtype=float32), 'eval/episode_x_velocity': Array(8.477, dtype=float32), 'eval/episode_y_position': Array(-53.1, dtype=float32), 'eval/episode_y_velocity': Array(-3.945, dtype=float32), 'eval/episode_distance_from_origin_std': Array(16.56, dtype=float32), 'eval/episode_forward_reward_std': Array(3.351, dtype=float32), 'eval/episode_reward_std': Array(90.733, dtype=float32), 'eval/episode_reward_alive_std': Array(92.004, dtype=float32), 'eval/episode_reward_linvel_std': Array(3.351, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(5.869, dtype=float32), 'eval/episode_x_position_std': Array(27.068, dtype=float32), 'eval/episode_x_velocity_std': Array(2.681, dtype=float32), 'eval/episode_y_position_std': Array(23.79, dtype=float32), 'eval/episode_y_velocity_std': Array(1.476, dtype=float32), 'eval/avg_episode_length': Array(998.367, dtype=float32), 'eval/epoch_eval_time': 10.347575426101685, 'eval/sps': 12370.047545352598}
time to jit: 0:00:54.610146
time to train: 0:04:38.243865

--------

Iteration3-Robot1

--------

{'eval/walltime': 51.875232458114624, 'training/sps': 34307.210276715225, 'training/walltime': 76.4107596874237, 'training/entropy_loss': Array(0.001, dtype=float32), 'training/policy_loss': Array(-0.036, dtype=float32), 'training/total_loss': Array(0.02, dtype=float32), 'training/v_loss': Array(0.055, dtype=float32), 'eval/episode_distance_from_origin': Array(55.258, dtype=float32), 'eval/episode_forward_reward': Array(33.344, dtype=float32), 'eval/episode_reward': Array(302.985, dtype=float32), 'eval/episode_reward_alive': Array(299.727, dtype=float32), 'eval/episode_reward_linvel': Array(33.344, dtype=float32), 'eval/episode_reward_quadctrl': Array(-30.086, dtype=float32), 'eval/episode_x_position': Array(15.189, dtype=float32), 'eval/episode_x_velocity': Array(26.676, dtype=float32), 'eval/episode_y_position': Array(1.339, dtype=float32), 'eval/episode_y_velocity': Array(-0.484, dtype=float32), 'eval/episode_distance_from_origin_std': Array(10.857, dtype=float32), 'eval/episode_forward_reward_std': Array(9.241, dtype=float32), 'eval/episode_reward_std': Array(49.998, dtype=float32), 'eval/episode_reward_alive_std': Array(52.296, dtype=float32), 'eval/episode_reward_linvel_std': Array(9.241, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(6.331, dtype=float32), 'eval/episode_x_position_std': Array(5.506, dtype=float32), 'eval/episode_x_velocity_std': Array(7.393, dtype=float32), 'eval/episode_y_position_std': Array(3.994, dtype=float32), 'eval/episode_y_velocity_std': Array(10.672, dtype=float32), 'eval/avg_episode_length': Array(59.945, dtype=float32), 'eval/epoch_eval_time': 10.421759605407715, 'eval/sps': 12281.995060947527}
time to jit: 0:00:55.989928
time to train: 0:01:26.913211

--------

Iteration3-Robot2

--------

{'eval/walltime': 51.79663896560669, 'training/sps': 34364.670879556776, 'training/walltime': 76.28299450874329, 'training/entropy_loss': Array(-0.001, dtype=float32), 'training/policy_loss': Array(-0.01, dtype=float32), 'training/total_loss': Array(0.003, dtype=float32), 'training/v_loss': Array(0.013, dtype=float32), 'eval/episode_distance_from_origin': Array(74.765, dtype=float32), 'eval/episode_forward_reward': Array(30.016, dtype=float32), 'eval/episode_reward': Array(422.405, dtype=float32), 'eval/episode_reward_alive': Array(430.43, dtype=float32), 'eval/episode_reward_linvel': Array(30.016, dtype=float32), 'eval/episode_reward_quadctrl': Array(-38.041, dtype=float32), 'eval/episode_x_position': Array(12.878, dtype=float32), 'eval/episode_x_velocity': Array(24.013, dtype=float32), 'eval/episode_y_position': Array(1.569, dtype=float32), 'eval/episode_y_velocity': Array(2.077, dtype=float32), 'eval/episode_distance_from_origin_std': Array(10.73, dtype=float32), 'eval/episode_forward_reward_std': Array(2.863, dtype=float32), 'eval/episode_reward_std': Array(56.388, dtype=float32), 'eval/episode_reward_alive_std': Array(59.5, dtype=float32), 'eval/episode_reward_linvel_std': Array(2.863, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(4.623, dtype=float32), 'eval/episode_x_position_std': Array(3.178, dtype=float32), 'eval/episode_x_velocity_std': Array(2.291, dtype=float32), 'eval/episode_y_position_std': Array(2.6, dtype=float32), 'eval/episode_y_velocity_std': Array(3.385, dtype=float32), 'eval/avg_episode_length': Array(86.086, dtype=float32), 'eval/epoch_eval_time': 10.259281396865845, 'eval/sps': 12476.507374006069}
time to jit: 0:00:56.333843
time to train: 0:01:26.652854

--------

Iteration3-Robot3

--------

{'eval/walltime': 51.87453889846802, 'training/sps': 34346.32280164593, 'training/walltime': 76.32374548912048, 'training/entropy_loss': Array(-0.004, dtype=float32), 'training/policy_loss': Array(-0.009, dtype=float32), 'training/total_loss': Array(0.024, dtype=float32), 'training/v_loss': Array(0.038, dtype=float32), 'eval/episode_distance_from_origin': Array(97.579, dtype=float32), 'eval/episode_forward_reward': Array(34.25, dtype=float32), 'eval/episode_reward': Array(462.823, dtype=float32), 'eval/episode_reward_alive': Array(471.016, dtype=float32), 'eval/episode_reward_linvel': Array(34.25, dtype=float32), 'eval/episode_reward_quadctrl': Array(-42.442, dtype=float32), 'eval/episode_x_position': Array(19.284, dtype=float32), 'eval/episode_x_velocity': Array(27.4, dtype=float32), 'eval/episode_y_position': Array(11.372, dtype=float32), 'eval/episode_y_velocity': Array(11.299, dtype=float32), 'eval/episode_distance_from_origin_std': Array(17.695, dtype=float32), 'eval/episode_forward_reward_std': Array(20.885, dtype=float32), 'eval/episode_reward_std': Array(79.593, dtype=float32), 'eval/episode_reward_alive_std': Array(82.475, dtype=float32), 'eval/episode_reward_linvel_std': Array(20.885, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(7.054, dtype=float32), 'eval/episode_x_position_std': Array(9.643, dtype=float32), 'eval/episode_x_velocity_std': Array(16.708, dtype=float32), 'eval/episode_y_position_std': Array(8.468, dtype=float32), 'eval/episode_y_velocity_std': Array(15.527, dtype=float32), 'eval/avg_episode_length': Array(94.203, dtype=float32), 'eval/epoch_eval_time': 10.281444787979126, 'eval/sps': 12449.612154670638}
time to jit: 0:00:56.478233
time to train: 0:01:26.690477
4

--------

Iteration4-Robot0

--------

{'eval/walltime': 51.375187158584595, 'training/sps': 34394.55124297246, 'training/walltime': 76.21672344207764, 'training/entropy_loss': Array(0.006, dtype=float32), 'training/policy_loss': Array(0.042, dtype=float32), 'training/total_loss': Array(0.094, dtype=float32), 'training/v_loss': Array(0.045, dtype=float32), 'eval/episode_distance_from_origin': Array(66.832, dtype=float32), 'eval/episode_forward_reward': Array(-8.662, dtype=float32), 'eval/episode_reward': Array(313.787, dtype=float32), 'eval/episode_reward_alive': Array(378.594, dtype=float32), 'eval/episode_reward_linvel': Array(-8.662, dtype=float32), 'eval/episode_reward_quadctrl': Array(-56.145, dtype=float32), 'eval/episode_x_position': Array(-3.732, dtype=float32), 'eval/episode_x_velocity': Array(-6.929, dtype=float32), 'eval/episode_y_position': Array(-6.887, dtype=float32), 'eval/episode_y_velocity': Array(-10.885, dtype=float32), 'eval/episode_distance_from_origin_std': Array(17.546, dtype=float32), 'eval/episode_forward_reward_std': Array(22.567, dtype=float32), 'eval/episode_reward_std': Array(90.386, dtype=float32), 'eval/episode_reward_alive_std': Array(97.844, dtype=float32), 'eval/episode_reward_linvel_std': Array(22.567, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(17.393, dtype=float32), 'eval/episode_x_position_std': Array(7.217, dtype=float32), 'eval/episode_x_velocity_std': Array(18.053, dtype=float32), 'eval/episode_y_position_std': Array(6.909, dtype=float32), 'eval/episode_y_velocity_std': Array(14.134, dtype=float32), 'eval/avg_episode_length': Array(75.719, dtype=float32), 'eval/epoch_eval_time': 10.243399620056152, 'eval/sps': 12495.851450467802}
{'eval/walltime': 61.77178740501404, 'training/sps': 49359.19813748773, 'training/walltime': 129.32617616653442, 'training/entropy_loss': Array(0.006, dtype=float32), 'training/policy_loss': Array(-0.009, dtype=float32), 'training/total_loss': Array(0.016, dtype=float32), 'training/v_loss': Array(0.019, dtype=float32), 'eval/episode_distance_from_origin': Array(108.231, dtype=float32), 'eval/episode_forward_reward': Array(10.163, dtype=float32), 'eval/episode_reward': Array(538.146, dtype=float32), 'eval/episode_reward_alive': Array(613.945, dtype=float32), 'eval/episode_reward_linvel': Array(10.163, dtype=float32), 'eval/episode_reward_quadctrl': Array(-85.962, dtype=float32), 'eval/episode_x_position': Array(5.766, dtype=float32), 'eval/episode_x_velocity': Array(8.13, dtype=float32), 'eval/episode_y_position': Array(-13.739, dtype=float32), 'eval/episode_y_velocity': Array(-9.239, dtype=float32), 'eval/episode_distance_from_origin_std': Array(73.333, dtype=float32), 'eval/episode_forward_reward_std': Array(24.239, dtype=float32), 'eval/episode_reward_std': Array(363.941, dtype=float32), 'eval/episode_reward_alive_std': Array(413.195, dtype=float32), 'eval/episode_reward_linvel_std': Array(24.239, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(52.341, dtype=float32), 'eval/episode_x_position_std': Array(15.301, dtype=float32), 'eval/episode_x_velocity_std': Array(19.391, dtype=float32), 'eval/episode_y_position_std': Array(17.959, dtype=float32), 'eval/episode_y_velocity_std': Array(12.339, dtype=float32), 'eval/avg_episode_length': Array(122.789, dtype=float32), 'eval/epoch_eval_time': 10.396600246429443, 'eval/sps': 12311.717000368431}
{'eval/walltime': 72.16561841964722, 'training/sps': 49172.71319286365, 'training/walltime': 182.63704371452332, 'training/entropy_loss': Array(0.004, dtype=float32), 'training/policy_loss': Array(-0.01, dtype=float32), 'training/total_loss': Array(0.004, dtype=float32), 'training/v_loss': Array(0.011, dtype=float32), 'eval/episode_distance_from_origin': Array(572.208, dtype=float32), 'eval/episode_forward_reward': Array(16.301, dtype=float32), 'eval/episode_reward': Array(2867.296, dtype=float32), 'eval/episode_reward_alive': Array(3180.117, dtype=float32), 'eval/episode_reward_linvel': Array(16.301, dtype=float32), 'eval/episode_reward_quadctrl': Array(-329.122, dtype=float32), 'eval/episode_x_position': Array(83.861, dtype=float32), 'eval/episode_x_velocity': Array(13.041, dtype=float32), 'eval/episode_y_position': Array(-144.342, dtype=float32), 'eval/episode_y_velocity': Array(-13.559, dtype=float32), 'eval/episode_distance_from_origin_std': Array(370.276, dtype=float32), 'eval/episode_forward_reward_std': Array(17.266, dtype=float32), 'eval/episode_reward_std': Array(1838.527, dtype=float32), 'eval/episode_reward_alive_std': Array(2036.165, dtype=float32), 'eval/episode_reward_linvel_std': Array(17.266, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(197.745, dtype=float32), 'eval/episode_x_position_std': Array(78.204, dtype=float32), 'eval/episode_x_velocity_std': Array(13.812, dtype=float32), 'eval/episode_y_position_std': Array(119.25, dtype=float32), 'eval/episode_y_velocity_std': Array(9.721, dtype=float32), 'eval/avg_episode_length': Array(636.023, dtype=float32), 'eval/epoch_eval_time': 10.393831014633179, 'eval/sps': 12314.99721515507}
{'eval/walltime': 82.5488018989563, 'training/sps': 48978.4402694437, 'training/walltime': 236.15936875343323, 'training/entropy_loss': Array(0.002, dtype=float32), 'training/policy_loss': Array(-0.019, dtype=float32), 'training/total_loss': Array(-0.015, dtype=float32), 'training/v_loss': Array(0.002, dtype=float32), 'eval/episode_distance_from_origin': Array(804.583, dtype=float32), 'eval/episode_forward_reward': Array(5.619, dtype=float32), 'eval/episode_reward': Array(4434.384, dtype=float32), 'eval/episode_reward_alive': Array(4707.656, dtype=float32), 'eval/episode_reward_linvel': Array(5.619, dtype=float32), 'eval/episode_reward_quadctrl': Array(-278.892, dtype=float32), 'eval/episode_x_position': Array(41.876, dtype=float32), 'eval/episode_x_velocity': Array(4.495, dtype=float32), 'eval/episode_y_position': Array(-113.694, dtype=float32), 'eval/episode_y_velocity': Array(-7.282, dtype=float32), 'eval/episode_distance_from_origin_std': Array(182.525, dtype=float32), 'eval/episode_forward_reward_std': Array(7.53, dtype=float32), 'eval/episode_reward_std': Array(1015.861, dtype=float32), 'eval/episode_reward_alive_std': Array(1072.977, dtype=float32), 'eval/episode_reward_linvel_std': Array(7.53, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(58.672, dtype=float32), 'eval/episode_x_position_std': Array(36.011, dtype=float32), 'eval/episode_x_velocity_std': Array(6.024, dtype=float32), 'eval/episode_y_position_std': Array(46.688, dtype=float32), 'eval/episode_y_velocity_std': Array(4.781, dtype=float32), 'eval/avg_episode_length': Array(941.531, dtype=float32), 'eval/epoch_eval_time': 10.383183479309082, 'eval/sps': 12327.625747447293}
time to jit: 0:00:55.125294
time to train: 0:04:37.863621

--------

Iteration4-Robot1

--------

{'eval/walltime': 52.14798307418823, 'training/sps': 35182.470996975186, 'training/walltime': 74.50983190536499, 'training/entropy_loss': Array(0.006, dtype=float32), 'training/policy_loss': Array(-0.01, dtype=float32), 'training/total_loss': Array(0.061, dtype=float32), 'training/v_loss': Array(0.065, dtype=float32), 'eval/episode_distance_from_origin': Array(72.356, dtype=float32), 'eval/episode_forward_reward': Array(23.905, dtype=float32), 'eval/episode_reward': Array(402.44, dtype=float32), 'eval/episode_reward_alive': Array(410.82, dtype=float32), 'eval/episode_reward_linvel': Array(23.905, dtype=float32), 'eval/episode_reward_quadctrl': Array(-32.285, dtype=float32), 'eval/episode_x_position': Array(10.547, dtype=float32), 'eval/episode_x_velocity': Array(19.124, dtype=float32), 'eval/episode_y_position': Array(-0.788, dtype=float32), 'eval/episode_y_velocity': Array(2.372, dtype=float32), 'eval/episode_distance_from_origin_std': Array(12.122, dtype=float32), 'eval/episode_forward_reward_std': Array(15.232, dtype=float32), 'eval/episode_reward_std': Array(61.197, dtype=float32), 'eval/episode_reward_alive_std': Array(68.812, dtype=float32), 'eval/episode_reward_linvel_std': Array(15.232, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(7.254, dtype=float32), 'eval/episode_x_position_std': Array(5.859, dtype=float32), 'eval/episode_x_velocity_std': Array(12.186, dtype=float32), 'eval/episode_y_position_std': Array(4.503, dtype=float32), 'eval/episode_y_velocity_std': Array(8.379, dtype=float32), 'eval/avg_episode_length': Array(82.164, dtype=float32), 'eval/epoch_eval_time': 10.26091194152832, 'eval/sps': 12474.524752712665}
{'eval/walltime': 62.559258460998535, 'training/sps': 49190.74597306197, 'training/walltime': 127.80115628242493, 'training/entropy_loss': Array(0.005, dtype=float32), 'training/policy_loss': Array(-0.01, dtype=float32), 'training/total_loss': Array(0.016, dtype=float32), 'training/v_loss': Array(0.021, dtype=float32), 'eval/episode_distance_from_origin': Array(114.281, dtype=float32), 'eval/episode_forward_reward': Array(32.081, dtype=float32), 'eval/episode_reward': Array(630.005, dtype=float32), 'eval/episode_reward_alive': Array(645.391, dtype=float32), 'eval/episode_reward_linvel': Array(32.081, dtype=float32), 'eval/episode_reward_quadctrl': Array(-47.467, dtype=float32), 'eval/episode_x_position': Array(22.816, dtype=float32), 'eval/episode_x_velocity': Array(25.665, dtype=float32), 'eval/episode_y_position': Array(-4.116, dtype=float32), 'eval/episode_y_velocity': Array(-0.244, dtype=float32), 'eval/episode_distance_from_origin_std': Array(31.108, dtype=float32), 'eval/episode_forward_reward_std': Array(14.831, dtype=float32), 'eval/episode_reward_std': Array(167.762, dtype=float32), 'eval/episode_reward_alive_std': Array(176.311, dtype=float32), 'eval/episode_reward_linvel_std': Array(14.831, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(13.071, dtype=float32), 'eval/episode_x_position_std': Array(11.586, dtype=float32), 'eval/episode_x_velocity_std': Array(11.865, dtype=float32), 'eval/episode_y_position_std': Array(6.83, dtype=float32), 'eval/episode_y_velocity_std': Array(9.392, dtype=float32), 'eval/avg_episode_length': Array(129.078, dtype=float32), 'eval/epoch_eval_time': 10.411275386810303, 'eval/sps': 12294.363105807279}
{'eval/walltime': 72.9728569984436, 'training/sps': 48959.29329514801, 'training/walltime': 181.3444128036499, 'training/entropy_loss': Array(0.005, dtype=float32), 'training/policy_loss': Array(-0.011, dtype=float32), 'training/total_loss': Array(0.009, dtype=float32), 'training/v_loss': Array(0.015, dtype=float32), 'eval/episode_distance_from_origin': Array(672.483, dtype=float32), 'eval/episode_forward_reward': Array(24.196, dtype=float32), 'eval/episode_reward': Array(3730.089, dtype=float32), 'eval/episode_reward_alive': Array(3864.531, dtype=float32), 'eval/episode_reward_linvel': Array(24.196, dtype=float32), 'eval/episode_reward_quadctrl': Array(-158.638, dtype=float32), 'eval/episode_x_position': Array(169.766, dtype=float32), 'eval/episode_x_velocity': Array(19.357, dtype=float32), 'eval/episode_y_position': Array(-66.203, dtype=float32), 'eval/episode_y_velocity': Array(-2.912, dtype=float32), 'eval/episode_distance_from_origin_std': Array(299.891, dtype=float32), 'eval/episode_forward_reward_std': Array(16.62, dtype=float32), 'eval/episode_reward_std': Array(1673.742, dtype=float32), 'eval/episode_reward_alive_std': Array(1738.651, dtype=float32), 'eval/episode_reward_linvel_std': Array(16.62, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(63.43, dtype=float32), 'eval/episode_x_position_std': Array(88.532, dtype=float32), 'eval/episode_x_velocity_std': Array(13.296, dtype=float32), 'eval/episode_y_position_std': Array(50.287, dtype=float32), 'eval/episode_y_velocity_std': Array(7.698, dtype=float32), 'eval/avg_episode_length': Array(772.906, dtype=float32), 'eval/epoch_eval_time': 10.413598537445068, 'eval/sps': 12291.620378847852}
{'eval/walltime': 83.34475231170654, 'training/sps': 48869.13062007115, 'training/walltime': 234.98645567893982, 'training/entropy_loss': Array(0.009, dtype=float32), 'training/policy_loss': Array(-0.015, dtype=float32), 'training/total_loss': Array(-0.005, dtype=float32), 'training/v_loss': Array(0.001, dtype=float32), 'eval/episode_distance_from_origin': Array(842.985, dtype=float32), 'eval/episode_forward_reward': Array(8.04, dtype=float32), 'eval/episode_reward': Array(4894.798, dtype=float32), 'eval/episode_reward_alive': Array(5000., dtype=float32), 'eval/episode_reward_linvel': Array(8.04, dtype=float32), 'eval/episode_reward_quadctrl': Array(-113.242, dtype=float32), 'eval/episode_x_position': Array(145.266, dtype=float32), 'eval/episode_x_velocity': Array(6.432, dtype=float32), 'eval/episode_y_position': Array(-55.328, dtype=float32), 'eval/episode_y_velocity': Array(-2.977, dtype=float32), 'eval/episode_distance_from_origin_std': Array(3.793, dtype=float32), 'eval/episode_forward_reward_std': Array(0.853, dtype=float32), 'eval/episode_reward_std': Array(3.481, dtype=float32), 'eval/episode_reward_alive_std': Array(0., dtype=float32), 'eval/episode_reward_linvel_std': Array(0.853, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(3.503, dtype=float32), 'eval/episode_x_position_std': Array(17.673, dtype=float32), 'eval/episode_x_velocity_std': Array(0.683, dtype=float32), 'eval/episode_y_position_std': Array(24.699, dtype=float32), 'eval/episode_y_velocity_std': Array(1.002, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/epoch_eval_time': 10.37189531326294, 'eval/sps': 12341.042416454156}
time to jit: 0:00:56.572269
time to train: 0:04:36.704030

--------

Iteration4-Robot2

--------

{'eval/walltime': 52.99498009681702, 'training/sps': 35075.96871315377, 'training/walltime': 74.73606848716736, 'training/entropy_loss': Array(0.009, dtype=float32), 'training/policy_loss': Array(0.042, dtype=float32), 'training/total_loss': Array(0.129, dtype=float32), 'training/v_loss': Array(0.078, dtype=float32), 'eval/episode_distance_from_origin': Array(51.364, dtype=float32), 'eval/episode_forward_reward': Array(8.687, dtype=float32), 'eval/episode_reward': Array(282.984, dtype=float32), 'eval/episode_reward_alive': Array(303.516, dtype=float32), 'eval/episode_reward_linvel': Array(8.687, dtype=float32), 'eval/episode_reward_quadctrl': Array(-29.218, dtype=float32), 'eval/episode_x_position': Array(3.402, dtype=float32), 'eval/episode_x_velocity': Array(6.95, dtype=float32), 'eval/episode_y_position': Array(5.016, dtype=float32), 'eval/episode_y_velocity': Array(13.704, dtype=float32), 'eval/episode_distance_from_origin_std': Array(7.699, dtype=float32), 'eval/episode_forward_reward_std': Array(15.99, dtype=float32), 'eval/episode_reward_std': Array(43.724, dtype=float32), 'eval/episode_reward_alive_std': Array(44.583, dtype=float32), 'eval/episode_reward_linvel_std': Array(15.99, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(4.523, dtype=float32), 'eval/episode_x_position_std': Array(4.123, dtype=float32), 'eval/episode_x_velocity_std': Array(12.792, dtype=float32), 'eval/episode_y_position_std': Array(2.758, dtype=float32), 'eval/episode_y_velocity_std': Array(6.76, dtype=float32), 'eval/avg_episode_length': Array(60.703, dtype=float32), 'eval/epoch_eval_time': 10.427213907241821, 'eval/sps': 12275.57055400029}
{'eval/walltime': 63.38982534408569, 'training/sps': 49101.29888625677, 'training/walltime': 128.1244728565216, 'training/entropy_loss': Array(0.007, dtype=float32), 'training/policy_loss': Array(-0.006, dtype=float32), 'training/total_loss': Array(0.02, dtype=float32), 'training/v_loss': Array(0.019, dtype=float32), 'eval/episode_distance_from_origin': Array(63.128, dtype=float32), 'eval/episode_forward_reward': Array(10.909, dtype=float32), 'eval/episode_reward': Array(353.022, dtype=float32), 'eval/episode_reward_alive': Array(374.102, dtype=float32), 'eval/episode_reward_linvel': Array(10.909, dtype=float32), 'eval/episode_reward_quadctrl': Array(-31.989, dtype=float32), 'eval/episode_x_position': Array(4.016, dtype=float32), 'eval/episode_x_velocity': Array(8.727, dtype=float32), 'eval/episode_y_position': Array(3.712, dtype=float32), 'eval/episode_y_velocity': Array(9.184, dtype=float32), 'eval/episode_distance_from_origin_std': Array(11.501, dtype=float32), 'eval/episode_forward_reward_std': Array(19.901, dtype=float32), 'eval/episode_reward_std': Array(60.253, dtype=float32), 'eval/episode_reward_alive_std': Array(68.287, dtype=float32), 'eval/episode_reward_linvel_std': Array(19.901, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(5.45, dtype=float32), 'eval/episode_x_position_std': Array(5.614, dtype=float32), 'eval/episode_x_velocity_std': Array(15.921, dtype=float32), 'eval/episode_y_position_std': Array(3.682, dtype=float32), 'eval/episode_y_velocity_std': Array(8.177, dtype=float32), 'eval/avg_episode_length': Array(74.82, dtype=float32), 'eval/epoch_eval_time': 10.394845247268677, 'eval/sps': 12313.795631891004}
time to jit: 0:00:57.445831
time to train: 0:02:29.104474

--------

Iteration4-Robot3

--------

{'eval/walltime': 52.91149401664734, 'training/sps': 35023.96468418884, 'training/walltime': 74.84703755378723, 'training/entropy_loss': Array(0.003, dtype=float32), 'training/policy_loss': Array(-0.013, dtype=float32), 'training/total_loss': Array(0.02, dtype=float32), 'training/v_loss': Array(0.03, dtype=float32), 'eval/episode_distance_from_origin': Array(83.735, dtype=float32), 'eval/episode_forward_reward': Array(51.348, dtype=float32), 'eval/episode_reward': Array(439.225, dtype=float32), 'eval/episode_reward_alive': Array(416.992, dtype=float32), 'eval/episode_reward_linvel': Array(51.348, dtype=float32), 'eval/episode_reward_quadctrl': Array(-29.115, dtype=float32), 'eval/episode_x_position': Array(27.14, dtype=float32), 'eval/episode_x_velocity': Array(41.078, dtype=float32), 'eval/episode_y_position': Array(-6.908, dtype=float32), 'eval/episode_y_velocity': Array(-12.34, dtype=float32), 'eval/episode_distance_from_origin_std': Array(11.274, dtype=float32), 'eval/episode_forward_reward_std': Array(9.208, dtype=float32), 'eval/episode_reward_std': Array(45.781, dtype=float32), 'eval/episode_reward_alive_std': Array(45.049, dtype=float32), 'eval/episode_reward_linvel_std': Array(9.208, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(4.303, dtype=float32), 'eval/episode_x_position_std': Array(7.249, dtype=float32), 'eval/episode_x_velocity_std': Array(7.366, dtype=float32), 'eval/episode_y_position_std': Array(3.364, dtype=float32), 'eval/episode_y_velocity_std': Array(7.825, dtype=float32), 'eval/avg_episode_length': Array(83.398, dtype=float32), 'eval/epoch_eval_time': 10.402711153030396, 'eval/sps': 12304.484678757282}
{'eval/walltime': 63.31783938407898, 'training/sps': 49137.14687260093, 'training/walltime': 128.19649243354797, 'training/entropy_loss': Array(0.004, dtype=float32), 'training/policy_loss': Array(-0.011, dtype=float32), 'training/total_loss': Array(0.021, dtype=float32), 'training/v_loss': Array(0.028, dtype=float32), 'eval/episode_distance_from_origin': Array(127.106, dtype=float32), 'eval/episode_forward_reward': Array(69.71, dtype=float32), 'eval/episode_reward': Array(613.633, dtype=float32), 'eval/episode_reward_alive': Array(583.438, dtype=float32), 'eval/episode_reward_linvel': Array(69.71, dtype=float32), 'eval/episode_reward_quadctrl': Array(-39.515, dtype=float32), 'eval/episode_x_position': Array(55.488, dtype=float32), 'eval/episode_x_velocity': Array(55.768, dtype=float32), 'eval/episode_y_position': Array(-10.436, dtype=float32), 'eval/episode_y_velocity': Array(-13.613, dtype=float32), 'eval/episode_distance_from_origin_std': Array(28.505, dtype=float32), 'eval/episode_forward_reward_std': Array(17.498, dtype=float32), 'eval/episode_reward_std': Array(101.897, dtype=float32), 'eval/episode_reward_alive_std': Array(95.463, dtype=float32), 'eval/episode_reward_linvel_std': Array(17.498, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(6.52, dtype=float32), 'eval/episode_x_position_std': Array(21.887, dtype=float32), 'eval/episode_x_velocity_std': Array(13.999, dtype=float32), 'eval/episode_y_position_std': Array(5.274, dtype=float32), 'eval/episode_y_velocity_std': Array(9.39, dtype=float32), 'eval/avg_episode_length': Array(116.688, dtype=float32), 'eval/epoch_eval_time': 10.40634536743164, 'eval/sps': 12300.187575995405}
{'eval/walltime': 73.71927499771118, 'training/sps': 48996.7164025092, 'training/walltime': 181.69885325431824, 'training/entropy_loss': Array(0.004, dtype=float32), 'training/policy_loss': Array(-0.01, dtype=float32), 'training/total_loss': Array(0.023, dtype=float32), 'training/v_loss': Array(0.029, dtype=float32), 'eval/episode_distance_from_origin': Array(372.122, dtype=float32), 'eval/episode_forward_reward': Array(141.598, dtype=float32), 'eval/episode_reward': Array(1198.658, dtype=float32), 'eval/episode_reward_alive': Array(1130.664, dtype=float32), 'eval/episode_reward_linvel': Array(141.598, dtype=float32), 'eval/episode_reward_quadctrl': Array(-73.603, dtype=float32), 'eval/episode_x_position': Array(272.558, dtype=float32), 'eval/episode_x_velocity': Array(113.278, dtype=float32), 'eval/episode_y_position': Array(-30.89, dtype=float32), 'eval/episode_y_velocity': Array(-14.938, dtype=float32), 'eval/episode_distance_from_origin_std': Array(262.924, dtype=float32), 'eval/episode_forward_reward_std': Array(51.136, dtype=float32), 'eval/episode_reward_std': Array(414.713, dtype=float32), 'eval/episode_reward_alive_std': Array(391.258, dtype=float32), 'eval/episode_reward_linvel_std': Array(51.136, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(25.752, dtype=float32), 'eval/episode_x_position_std': Array(250.245, dtype=float32), 'eval/episode_x_velocity_std': Array(40.909, dtype=float32), 'eval/episode_y_position_std': Array(29.402, dtype=float32), 'eval/episode_y_velocity_std': Array(11.989, dtype=float32), 'eval/avg_episode_length': Array(226.133, dtype=float32), 'eval/epoch_eval_time': 10.401435613632202, 'eval/sps': 12305.993591138727}
time to jit: 0:00:57.633142
time to train: 0:03:33.119957
5

--------

Iteration5-Robot0

--------

{'eval/walltime': 52.83778643608093, 'training/sps': 35210.79652403462, 'training/walltime': 74.44989204406738, 'training/entropy_loss': Array(0.012, dtype=float32), 'training/policy_loss': Array(-0.02, dtype=float32), 'training/total_loss': Array(-0.006, dtype=float32), 'training/v_loss': Array(0.002, dtype=float32), 'eval/episode_distance_from_origin': Array(876.598, dtype=float32), 'eval/episode_forward_reward': Array(2.413, dtype=float32), 'eval/episode_reward': Array(4916.453, dtype=float32), 'eval/episode_reward_alive': Array(5000., dtype=float32), 'eval/episode_reward_linvel': Array(2.413, dtype=float32), 'eval/episode_reward_quadctrl': Array(-85.961, dtype=float32), 'eval/episode_x_position': Array(37.697, dtype=float32), 'eval/episode_x_velocity': Array(1.931, dtype=float32), 'eval/episode_y_position': Array(-37.2, dtype=float32), 'eval/episode_y_velocity': Array(-2.844, dtype=float32), 'eval/episode_distance_from_origin_std': Array(1.661, dtype=float32), 'eval/episode_forward_reward_std': Array(1.002, dtype=float32), 'eval/episode_reward_std': Array(2.475, dtype=float32), 'eval/episode_reward_alive_std': Array(0., dtype=float32), 'eval/episode_reward_linvel_std': Array(1.002, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(2.481, dtype=float32), 'eval/episode_x_position_std': Array(25.063, dtype=float32), 'eval/episode_x_velocity_std': Array(0.802, dtype=float32), 'eval/episode_y_position_std': Array(23.763, dtype=float32), 'eval/episode_y_velocity_std': Array(0.831, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/epoch_eval_time': 10.39812159538269, 'eval/sps': 12309.915673311485}
{'eval/walltime': 63.23491644859314, 'training/sps': 49114.5970387146, 'training/walltime': 127.8238410949707, 'training/entropy_loss': Array(0.017, dtype=float32), 'training/policy_loss': Array(-0.061, dtype=float32), 'training/total_loss': Array(-0.044, dtype=float32), 'training/v_loss': Array(0., dtype=float32), 'eval/episode_distance_from_origin': Array(873.121, dtype=float32), 'eval/episode_forward_reward': Array(3.028, dtype=float32), 'eval/episode_reward': Array(4940.922, dtype=float32), 'eval/episode_reward_alive': Array(4986.367, dtype=float32), 'eval/episode_reward_linvel': Array(3.028, dtype=float32), 'eval/episode_reward_quadctrl': Array(-48.473, dtype=float32), 'eval/episode_x_position': Array(51.634, dtype=float32), 'eval/episode_x_velocity': Array(2.423, dtype=float32), 'eval/episode_y_position': Array(-23.17, dtype=float32), 'eval/episode_y_velocity': Array(-2.052, dtype=float32), 'eval/episode_distance_from_origin_std': Array(20.5, dtype=float32), 'eval/episode_forward_reward_std': Array(3.449, dtype=float32), 'eval/episode_reward_std': Array(120.981, dtype=float32), 'eval/episode_reward_alive_std': Array(118.554, dtype=float32), 'eval/episode_reward_linvel_std': Array(3.449, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(6.185, dtype=float32), 'eval/episode_x_position_std': Array(17.609, dtype=float32), 'eval/episode_x_velocity_std': Array(2.759, dtype=float32), 'eval/episode_y_position_std': Array(20.156, dtype=float32), 'eval/episode_y_velocity_std': Array(2.435, dtype=float32), 'eval/avg_episode_length': Array(997.273, dtype=float32), 'eval/epoch_eval_time': 10.397130012512207, 'eval/sps': 12311.089680129142}
{'eval/walltime': 73.63731956481934, 'training/sps': 48972.01491070957, 'training/walltime': 181.35318851470947, 'training/entropy_loss': Array(0.02, dtype=float32), 'training/policy_loss': Array(-0.044, dtype=float32), 'training/total_loss': Array(-0.024, dtype=float32), 'training/v_loss': Array(0., dtype=float32), 'eval/episode_distance_from_origin': Array(875.47, dtype=float32), 'eval/episode_forward_reward': Array(3.263, dtype=float32), 'eval/episode_reward': Array(4970.113, dtype=float32), 'eval/episode_reward_alive': Array(5000., dtype=float32), 'eval/episode_reward_linvel': Array(3.263, dtype=float32), 'eval/episode_reward_quadctrl': Array(-33.149, dtype=float32), 'eval/episode_x_position': Array(65.731, dtype=float32), 'eval/episode_x_velocity': Array(2.61, dtype=float32), 'eval/episode_y_position': Array(-5.947, dtype=float32), 'eval/episode_y_velocity': Array(-0.885, dtype=float32), 'eval/episode_distance_from_origin_std': Array(1.5, dtype=float32), 'eval/episode_forward_reward_std': Array(0.619, dtype=float32), 'eval/episode_reward_std': Array(3.413, dtype=float32), 'eval/episode_reward_alive_std': Array(0., dtype=float32), 'eval/episode_reward_linvel_std': Array(0.619, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(3.324, dtype=float32), 'eval/episode_x_position_std': Array(17.083, dtype=float32), 'eval/episode_x_velocity_std': Array(0.495, dtype=float32), 'eval/episode_y_position_std': Array(22.544, dtype=float32), 'eval/episode_y_velocity_std': Array(0.651, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/epoch_eval_time': 10.402403116226196, 'eval/sps': 12304.849040154875}
{'eval/walltime': 84.04563164710999, 'training/sps': 48872.0233074815, 'training/walltime': 234.9920563697815, 'training/entropy_loss': Array(0.024, dtype=float32), 'training/policy_loss': Array(-0.044, dtype=float32), 'training/total_loss': Array(-0.019, dtype=float32), 'training/v_loss': Array(0., dtype=float32), 'eval/episode_distance_from_origin': Array(874.749, dtype=float32), 'eval/episode_forward_reward': Array(3.213, dtype=float32), 'eval/episode_reward': Array(4975.062, dtype=float32), 'eval/episode_reward_alive': Array(5000., dtype=float32), 'eval/episode_reward_linvel': Array(3.213, dtype=float32), 'eval/episode_reward_quadctrl': Array(-28.151, dtype=float32), 'eval/episode_x_position': Array(69.721, dtype=float32), 'eval/episode_x_velocity': Array(2.571, dtype=float32), 'eval/episode_y_position': Array(1.647, dtype=float32), 'eval/episode_y_velocity': Array(-0.318, dtype=float32), 'eval/episode_distance_from_origin_std': Array(1.744, dtype=float32), 'eval/episode_forward_reward_std': Array(0.615, dtype=float32), 'eval/episode_reward_std': Array(2.561, dtype=float32), 'eval/episode_reward_alive_std': Array(0., dtype=float32), 'eval/episode_reward_linvel_std': Array(0.615, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(2.266, dtype=float32), 'eval/episode_x_position_std': Array(17.106, dtype=float32), 'eval/episode_x_velocity_std': Array(0.492, dtype=float32), 'eval/episode_y_position_std': Array(19.989, dtype=float32), 'eval/episode_y_velocity_std': Array(0.562, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/epoch_eval_time': 10.40831208229065, 'eval/sps': 12297.863379575942}
time to jit: 0:00:56.764612
time to train: 0:04:36.854757

--------

Iteration5-Robot1

--------

{'eval/walltime': 53.35356521606445, 'training/sps': 35111.37335268208, 'training/walltime': 74.66070818901062, 'training/entropy_loss': Array(-0.002, dtype=float32), 'training/policy_loss': Array(-0.014, dtype=float32), 'training/total_loss': Array(0.024, dtype=float32), 'training/v_loss': Array(0.04, dtype=float32), 'eval/episode_distance_from_origin': Array(98.895, dtype=float32), 'eval/episode_forward_reward': Array(15.598, dtype=float32), 'eval/episode_reward': Array(571.406, dtype=float32), 'eval/episode_reward_alive': Array(596.094, dtype=float32), 'eval/episode_reward_linvel': Array(15.598, dtype=float32), 'eval/episode_reward_quadctrl': Array(-40.286, dtype=float32), 'eval/episode_x_position': Array(7.167, dtype=float32), 'eval/episode_x_velocity': Array(12.478, dtype=float32), 'eval/episode_y_position': Array(0.106, dtype=float32), 'eval/episode_y_velocity': Array(1.751, dtype=float32), 'eval/episode_distance_from_origin_std': Array(18.408, dtype=float32), 'eval/episode_forward_reward_std': Array(19.991, dtype=float32), 'eval/episode_reward_std': Array(102.508, dtype=float32), 'eval/episode_reward_alive_std': Array(110.646, dtype=float32), 'eval/episode_reward_linvel_std': Array(19.991, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(7.023, dtype=float32), 'eval/episode_x_position_std': Array(7.598, dtype=float32), 'eval/episode_x_velocity_std': Array(15.993, dtype=float32), 'eval/episode_y_position_std': Array(7.504, dtype=float32), 'eval/episode_y_velocity_std': Array(8.939, dtype=float32), 'eval/avg_episode_length': Array(119.219, dtype=float32), 'eval/epoch_eval_time': 10.394256353378296, 'eval/sps': 12314.493278626709}
time to jit: 0:00:58.052975
time to train: 0:01:25.142049

--------

Iteration5-Robot2

--------

{'eval/walltime': 53.51969885826111, 'training/sps': 35113.12043248841, 'training/walltime': 74.65699338912964, 'training/entropy_loss': Array(-0.001, dtype=float32), 'training/policy_loss': Array(-0.008, dtype=float32), 'training/total_loss': Array(0.046, dtype=float32), 'training/v_loss': Array(0.055, dtype=float32), 'eval/episode_distance_from_origin': Array(53.305, dtype=float32), 'eval/episode_forward_reward': Array(17.608, dtype=float32), 'eval/episode_reward': Array(307.542, dtype=float32), 'eval/episode_reward_alive': Array(314.141, dtype=float32), 'eval/episode_reward_linvel': Array(17.608, dtype=float32), 'eval/episode_reward_quadctrl': Array(-24.207, dtype=float32), 'eval/episode_x_position': Array(7.149, dtype=float32), 'eval/episode_x_velocity': Array(14.086, dtype=float32), 'eval/episode_y_position': Array(1.077, dtype=float32), 'eval/episode_y_velocity': Array(-2.881, dtype=float32), 'eval/episode_distance_from_origin_std': Array(10.31, dtype=float32), 'eval/episode_forward_reward_std': Array(13.498, dtype=float32), 'eval/episode_reward_std': Array(58.869, dtype=float32), 'eval/episode_reward_alive_std': Array(60.802, dtype=float32), 'eval/episode_reward_linvel_std': Array(13.498, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(4.968, dtype=float32), 'eval/episode_x_position_std': Array(4.517, dtype=float32), 'eval/episode_x_velocity_std': Array(10.798, dtype=float32), 'eval/episode_y_position_std': Array(4.68, dtype=float32), 'eval/episode_y_velocity_std': Array(11.692, dtype=float32), 'eval/avg_episode_length': Array(62.828, dtype=float32), 'eval/epoch_eval_time': 10.349073648452759, 'eval/sps': 12368.256749157126}
time to jit: 0:00:58.329071
time to train: 0:01:25.098016

--------

Iteration5-Robot3

--------

{'eval/walltime': 53.455074071884155, 'training/sps': 35151.761040836376, 'training/walltime': 74.57492661476135, 'training/entropy_loss': Array(0.005, dtype=float32), 'training/policy_loss': Array(0.021, dtype=float32), 'training/total_loss': Array(0.075, dtype=float32), 'training/v_loss': Array(0.05, dtype=float32), 'eval/episode_distance_from_origin': Array(75.415, dtype=float32), 'eval/episode_forward_reward': Array(39.301, dtype=float32), 'eval/episode_reward': Array(381.366, dtype=float32), 'eval/episode_reward_alive': Array(375.703, dtype=float32), 'eval/episode_reward_linvel': Array(39.301, dtype=float32), 'eval/episode_reward_quadctrl': Array(-33.637, dtype=float32), 'eval/episode_x_position': Array(21.739, dtype=float32), 'eval/episode_x_velocity': Array(31.441, dtype=float32), 'eval/episode_y_position': Array(7.997, dtype=float32), 'eval/episode_y_velocity': Array(10.107, dtype=float32), 'eval/episode_distance_from_origin_std': Array(12.181, dtype=float32), 'eval/episode_forward_reward_std': Array(15.662, dtype=float32), 'eval/episode_reward_std': Array(50.032, dtype=float32), 'eval/episode_reward_alive_std': Array(54.187, dtype=float32), 'eval/episode_reward_linvel_std': Array(15.662, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(6.691, dtype=float32), 'eval/episode_x_position_std': Array(7.037, dtype=float32), 'eval/episode_x_velocity_std': Array(12.53, dtype=float32), 'eval/episode_y_position_std': Array(6.35, dtype=float32), 'eval/episode_y_velocity_std': Array(12.373, dtype=float32), 'eval/avg_episode_length': Array(75.141, dtype=float32), 'eval/epoch_eval_time': 10.39911937713623, 'eval/sps': 12308.734553179962}
time to jit: 0:00:58.332549
time to train: 0:01:25.065536
6

--------

Iteration6-Robot0

--------

{'eval/walltime': 49.813138008117676, 'training/sps': 33892.32246451389, 'training/walltime': 77.34613060951233, 'training/entropy_loss': Array(-0., dtype=float32), 'training/policy_loss': Array(-0.011, dtype=float32), 'training/total_loss': Array(0.112, dtype=float32), 'training/v_loss': Array(0.123, dtype=float32), 'eval/episode_distance_from_origin': Array(47.903, dtype=float32), 'eval/episode_forward_reward': Array(14.457, dtype=float32), 'eval/episode_reward': Array(266.468, dtype=float32), 'eval/episode_reward_alive': Array(275.195, dtype=float32), 'eval/episode_reward_linvel': Array(14.457, dtype=float32), 'eval/episode_reward_quadctrl': Array(-23.185, dtype=float32), 'eval/episode_x_position': Array(6.215, dtype=float32), 'eval/episode_x_velocity': Array(11.566, dtype=float32), 'eval/episode_y_position': Array(1.451, dtype=float32), 'eval/episode_y_velocity': Array(2.351, dtype=float32), 'eval/episode_distance_from_origin_std': Array(10.704, dtype=float32), 'eval/episode_forward_reward_std': Array(15.307, dtype=float32), 'eval/episode_reward_std': Array(58.267, dtype=float32), 'eval/episode_reward_alive_std': Array(59.544, dtype=float32), 'eval/episode_reward_linvel_std': Array(15.307, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(5.125, dtype=float32), 'eval/episode_x_position_std': Array(5.342, dtype=float32), 'eval/episode_x_velocity_std': Array(12.246, dtype=float32), 'eval/episode_y_position_std': Array(5.692, dtype=float32), 'eval/episode_y_velocity_std': Array(12.448, dtype=float32), 'eval/avg_episode_length': Array(55.039, dtype=float32), 'eval/epoch_eval_time': 10.319008588790894, 'eval/sps': 12404.292418076}
{'eval/walltime': 60.17124080657959, 'training/sps': 49303.62970496715, 'training/walltime': 130.5154411792755, 'training/entropy_loss': Array(0., dtype=float32), 'training/policy_loss': Array(-0.01, dtype=float32), 'training/total_loss': Array(0.06, dtype=float32), 'training/v_loss': Array(0.07, dtype=float32), 'eval/episode_distance_from_origin': Array(59.239, dtype=float32), 'eval/episode_forward_reward': Array(28.058, dtype=float32), 'eval/episode_reward': Array(329.925, dtype=float32), 'eval/episode_reward_alive': Array(329.336, dtype=float32), 'eval/episode_reward_linvel': Array(28.058, dtype=float32), 'eval/episode_reward_quadctrl': Array(-27.469, dtype=float32), 'eval/episode_x_position': Array(13.627, dtype=float32), 'eval/episode_x_velocity': Array(22.447, dtype=float32), 'eval/episode_y_position': Array(5.445, dtype=float32), 'eval/episode_y_velocity': Array(5.693, dtype=float32), 'eval/episode_distance_from_origin_std': Array(14.641, dtype=float32), 'eval/episode_forward_reward_std': Array(14.104, dtype=float32), 'eval/episode_reward_std': Array(72.423, dtype=float32), 'eval/episode_reward_alive_std': Array(73.379, dtype=float32), 'eval/episode_reward_linvel_std': Array(14.104, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(6.229, dtype=float32), 'eval/episode_x_position_std': Array(8.056, dtype=float32), 'eval/episode_x_velocity_std': Array(11.283, dtype=float32), 'eval/episode_y_position_std': Array(8.144, dtype=float32), 'eval/episode_y_velocity_std': Array(12.657, dtype=float32), 'eval/avg_episode_length': Array(65.867, dtype=float32), 'eval/epoch_eval_time': 10.358102798461914, 'eval/sps': 12357.475349540542}
{'eval/walltime': 70.5915060043335, 'training/sps': 49145.334253706875, 'training/walltime': 183.8560082912445, 'training/entropy_loss': Array(0.001, dtype=float32), 'training/policy_loss': Array(-0.009, dtype=float32), 'training/total_loss': Array(0.045, dtype=float32), 'training/v_loss': Array(0.053, dtype=float32), 'eval/episode_distance_from_origin': Array(75.276, dtype=float32), 'eval/episode_forward_reward': Array(37.894, dtype=float32), 'eval/episode_reward': Array(394.176, dtype=float32), 'eval/episode_reward_alive': Array(388.008, dtype=float32), 'eval/episode_reward_linvel': Array(37.894, dtype=float32), 'eval/episode_reward_quadctrl': Array(-31.726, dtype=float32), 'eval/episode_x_position': Array(26.624, dtype=float32), 'eval/episode_x_velocity': Array(30.316, dtype=float32), 'eval/episode_y_position': Array(10.661, dtype=float32), 'eval/episode_y_velocity': Array(8.206, dtype=float32), 'eval/episode_distance_from_origin_std': Array(23.475, dtype=float32), 'eval/episode_forward_reward_std': Array(19.868, dtype=float32), 'eval/episode_reward_std': Array(93.447, dtype=float32), 'eval/episode_reward_alive_std': Array(92.859, dtype=float32), 'eval/episode_reward_linvel_std': Array(19.868, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(8.061, dtype=float32), 'eval/episode_x_position_std': Array(17.153, dtype=float32), 'eval/episode_x_velocity_std': Array(15.894, dtype=float32), 'eval/episode_y_position_std': Array(11.141, dtype=float32), 'eval/episode_y_velocity_std': Array(14.689, dtype=float32), 'eval/avg_episode_length': Array(77.602, dtype=float32), 'eval/epoch_eval_time': 10.420265197753906, 'eval/sps': 12283.756465966957}
{'eval/walltime': 80.98139095306396, 'training/sps': 48994.63723499846, 'training/walltime': 237.36063957214355, 'training/entropy_loss': Array(0.002, dtype=float32), 'training/policy_loss': Array(-0.008, dtype=float32), 'training/total_loss': Array(0.049, dtype=float32), 'training/v_loss': Array(0.055, dtype=float32), 'eval/episode_distance_from_origin': Array(91.153, dtype=float32), 'eval/episode_forward_reward': Array(43.686, dtype=float32), 'eval/episode_reward': Array(460.605, dtype=float32), 'eval/episode_reward_alive': Array(452.617, dtype=float32), 'eval/episode_reward_linvel': Array(43.686, dtype=float32), 'eval/episode_reward_quadctrl': Array(-35.698, dtype=float32), 'eval/episode_x_position': Array(37.833, dtype=float32), 'eval/episode_x_velocity': Array(34.948, dtype=float32), 'eval/episode_y_position': Array(13.727, dtype=float32), 'eval/episode_y_velocity': Array(7.337, dtype=float32), 'eval/episode_distance_from_origin_std': Array(23.441, dtype=float32), 'eval/episode_forward_reward_std': Array(18.64, dtype=float32), 'eval/episode_reward_std': Array(94.476, dtype=float32), 'eval/episode_reward_alive_std': Array(93.23, dtype=float32), 'eval/episode_reward_linvel_std': Array(18.64, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(7.061, dtype=float32), 'eval/episode_x_position_std': Array(18.085, dtype=float32), 'eval/episode_x_velocity_std': Array(14.912, dtype=float32), 'eval/episode_y_position_std': Array(11.828, dtype=float32), 'eval/episode_y_velocity_std': Array(16.542, dtype=float32), 'eval/avg_episode_length': Array(90.523, dtype=float32), 'eval/epoch_eval_time': 10.389884948730469, 'eval/sps': 12319.674436398856}
time to jit: 0:00:54.155833
time to train: 0:04:39.166512

--------

Iteration6-Robot1

--------

{'eval/walltime': 50.38920831680298, 'training/sps': 33782.751148317184, 'training/walltime': 77.59699583053589, 'training/entropy_loss': Array(0.007, dtype=float32), 'training/policy_loss': Array(0.027, dtype=float32), 'training/total_loss': Array(0.11, dtype=float32), 'training/v_loss': Array(0.076, dtype=float32), 'eval/episode_distance_from_origin': Array(62.725, dtype=float32), 'eval/episode_forward_reward': Array(31.485, dtype=float32), 'eval/episode_reward': Array(315.591, dtype=float32), 'eval/episode_reward_alive': Array(329.961, dtype=float32), 'eval/episode_reward_linvel': Array(31.485, dtype=float32), 'eval/episode_reward_quadctrl': Array(-45.855, dtype=float32), 'eval/episode_x_position': Array(13.069, dtype=float32), 'eval/episode_x_velocity': Array(25.188, dtype=float32), 'eval/episode_y_position': Array(3.054, dtype=float32), 'eval/episode_y_velocity': Array(1.579, dtype=float32), 'eval/episode_distance_from_origin_std': Array(10.713, dtype=float32), 'eval/episode_forward_reward_std': Array(11.797, dtype=float32), 'eval/episode_reward_std': Array(50.1, dtype=float32), 'eval/episode_reward_alive_std': Array(55.115, dtype=float32), 'eval/episode_reward_linvel_std': Array(11.797, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(9.543, dtype=float32), 'eval/episode_x_position_std': Array(4.927, dtype=float32), 'eval/episode_x_velocity_std': Array(9.437, dtype=float32), 'eval/episode_y_position_std': Array(3.928, dtype=float32), 'eval/episode_y_velocity_std': Array(5.028, dtype=float32), 'eval/avg_episode_length': Array(65.992, dtype=float32), 'eval/epoch_eval_time': 10.328339099884033, 'eval/sps': 12393.086512955137}
{'eval/walltime': 60.781928300857544, 'training/sps': 49209.95154676946, 'training/walltime': 130.8675217628479, 'training/entropy_loss': Array(0.007, dtype=float32), 'training/policy_loss': Array(-0.005, dtype=float32), 'training/total_loss': Array(0.033, dtype=float32), 'training/v_loss': Array(0.031, dtype=float32), 'eval/episode_distance_from_origin': Array(76.513, dtype=float32), 'eval/episode_forward_reward': Array(31.039, dtype=float32), 'eval/episode_reward': Array(383.76, dtype=float32), 'eval/episode_reward_alive': Array(403.32, dtype=float32), 'eval/episode_reward_linvel': Array(31.039, dtype=float32), 'eval/episode_reward_quadctrl': Array(-50.6, dtype=float32), 'eval/episode_x_position': Array(15.348, dtype=float32), 'eval/episode_x_velocity': Array(24.832, dtype=float32), 'eval/episode_y_position': Array(3.33, dtype=float32), 'eval/episode_y_velocity': Array(2.007, dtype=float32), 'eval/episode_distance_from_origin_std': Array(12.658, dtype=float32), 'eval/episode_forward_reward_std': Array(17.792, dtype=float32), 'eval/episode_reward_std': Array(56.229, dtype=float32), 'eval/episode_reward_alive_std': Array(66.301, dtype=float32), 'eval/episode_reward_linvel_std': Array(17.792, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(11.11, dtype=float32), 'eval/episode_x_position_std': Array(6.354, dtype=float32), 'eval/episode_x_velocity_std': Array(14.233, dtype=float32), 'eval/episode_y_position_std': Array(5.389, dtype=float32), 'eval/episode_y_velocity_std': Array(7.837, dtype=float32), 'eval/avg_episode_length': Array(80.664, dtype=float32), 'eval/epoch_eval_time': 10.392719984054565, 'eval/sps': 12316.313746198202}
{'eval/walltime': 71.20919990539551, 'training/sps': 48997.053522424874, 'training/walltime': 184.36951446533203, 'training/entropy_loss': Array(0.009, dtype=float32), 'training/policy_loss': Array(0.029, dtype=float32), 'training/total_loss': Array(0.083, dtype=float32), 'training/v_loss': Array(0.045, dtype=float32), 'eval/episode_distance_from_origin': Array(45.495, dtype=float32), 'eval/episode_forward_reward': Array(9.194, dtype=float32), 'eval/episode_reward': Array(209.641, dtype=float32), 'eval/episode_reward_alive': Array(243.398, dtype=float32), 'eval/episode_reward_linvel': Array(9.194, dtype=float32), 'eval/episode_reward_quadctrl': Array(-42.952, dtype=float32), 'eval/episode_x_position': Array(3.444, dtype=float32), 'eval/episode_x_velocity': Array(7.355, dtype=float32), 'eval/episode_y_position': Array(-0.083, dtype=float32), 'eval/episode_y_velocity': Array(-3.065, dtype=float32), 'eval/episode_distance_from_origin_std': Array(13.987, dtype=float32), 'eval/episode_forward_reward_std': Array(10.985, dtype=float32), 'eval/episode_reward_std': Array(66.284, dtype=float32), 'eval/episode_reward_alive_std': Array(75.317, dtype=float32), 'eval/episode_reward_linvel_std': Array(10.985, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(13.75, dtype=float32), 'eval/episode_x_position_std': Array(4.554, dtype=float32), 'eval/episode_x_velocity_std': Array(8.788, dtype=float32), 'eval/episode_y_position_std': Array(4.124, dtype=float32), 'eval/episode_y_velocity_std': Array(10.355, dtype=float32), 'eval/avg_episode_length': Array(48.68, dtype=float32), 'eval/epoch_eval_time': 10.427271604537964, 'eval/sps': 12275.502629498422}
time to jit: 0:00:54.931663
time to train: 0:03:35.810271

--------

Iteration6-Robot2

--------

{'eval/walltime': 50.7961368560791, 'training/sps': 33683.972098692226, 'training/walltime': 77.82455086708069, 'training/entropy_loss': Array(0.01, dtype=float32), 'training/policy_loss': Array(-0.011, dtype=float32), 'training/total_loss': Array(0.013, dtype=float32), 'training/v_loss': Array(0.014, dtype=float32), 'eval/episode_distance_from_origin': Array(116.907, dtype=float32), 'eval/episode_forward_reward': Array(-25.199, dtype=float32), 'eval/episode_reward': Array(599.044, dtype=float32), 'eval/episode_reward_alive': Array(663.242, dtype=float32), 'eval/episode_reward_linvel': Array(-25.199, dtype=float32), 'eval/episode_reward_quadctrl': Array(-39., dtype=float32), 'eval/episode_x_position': Array(-12.729, dtype=float32), 'eval/episode_x_velocity': Array(-20.159, dtype=float32), 'eval/episode_y_position': Array(10.674, dtype=float32), 'eval/episode_y_velocity': Array(14.362, dtype=float32), 'eval/episode_distance_from_origin_std': Array(26.603, dtype=float32), 'eval/episode_forward_reward_std': Array(4.189, dtype=float32), 'eval/episode_reward_std': Array(146.817, dtype=float32), 'eval/episode_reward_alive_std': Array(152.175, dtype=float32), 'eval/episode_reward_linvel_std': Array(4.189, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(6.481, dtype=float32), 'eval/episode_x_position_std': Array(3.309, dtype=float32), 'eval/episode_x_velocity_std': Array(3.351, dtype=float32), 'eval/episode_y_position_std': Array(5.967, dtype=float32), 'eval/episode_y_velocity_std': Array(8.621, dtype=float32), 'eval/avg_episode_length': Array(132.648, dtype=float32), 'eval/epoch_eval_time': 10.341509819030762, 'eval/sps': 12377.30295091443}
{'eval/walltime': 61.142884492874146, 'training/sps': 49109.25125012988, 'training/walltime': 131.20430994033813, 'training/entropy_loss': Array(0.014, dtype=float32), 'training/policy_loss': Array(-0.018, dtype=float32), 'training/total_loss': Array(-0.001, dtype=float32), 'training/v_loss': Array(0.003, dtype=float32), 'eval/episode_distance_from_origin': Array(855.344, dtype=float32), 'eval/episode_forward_reward': Array(-1.846, dtype=float32), 'eval/episode_reward': Array(4827.862, dtype=float32), 'eval/episode_reward_alive': Array(4965.898, dtype=float32), 'eval/episode_reward_linvel': Array(-1.846, dtype=float32), 'eval/episode_reward_quadctrl': Array(-136.189, dtype=float32), 'eval/episode_x_position': Array(-22.013, dtype=float32), 'eval/episode_x_velocity': Array(-1.477, dtype=float32), 'eval/episode_y_position': Array(2.386, dtype=float32), 'eval/episode_y_velocity': Array(-0.286, dtype=float32), 'eval/episode_distance_from_origin_std': Array(65.797, dtype=float32), 'eval/episode_forward_reward_std': Array(2.124, dtype=float32), 'eval/episode_reward_std': Array(377.46, dtype=float32), 'eval/episode_reward_alive_std': Array(384.305, dtype=float32), 'eval/episode_reward_linvel_std': Array(2.124, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(9.837, dtype=float32), 'eval/episode_x_position_std': Array(21.274, dtype=float32), 'eval/episode_x_velocity_std': Array(1.699, dtype=float32), 'eval/episode_y_position_std': Array(20.809, dtype=float32), 'eval/episode_y_velocity_std': Array(2.331, dtype=float32), 'eval/avg_episode_length': Array(993.18, dtype=float32), 'eval/epoch_eval_time': 10.346747636795044, 'eval/sps': 12371.037208330774}
{'eval/walltime': 71.56124019622803, 'training/sps': 48941.28469455827, 'training/walltime': 184.76726841926575, 'training/entropy_loss': Array(0.02, dtype=float32), 'training/policy_loss': Array(-0.049, dtype=float32), 'training/total_loss': Array(-0.029, dtype=float32), 'training/v_loss': Array(0., dtype=float32), 'eval/episode_distance_from_origin': Array(862.472, dtype=float32), 'eval/episode_forward_reward': Array(-0.838, dtype=float32), 'eval/episode_reward': Array(4918.717, dtype=float32), 'eval/episode_reward_alive': Array(5000., dtype=float32), 'eval/episode_reward_linvel': Array(-0.838, dtype=float32), 'eval/episode_reward_quadctrl': Array(-80.444, dtype=float32), 'eval/episode_x_position': Array(-10.104, dtype=float32), 'eval/episode_x_velocity': Array(-0.671, dtype=float32), 'eval/episode_y_position': Array(-0.819, dtype=float32), 'eval/episode_y_velocity': Array(-0.395, dtype=float32), 'eval/episode_distance_from_origin_std': Array(1.107, dtype=float32), 'eval/episode_forward_reward_std': Array(0.697, dtype=float32), 'eval/episode_reward_std': Array(4.059, dtype=float32), 'eval/episode_reward_alive_std': Array(0., dtype=float32), 'eval/episode_reward_linvel_std': Array(0.697, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(4.143, dtype=float32), 'eval/episode_x_position_std': Array(19.622, dtype=float32), 'eval/episode_x_velocity_std': Array(0.558, dtype=float32), 'eval/episode_y_position_std': Array(24.422, dtype=float32), 'eval/episode_y_velocity_std': Array(0.711, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/epoch_eval_time': 10.418355703353882, 'eval/sps': 12286.007854271493}
{'eval/walltime': 81.98059225082397, 'training/sps': 48768.56950768102, 'training/walltime': 238.519921541214, 'training/entropy_loss': Array(0.024, dtype=float32), 'training/policy_loss': Array(-0.046, dtype=float32), 'training/total_loss': Array(-0.023, dtype=float32), 'training/v_loss': Array(0., dtype=float32), 'eval/episode_distance_from_origin': Array(864.727, dtype=float32), 'eval/episode_forward_reward': Array(-0.401, dtype=float32), 'eval/episode_reward': Array(4949.3, dtype=float32), 'eval/episode_reward_alive': Array(5000., dtype=float32), 'eval/episode_reward_linvel': Array(-0.401, dtype=float32), 'eval/episode_reward_quadctrl': Array(-50.299, dtype=float32), 'eval/episode_x_position': Array(-0.633, dtype=float32), 'eval/episode_x_velocity': Array(-0.321, dtype=float32), 'eval/episode_y_position': Array(-16.079, dtype=float32), 'eval/episode_y_velocity': Array(-1.003, dtype=float32), 'eval/episode_distance_from_origin_std': Array(1.297, dtype=float32), 'eval/episode_forward_reward_std': Array(0.578, dtype=float32), 'eval/episode_reward_std': Array(2.784, dtype=float32), 'eval/episode_reward_alive_std': Array(0., dtype=float32), 'eval/episode_reward_linvel_std': Array(0.578, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(2.747, dtype=float32), 'eval/episode_x_position_std': Array(17.659, dtype=float32), 'eval/episode_x_velocity_std': Array(0.463, dtype=float32), 'eval/episode_y_position_std': Array(20.482, dtype=float32), 'eval/episode_y_velocity_std': Array(0.564, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/epoch_eval_time': 10.419352054595947, 'eval/sps': 12284.833003942846}
time to jit: 0:00:55.601557
time to train: 0:04:40.430928

--------

Iteration6-Robot3

--------

{'eval/walltime': 51.19771099090576, 'training/sps': 33516.555877945815, 'training/walltime': 78.21328687667847, 'training/entropy_loss': Array(0.012, dtype=float32), 'training/policy_loss': Array(-0.008, dtype=float32), 'training/total_loss': Array(0.047, dtype=float32), 'training/v_loss': Array(0.043, dtype=float32), 'eval/episode_distance_from_origin': Array(85.069, dtype=float32), 'eval/episode_forward_reward': Array(-8.721, dtype=float32), 'eval/episode_reward': Array(484.95, dtype=float32), 'eval/episode_reward_alive': Array(537.695, dtype=float32), 'eval/episode_reward_linvel': Array(-8.721, dtype=float32), 'eval/episode_reward_quadctrl': Array(-44.024, dtype=float32), 'eval/episode_x_position': Array(-1.503, dtype=float32), 'eval/episode_x_velocity': Array(-6.977, dtype=float32), 'eval/episode_y_position': Array(-3.893, dtype=float32), 'eval/episode_y_velocity': Array(2.756, dtype=float32), 'eval/episode_distance_from_origin_std': Array(54.518, dtype=float32), 'eval/episode_forward_reward_std': Array(14.66, dtype=float32), 'eval/episode_reward_std': Array(332.592, dtype=float32), 'eval/episode_reward_alive_std': Array(351.902, dtype=float32), 'eval/episode_reward_linvel_std': Array(14.66, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(17.813, dtype=float32), 'eval/episode_x_position_std': Array(6.157, dtype=float32), 'eval/episode_x_velocity_std': Array(11.728, dtype=float32), 'eval/episode_y_position_std': Array(6.945, dtype=float32), 'eval/episode_y_velocity_std': Array(11.411, dtype=float32), 'eval/avg_episode_length': Array(107.539, dtype=float32), 'eval/epoch_eval_time': 10.263505458831787, 'eval/sps': 12471.372526026718}
{'eval/walltime': 61.54236721992493, 'training/sps': 49063.85012129107, 'training/walltime': 131.64244079589844, 'training/entropy_loss': Array(0.009, dtype=float32), 'training/policy_loss': Array(-0.012, dtype=float32), 'training/total_loss': Array(0.009, dtype=float32), 'training/v_loss': Array(0.012, dtype=float32), 'eval/episode_distance_from_origin': Array(506.2, dtype=float32), 'eval/episode_forward_reward': Array(-6.476, dtype=float32), 'eval/episode_reward': Array(3098.593, dtype=float32), 'eval/episode_reward_alive': Array(3246.25, dtype=float32), 'eval/episode_reward_linvel': Array(-6.476, dtype=float32), 'eval/episode_reward_quadctrl': Array(-141.181, dtype=float32), 'eval/episode_x_position': Array(13.778, dtype=float32), 'eval/episode_x_velocity': Array(-5.181, dtype=float32), 'eval/episode_y_position': Array(-58.391, dtype=float32), 'eval/episode_y_velocity': Array(-1.875, dtype=float32), 'eval/episode_distance_from_origin_std': Array(297.084, dtype=float32), 'eval/episode_forward_reward_std': Array(14.217, dtype=float32), 'eval/episode_reward_std': Array(1859.953, dtype=float32), 'eval/episode_reward_alive_std': Array(1914.829, dtype=float32), 'eval/episode_reward_linvel_std': Array(14.217, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(62.856, dtype=float32), 'eval/episode_x_position_std': Array(27.26, dtype=float32), 'eval/episode_x_velocity_std': Array(11.374, dtype=float32), 'eval/episode_y_position_std': Array(45.493, dtype=float32), 'eval/episode_y_velocity_std': Array(8.547, dtype=float32), 'eval/avg_episode_length': Array(649.25, dtype=float32), 'eval/epoch_eval_time': 10.344656229019165, 'eval/sps': 12373.538295156706}
time to jit: 0:00:56.191578
time to train: 0:02:32.458706
7

--------

Iteration7-Robot0

--------

{'eval/walltime': 50.835638999938965, 'training/sps': 33658.383175943105, 'training/walltime': 77.88371729850769, 'training/entropy_loss': Array(0.01, dtype=float32), 'training/policy_loss': Array(-0.018, dtype=float32), 'training/total_loss': Array(0., dtype=float32), 'training/v_loss': Array(0.008, dtype=float32), 'eval/episode_distance_from_origin': Array(894.138, dtype=float32), 'eval/episode_forward_reward': Array(2.522, dtype=float32), 'eval/episode_reward': Array(4834.474, dtype=float32), 'eval/episode_reward_alive': Array(4938.125, dtype=float32), 'eval/episode_reward_linvel': Array(2.522, dtype=float32), 'eval/episode_reward_quadctrl': Array(-106.174, dtype=float32), 'eval/episode_x_position': Array(54.547, dtype=float32), 'eval/episode_x_velocity': Array(2.018, dtype=float32), 'eval/episode_y_position': Array(-17.528, dtype=float32), 'eval/episode_y_velocity': Array(-1.484, dtype=float32), 'eval/episode_distance_from_origin_std': Array(89.1, dtype=float32), 'eval/episode_forward_reward_std': Array(4.354, dtype=float32), 'eval/episode_reward_std': Array(483.033, dtype=float32), 'eval/episode_reward_alive_std': Array(493.916, dtype=float32), 'eval/episode_reward_linvel_std': Array(4.354, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(8.697, dtype=float32), 'eval/episode_x_position_std': Array(22.198, dtype=float32), 'eval/episode_x_velocity_std': Array(3.483, dtype=float32), 'eval/episode_y_position_std': Array(22.109, dtype=float32), 'eval/episode_y_velocity_std': Array(0.996, dtype=float32), 'eval/avg_episode_length': Array(987.625, dtype=float32), 'eval/epoch_eval_time': 10.358407258987427, 'eval/sps': 12357.112131205438}
{'eval/walltime': 61.19984984397888, 'training/sps': 49218.684268532386, 'training/walltime': 131.14479160308838, 'training/entropy_loss': Array(0.017, dtype=float32), 'training/policy_loss': Array(-0.047, dtype=float32), 'training/total_loss': Array(-0.03, dtype=float32), 'training/v_loss': Array(0., dtype=float32), 'eval/episode_distance_from_origin': Array(916.32, dtype=float32), 'eval/episode_forward_reward': Array(2.624, dtype=float32), 'eval/episode_reward': Array(4915.95, dtype=float32), 'eval/episode_reward_alive': Array(5000., dtype=float32), 'eval/episode_reward_linvel': Array(2.624, dtype=float32), 'eval/episode_reward_quadctrl': Array(-86.674, dtype=float32), 'eval/episode_x_position': Array(61.354, dtype=float32), 'eval/episode_x_velocity': Array(2.099, dtype=float32), 'eval/episode_y_position': Array(-13.765, dtype=float32), 'eval/episode_y_velocity': Array(-0.9, dtype=float32), 'eval/episode_distance_from_origin_std': Array(1.499, dtype=float32), 'eval/episode_forward_reward_std': Array(0.617, dtype=float32), 'eval/episode_reward_std': Array(3.481, dtype=float32), 'eval/episode_reward_alive_std': Array(0., dtype=float32), 'eval/episode_reward_linvel_std': Array(0.617, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(3.377, dtype=float32), 'eval/episode_x_position_std': Array(17.693, dtype=float32), 'eval/episode_x_velocity_std': Array(0.494, dtype=float32), 'eval/episode_y_position_std': Array(19.919, dtype=float32), 'eval/episode_y_velocity_std': Array(0.562, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/epoch_eval_time': 10.364210844039917, 'eval/sps': 12350.192593159003}
{'eval/walltime': 71.62964034080505, 'training/sps': 48970.15768664945, 'training/walltime': 184.6761691570282, 'training/entropy_loss': Array(0.022, dtype=float32), 'training/policy_loss': Array(-0.043, dtype=float32), 'training/total_loss': Array(-0.022, dtype=float32), 'training/v_loss': Array(0., dtype=float32), 'eval/episode_distance_from_origin': Array(919.501, dtype=float32), 'eval/episode_forward_reward': Array(2.57, dtype=float32), 'eval/episode_reward': Array(4926.594, dtype=float32), 'eval/episode_reward_alive': Array(5000., dtype=float32), 'eval/episode_reward_linvel': Array(2.57, dtype=float32), 'eval/episode_reward_quadctrl': Array(-75.976, dtype=float32), 'eval/episode_x_position': Array(61.386, dtype=float32), 'eval/episode_x_velocity': Array(2.056, dtype=float32), 'eval/episode_y_position': Array(-5.253, dtype=float32), 'eval/episode_y_velocity': Array(-0.472, dtype=float32), 'eval/episode_distance_from_origin_std': Array(1.533, dtype=float32), 'eval/episode_forward_reward_std': Array(0.714, dtype=float32), 'eval/episode_reward_std': Array(4.168, dtype=float32), 'eval/episode_reward_alive_std': Array(0., dtype=float32), 'eval/episode_reward_linvel_std': Array(0.714, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(4.009, dtype=float32), 'eval/episode_x_position_std': Array(18.58, dtype=float32), 'eval/episode_x_velocity_std': Array(0.571, dtype=float32), 'eval/episode_y_position_std': Array(20.086, dtype=float32), 'eval/episode_y_velocity_std': Array(0.664, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/epoch_eval_time': 10.429790496826172, 'eval/sps': 12272.537980408228}
{'eval/walltime': 81.97017979621887, 'training/sps': 48810.80415772914, 'training/walltime': 238.3823115825653, 'training/entropy_loss': Array(0.026, dtype=float32), 'training/policy_loss': Array(-0.033, dtype=float32), 'training/total_loss': Array(-0.006, dtype=float32), 'training/v_loss': Array(0., dtype=float32), 'eval/episode_distance_from_origin': Array(919.299, dtype=float32), 'eval/episode_forward_reward': Array(2.31, dtype=float32), 'eval/episode_reward': Array(4943.287, dtype=float32), 'eval/episode_reward_alive': Array(5000., dtype=float32), 'eval/episode_reward_linvel': Array(2.31, dtype=float32), 'eval/episode_reward_quadctrl': Array(-59.023, dtype=float32), 'eval/episode_x_position': Array(58.786, dtype=float32), 'eval/episode_x_velocity': Array(1.848, dtype=float32), 'eval/episode_y_position': Array(-3.471, dtype=float32), 'eval/episode_y_velocity': Array(-0.462, dtype=float32), 'eval/episode_distance_from_origin_std': Array(1.53, dtype=float32), 'eval/episode_forward_reward_std': Array(0.65, dtype=float32), 'eval/episode_reward_std': Array(4.316, dtype=float32), 'eval/episode_reward_alive_std': Array(0., dtype=float32), 'eval/episode_reward_linvel_std': Array(0.65, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(4.154, dtype=float32), 'eval/episode_x_position_std': Array(17.716, dtype=float32), 'eval/episode_x_velocity_std': Array(0.52, dtype=float32), 'eval/episode_y_position_std': Array(18.451, dtype=float32), 'eval/episode_y_velocity_std': Array(0.504, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/epoch_eval_time': 10.340539455413818, 'eval/sps': 12378.464445874266}
time to jit: 0:00:55.574275
time to train: 0:04:40.146527

--------

Iteration7-Robot1

--------

{'eval/walltime': 51.08972239494324, 'training/sps': 33552.37961994293, 'training/walltime': 78.12977886199951, 'training/entropy_loss': Array(0.002, dtype=float32), 'training/policy_loss': Array(-0.016, dtype=float32), 'training/total_loss': Array(0.014, dtype=float32), 'training/v_loss': Array(0.028, dtype=float32), 'eval/episode_distance_from_origin': Array(82.357, dtype=float32), 'eval/episode_forward_reward': Array(14.974, dtype=float32), 'eval/episode_reward': Array(496.23, dtype=float32), 'eval/episode_reward_alive': Array(510.938, dtype=float32), 'eval/episode_reward_linvel': Array(14.974, dtype=float32), 'eval/episode_reward_quadctrl': Array(-29.682, dtype=float32), 'eval/episode_x_position': Array(5.02, dtype=float32), 'eval/episode_x_velocity': Array(11.979, dtype=float32), 'eval/episode_y_position': Array(-6.549, dtype=float32), 'eval/episode_y_velocity': Array(-6.971, dtype=float32), 'eval/episode_distance_from_origin_std': Array(19.761, dtype=float32), 'eval/episode_forward_reward_std': Array(18.119, dtype=float32), 'eval/episode_reward_std': Array(115.818, dtype=float32), 'eval/episode_reward_alive_std': Array(124.164, dtype=float32), 'eval/episode_reward_linvel_std': Array(18.119, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(9.068, dtype=float32), 'eval/episode_x_position_std': Array(6.387, dtype=float32), 'eval/episode_x_velocity_std': Array(14.495, dtype=float32), 'eval/episode_y_position_std': Array(5.644, dtype=float32), 'eval/episode_y_velocity_std': Array(8.94, dtype=float32), 'eval/avg_episode_length': Array(102.188, dtype=float32), 'eval/epoch_eval_time': 10.393933773040771, 'eval/sps': 12314.875464378996}
time to jit: 0:00:55.951042
time to train: 0:01:28.613433

--------

Iteration7-Robot2

--------

{'eval/walltime': 51.21035861968994, 'training/sps': 33583.88626123339, 'training/walltime': 78.05648159980774, 'training/entropy_loss': Array(0.01, dtype=float32), 'training/policy_loss': Array(-0.012, dtype=float32), 'training/total_loss': Array(0.017, dtype=float32), 'training/v_loss': Array(0.02, dtype=float32), 'eval/episode_distance_from_origin': Array(95.465, dtype=float32), 'eval/episode_forward_reward': Array(-18.582, dtype=float32), 'eval/episode_reward': Array(537.239, dtype=float32), 'eval/episode_reward_alive': Array(594.961, dtype=float32), 'eval/episode_reward_linvel': Array(-18.582, dtype=float32), 'eval/episode_reward_quadctrl': Array(-39.14, dtype=float32), 'eval/episode_x_position': Array(-13.576, dtype=float32), 'eval/episode_x_velocity': Array(-14.865, dtype=float32), 'eval/episode_y_position': Array(1.142, dtype=float32), 'eval/episode_y_velocity': Array(7.764, dtype=float32), 'eval/episode_distance_from_origin_std': Array(23.363, dtype=float32), 'eval/episode_forward_reward_std': Array(14.793, dtype=float32), 'eval/episode_reward_std': Array(132.021, dtype=float32), 'eval/episode_reward_alive_std': Array(146.56, dtype=float32), 'eval/episode_reward_linvel_std': Array(14.793, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(8.135, dtype=float32), 'eval/episode_x_position_std': Array(7.452, dtype=float32), 'eval/episode_x_velocity_std': Array(11.834, dtype=float32), 'eval/episode_y_position_std': Array(4.86, dtype=float32), 'eval/episode_y_velocity_std': Array(8.333, dtype=float32), 'eval/avg_episode_length': Array(118.992, dtype=float32), 'eval/epoch_eval_time': 10.343889474868774, 'eval/sps': 12374.45549964404}
time to jit: 0:00:56.258660
time to train: 0:01:28.516138

--------

Iteration7-Robot3

--------

{'eval/walltime': 51.79622411727905, 'training/sps': 33432.04396772945, 'training/walltime': 78.41100001335144, 'training/entropy_loss': Array(0.011, dtype=float32), 'training/policy_loss': Array(-0.008, dtype=float32), 'training/total_loss': Array(0.029, dtype=float32), 'training/v_loss': Array(0.026, dtype=float32), 'eval/episode_distance_from_origin': Array(660.943, dtype=float32), 'eval/episode_forward_reward': Array(19.165, dtype=float32), 'eval/episode_reward': Array(3423.816, dtype=float32), 'eval/episode_reward_alive': Array(3478.633, dtype=float32), 'eval/episode_reward_linvel': Array(19.165, dtype=float32), 'eval/episode_reward_quadctrl': Array(-73.982, dtype=float32), 'eval/episode_x_position': Array(62.975, dtype=float32), 'eval/episode_x_velocity': Array(15.332, dtype=float32), 'eval/episode_y_position': Array(6.846, dtype=float32), 'eval/episode_y_velocity': Array(-0.314, dtype=float32), 'eval/episode_distance_from_origin_std': Array(361.244, dtype=float32), 'eval/episode_forward_reward_std': Array(21.022, dtype=float32), 'eval/episode_reward_std': Array(1888.654, dtype=float32), 'eval/episode_reward_alive_std': Array(1924.086, dtype=float32), 'eval/episode_reward_linvel_std': Array(21.022, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(24.322, dtype=float32), 'eval/episode_x_position_std': Array(30.835, dtype=float32), 'eval/episode_x_velocity_std': Array(16.818, dtype=float32), 'eval/episode_y_position_std': Array(15.447, dtype=float32), 'eval/episode_y_velocity_std': Array(11.178, dtype=float32), 'eval/avg_episode_length': Array(695.727, dtype=float32), 'eval/epoch_eval_time': 10.353419303894043, 'eval/sps': 12363.065403123168}
{'eval/walltime': 62.16499900817871, 'training/sps': 49237.839125769846, 'training/walltime': 131.65135431289673, 'training/entropy_loss': Array(0.019, dtype=float32), 'training/policy_loss': Array(-0.01, dtype=float32), 'training/total_loss': Array(0.013, dtype=float32), 'training/v_loss': Array(0.004, dtype=float32), 'eval/episode_distance_from_origin': Array(890.89, dtype=float32), 'eval/episode_forward_reward': Array(5.987, dtype=float32), 'eval/episode_reward': Array(4646.355, dtype=float32), 'eval/episode_reward_alive': Array(4709.844, dtype=float32), 'eval/episode_reward_linvel': Array(5.987, dtype=float32), 'eval/episode_reward_quadctrl': Array(-69.475, dtype=float32), 'eval/episode_x_position': Array(68.873, dtype=float32), 'eval/episode_x_velocity': Array(4.79, dtype=float32), 'eval/episode_y_position': Array(-0.063, dtype=float32), 'eval/episode_y_velocity': Array(-0.532, dtype=float32), 'eval/episode_distance_from_origin_std': Array(198.552, dtype=float32), 'eval/episode_forward_reward_std': Array(10.841, dtype=float32), 'eval/episode_reward_std': Array(1048.104, dtype=float32), 'eval/episode_reward_alive_std': Array(1063.902, dtype=float32), 'eval/episode_reward_linvel_std': Array(10.841, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(9.653, dtype=float32), 'eval/episode_x_position_std': Array(29.343, dtype=float32), 'eval/episode_x_velocity_std': Array(8.673, dtype=float32), 'eval/episode_y_position_std': Array(22.611, dtype=float32), 'eval/episode_y_velocity_std': Array(5.675, dtype=float32), 'eval/avg_episode_length': Array(941.969, dtype=float32), 'eval/epoch_eval_time': 10.368774890899658, 'eval/sps': 12344.7563812328}
Traceback (most recent call last):
  File "/home/name/Desktop/Codes/ScalingOptimization (1)/TheAlgorithm.py", line 309, in <module>
    class Humanoid(MjxEnv):
NameError: name 'MjxEnv' is not defined
2024-01-09 22:41:13.179573: W external/tsl/tsl/framework/bfc_allocator.cc:485] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.83GiB (rounded to 1964845312)requested by op 
2024-01-09 22:41:13.179796: W external/tsl/tsl/framework/bfc_allocator.cc:497] *********************_______________________________________________________________________________
2024-01-09 22:41:13.180611: E external/xla/xla/pjrt/pjrt_stream_executor_client.cc:2732] Execution of replica 0 failed: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 1964845216 bytes.
BufferAssignment OOM Debugging.
BufferAssignment stats:
             parameter allocation:   95.27MiB
              constant allocation:    59.9KiB
        maybe_live_out allocation:   95.27MiB
     preallocated temp allocation:    1.83GiB
  preallocated temp fragmentation:    3.44MiB (0.18%)
                 total allocation:    2.02GiB
              total fragmentation:   25.98MiB (1.26%)
Peak buffers:
	Buffer 1:
		Size: 420.00MiB
		Operator: op_name="pmap(training_epoch)/jit(main)/while/body/transpose[permutation=(0, 2, 1, 3)]" source_file="/home/name/Desktop/Codes/ScalingOptimization (1)/train.py" source_line=263
		XLA Label: fusion
		Shape: f32[16,2048,10,336]
		==========================

	Buffer 2:
		Size: 420.00MiB
		Operator: op_name="pmap(training_epoch)/jit(main)/while/body/transpose[permutation=(0, 2, 1, 3)]" source_file="/home/name/Desktop/Codes/ScalingOptimization (1)/train.py" source_line=263
		XLA Label: fusion
		Shape: f32[16,2048,10,336]
		==========================

	Buffer 3:
		Size: 420.00MiB
		Operator: op_name="pmap(training_epoch)/jit(main)/while/body/while/body/jit(_take)/select_n" source_file="/home/name/Desktop/Codes/ScalingOptimization (1)/train.py" source_line=226
		XLA Label: fusion
		Shape: f32[1,32768,10,336]
		==========================

	Buffer 4:
		Size: 420.00MiB
		Operator: op_name="pmap(training_epoch)/jit(main)/while/body/while/body/jit(_take)/select_n" source_file="/home/name/Desktop/Codes/ScalingOptimization (1)/train.py" source_line=226
		XLA Label: fusion
		Shape: f32[1,32768,10,336]
		==========================

	Buffer 5:
		Size: 26.25MiB
		Operator: op_name="pmap(training_epoch)/jit(main)/while/body/transpose[permutation=(0, 2, 1, 3)]" source_file="/home/name/Desktop/Codes/ScalingOptimization (1)/train.py" source_line=263
		XLA Label: fusion
		Shape: f32[16,2048,10,21]
		==========================

	Buffer 6:
		Size: 26.25MiB
		Operator: op_name="pmap(training_epoch)/jit(main)/while/body/while/body/jit(_take)/select_n" source_file="/home/name/Desktop/Codes/ScalingOptimization (1)/train.py" source_line=226
		XLA Label: fusion
		Shape: f32[1,32768,10,21]
		==========================

	Buffer 7:
		Size: 13.12MiB
		Operator: op_name="pmap(training_epoch)/jit(main)/while/body/while/body/while/body/div" source_file="/home/name/.local/lib/python3.10/site-packages/brax/training/acme/running_statistics.py" source_line=208
		XLA Label: fusion
		Shape: f32[10,1024,336]
		==========================

	Buffer 8:
		Size: 11.18MiB
		XLA Label: fusion
		Shape: f32[2048,27,53]
		==========================

	Buffer 9:
		Size: 11.18MiB
		Entry Parameter Subshape: f32[2048,53,27]
		==========================

	Buffer 10:
		Size: 11.18MiB
		Entry Parameter Subshape: f32[2048,53,27]
		==========================

	Buffer 11:
		Size: 11.18MiB
		XLA Label: fusion
		Shape: f32[2048,53,27]
		==========================

	Buffer 12:
		Size: 11.18MiB
		XLA Label: fusion
		Shape: f32[2048,53,27]
		==========================

	Buffer 13:
		Size: 11.00MiB
		Operator: op_name="pmap(training_epoch)/jit(main)/while/body/while/body/while/body/jvp(MLP)/hidden_4/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/home/name/.local/lib/python3.10/site-packages/flax/linen/linear.py" source_line=255
		XLA Label: custom-call
		Shape: f32[11264,256]
		==========================

	Buffer 14:
		Size: 11.00MiB
		Operator: op_name="pmap(training_epoch)/jit(main)/while/body/while/body/while/body/jvp(MLP)/jit(silu)/mul" source_file="/home/name/.local/lib/python3.10/site-packages/brax/training/networks.py" source_line=56 deduplicated_name="fusion.116"
		XLA Label: fusion
		Shape: f32[11264,256]
		==========================

	Buffer 15:
		Size: 11.00MiB
		Operator: op_name="pmap(training_epoch)/jit(main)/while/body/while/body/while/body/jvp(MLP)/hidden_3/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/home/name/.local/lib/python3.10/site-packages/flax/linen/linear.py" source_line=255
		XLA Label: custom-call
		Shape: f32[11264,256]
		==========================


Process SpawnPoolWorker-1:
Traceback (most recent call last):
  File "/usr/lib/python3.10/multiprocessing/pool.py", line 131, in worker
    put((job, i, result))
  File "/usr/lib/python3.10/multiprocessing/queues.py", line 377, in put
    self._writer.send_bytes(obj)
  File "/usr/lib/python3.10/multiprocessing/connection.py", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/usr/lib/python3.10/multiprocessing/connection.py", line 411, in _send_bytes
    self._send(header + buf)
  File "/usr/lib/python3.10/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/usr/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/lib/python3.10/multiprocessing/pool.py", line 136, in worker
    put((job, i, (False, wrapped)))
  File "/usr/lib/python3.10/multiprocessing/queues.py", line 377, in put
    self._writer.send_bytes(obj)
  File "/usr/lib/python3.10/multiprocessing/connection.py", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/usr/lib/python3.10/multiprocessing/connection.py", line 411, in _send_bytes
    self._send(header + buf)
  File "/usr/lib/python3.10/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
/usr/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 6 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
time to jit: 0:00:49.579301
time to train: 0:04:48.257534
